<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Michaël Karpe</title>
    <link>https://michaelkarpe.github.io/quantitative-finance-projects/</link>
      <atom:link href="https://michaelkarpe.github.io/quantitative-finance-projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 16 Oct 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://michaelkarpe.github.io/media/icon_hu942ec82bbfdd4b8fc9bf90d8cd76fd06_20106_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://michaelkarpe.github.io/quantitative-finance-projects/</link>
    </image>
    
    <item>
      <title>Covariance estimation by sparse method</title>
      <link>https://michaelkarpe.github.io/quantitative-finance-projects/covariance/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/quantitative-finance-projects/covariance/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;presentation-of-the-project&#34;&gt;Presentation of the project&lt;/h1&gt;
&lt;p&gt;Determining the dependency structures between different assets or risk factors is at the heart of multidimensional financial modelling problems. For example, when considering a portfolio of assets, Markowitz theory states that it is important that the portfolio be diversified and therefore that the assets be as uncorrelated as possible.&lt;/p&gt;
&lt;p&gt;Mathematically, the natural variable to model this dependence structure is the covariance matrix. Indeed, in the context of Gaussian variables, covariance is sufficient to describe correlation structures (which is not the case for other distributions). Moreover, although the focus is on covariance and the criterion studied is based on Gaussian modelling, the information obtained is actually richer and can be applied to other distributions.&lt;/p&gt;
&lt;p&gt;However, the empirical data that we exploit are affected by noise that biases the estimation and conventional methods then provide poor results.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;purpose-and-method-of-resolution&#34;&gt;Purpose and method of resolution&lt;/h1&gt;
&lt;p&gt;The aim is therefore to obtain a noise-free covariance matrix, called sparse due to a large number of zero coefficients, from a noisy covariance matrix. This problem can be written as an optimization problem in which one seeks to maximize the log-likelihood of the solution by penalizing the number of zeros in the inverse covariance matrix. In addition, we impose constraints on its eigenvalues to ensure that the matrix is positive, and to further limit the solution, given the information that we would have a priori on the problem. In the Gaussian framework, the latter is formulated as follows:&lt;/p&gt;
&lt;p&gt;$$\text{max} \ f(X) := \log \det X - &amp;lt; \Sigma, X &amp;gt; - \rho \ Card(X)$$
$$\text{s.c.} \ \alpha I_n \leq X \leq \beta I_n$$
$$\Sigma \in S_n^{+}, \ X \in S_n$$
$$\rho, \ \alpha, \ \beta &amp;gt; 0$$&lt;/p&gt;
&lt;p&gt;However, this problem is described as NP-difficult, which means that it cannot be solved by computer in a reasonable time - one reason for this is the non-convexity of the objective function. It is therefore necessary to transform it. For this, convex relaxation methods are applied to the initial problem. These methods make it possible to put the problem into a form for which numerical solving algorithms exist. The algorithm used here is the algorithm of Nesterov (2005).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;nesterovs-algorithm&#34;&gt;Nesterov&amp;rsquo;s algorithm&lt;/h2&gt;
&lt;p&gt;Nesterov&amp;rsquo;s algorithm is based on recent and efficient optimization methods to determine the extremum of a function $\phi(y)$ which is close to the function $f(X)$. By reducing the difference between the exact solution and the approximated solution by successive iterations (see figure below), we obtain a result very close to the exact solution.&lt;/p&gt;
















&lt;figure  id=&#34;figure-illustration-of-the-nesterov-algorithm-with-fx-exact-function-phiy-approximate-function&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov.png&#34; alt=&#34;Illustration of the Nesterov algorithm with $f(x)$ exact function, $\phi(y)$ approximate function.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Illustration of the Nesterov algorithm with $f(x)$ exact function, $\phi(y)$ approximate function.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;checking-the-algorithm&#34;&gt;Checking the algorithm&lt;/h1&gt;
&lt;p&gt;In order to make sure that the algorithm is properly implemented, we tested it on a very simple noisy matrix. It was constructed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: An $A$ matrix is constructed whose diagonal coefficients are equal to 1 and of which a few non-diagonal coefficients (i.e. a negligible number compared to the size of the chosen matrix), randomly drawn according to a uniform probability, are equal to $1$ or $-1$ with equiprobability. The randomly drawn non-diagonal coefficients are copied symmetrically with respect to the diagonal so as to obtain a diagonal matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the resulting matrix is not invertible, the procedure is repeated. The matrix finally created corresponds to the inverse of a covariance matrix without noise but with some non-zero covariances, i.e. only a few variables are correlated.&lt;/p&gt;
















&lt;figure  id=&#34;figure-matrix-a-from-step-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape1.png&#34; alt=&#34;Matrix $A$ from step 1.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrix $A$ from step 1.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step 2: A second $V$ matrix whose coefficients follow a uniform law on $[-1, 1]$ is constructed and then symmetrized. This matrix corresponds to the noise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 3: We constitute a covariance matrix $B$ noise by summing the inverse of $A$ with $\sigma V$ where $\sigma$ allows to intensify or to attenuate the noise.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  id=&#34;figure-matrix-b-from-step-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape3.png&#34; alt=&#34;Matrix $B$ from step 3.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrix $B$ from step 3.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;We applied Nesterov&amp;rsquo;s algorithm to this test matrix with $\epsilon = 10^{-5}$, $\rho = 0.5$, $\alpha = 10^{-1}$, $\beta = 10$ and $\sigma = 0.15$.&lt;/p&gt;
















&lt;figure  id=&#34;figure-matrix-obtained-by-applying-nesterovs-algorithm-to-b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov_matrix.png&#34; alt=&#34;Matrix obtained by applying Nesterov&amp;#39;s algorithm to $B$.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrix obtained by applying Nesterov&amp;rsquo;s algorithm to $B$.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;In order to present the results obtained by the Nesterov algorithm for the test matrix, we represent the matrices at the input and output of the algorithm by arrays of pixels where each pixel has a color that depends on the value of the coefficient with which it is associated.&lt;/p&gt;
&lt;p&gt;The results obtained are very satisfactory. Indeed, the matrix obtained by the algorithm looks very much like the initial noiseless matrix.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;application-to-interest-rate-analysis&#34;&gt;Application to interest rate analysis&lt;/h1&gt;
&lt;p&gt;We then applied our algorithm to a covariance matrix obtained empirically from data on interest rate changes.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-noisy-empirical-covariance-matrix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/empirique.png&#34; alt=&#34;Noisy empirical covariance matrix.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Noisy empirical covariance matrix.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-selected-covariance-matrix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/selection.png&#34; alt=&#34;Selected covariance matrix.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Selected covariance matrix.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;After applying the algorithm, the selected matrix has a globally diagonal block structure. To interpret this structure, a graphical representation is used where the nodes are the different assets and an edge is drawn between two nodes if they are correlated (i.e. of non-zero covariance).&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/illustration.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Thus, we obtain two clusters that show that swaps are correlated according to their maturity. The top cluster has maturities of $3$, $6$ and $9$ months and the bottom cluster has maturities of $1$ to $30$ years. These different maturities correspond to different markets and different financing needs.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;analysis-of-the-method-presented&#34;&gt;Analysis of the method presented&lt;/h1&gt;
&lt;p&gt;Variance-covariance estimation by sparse method is a powerful and generic tool to manage constraints and keep some traceability at the data level. This method helps to extract fine information from noisy data, which could not have been properly analyzed without removing the noise contribution.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

&amp;nbsp;

# Présentation du projet

La détermination des structures de dépendance entre différents actifs ou facteurs de risques est au cœur des problèmes de modélisation financière multidimensionnels. Par exemple, lorsque l’on considère un portefeuille d’actifs, la théorie de Markowitz énonce qu’il est important que le portefeuille soit diversifié et donc que les actifs soient le moins corrélé possible.

Mathématiquement, la variable naturelle pour modéliser cette structure de dépendance est la matrice de covariance. En effet, dans le cadre de variables gaussiennes, la covariance est suffisante à décrire les structures de corrélation (ce qui n’est pas le cas pour d’autres distributions). De plus, bien que l’on s’intéresse à la covariance et que le critère étudié se base sur une modélisation gaussienne, l’information obtenue est en fait plus riche et peut s’appliquer à d’autres distributions.

Cependant, les données empiriques que nous exploitons sont affectées par du bruit qui biaise l’estimation et les méthodes classiques fournissent alors de piètres résultats.

&amp;nbsp;

# Objectif et méthode de résolution

Le but est donc d’obtenir une matrice de covariance débarrassée du bruit, dite sparse en raison d’un grand nombre de coefficients nuls, à partir d’une matrice de covariance bruitée. Ce problème peut s’écrire comme un problème d’optimisation dans lequel on cherche à maximiser la log-vraisemblance de la solution en pénalisant le nombre de zéros dans la matrice de covariance inverse. On impose, de plus, des contraintes sur ses valeurs propres permettant d’une part de s’assurer que la matrice est positive, et d’autre part de borner davantage la solution, compte tenu d’informations qu’on aurait a priori sur le problème. Dans le cadre gaussien, ce dernier se formule ainsi :

$$\text{max} \ f(X) := \log \det X - &lt; \Sigma, X &gt; - \rho \ Card(X)$$
$$\text{s.c.} \ \alpha I_n \leq X \leq \beta I_n$$
$$\Sigma \in S_n^{+}, \ X \in S_n$$
$$\rho, \ \alpha, \ \beta &gt; 0$$

Ce problème est néanmoins qualifié de NP-difficile, ce qui signifie qu’on ne peut pas le résoudre informatiquement en temps raisonnable – cela est notamment dû à la non-convexité de la fonction objectif. Il est donc nécessaire de le transformer. Pour cela, on applique au problème initial des méthodes de relaxation convexe. Ces méthodes permettent de mettre le problème sous une forme pour laquelle il existe des algorithmes de résolution numérique. L’algorithme utilisé ici est l’algorithme de Nesterov (2005).

&amp;nbsp;

## Algorithme de Nesterov

L’algorithme de Nesterov s’inspire de méthodes d’optimisation récentes et efficaces pour déterminer l’extremum d’une fonction $\phi(y)$ qui est proche de la fonction $f(X)$. En réduisant l’écart entre la solution exacte et la solution approchée par itérations successives (voir figure ci-dessous), on obtient un résultat très proche de la solution exacte.

















&lt;figure  id=&#34;figure-illustration-de-lalgorithme-de-nesterov-avec-fx-fonction-exacte-phiy-fonction-approchée&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov.png&#34; alt=&#34;Illustration de l’algorithme de Nesterov avec $f(x)$ fonction exacte, $\phi(y)$ fonction approchée.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Illustration de l’algorithme de Nesterov avec $f(x)$ fonction exacte, $\phi(y)$ fonction approchée.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

# Vérification de l&#39;algorithme

Afin de s’assurer de la bonne implémentation de l’algorithme, nous l’avons testé sur une matrice bruitée très simple. Celle-ci a été construite de la façon suivante :

- Etape 1 : On construit une matrice $A$ dont les coefficients diagonaux sont égaux à 1 et dont quelques coefficients non diagonaux (c’est-à-dire un nombre négligeable par rapport à la taille de la matrice choisie), tirés aléatoirement selon une probabilité uniforme, sont égaux à $1$ ou $-1$ avec équiprobabilité. Les coefficients non diagonaux tirés aléatoirement sont recopiés symétriquement par rapport à la diagonale de façon à obtenir une matrice diagonale.

Si la matrice alors obtenue n’est pas inversible, on répète la procédure. La matrice finalement créée correspond à l’inverse d’une matrice de covariance sans bruit mais avec quelques covariances non nulles, c’est-à-dire que seuls quelques variables sont corrélées.

















&lt;figure  id=&#34;figure-matrice-a-de-létape-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape1.png&#34; alt=&#34;Matrice $A$ de l&amp;#39;étape 1.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice $A$ de l&amp;rsquo;étape 1.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

- Etape 2 : On construit une seconde matrice $V$ dont les coefficients suivent une loi uniforme sur $[-1, 1]$ puis on la symétrise. Cette matrice correspond au bruit.

- Etape 3 : On constitue une matrice de covariance $B$ bruitée en sommant l’inverse de $A$ avec $\sigma V$ où $\sigma$ permet d’intensifier ou d’atténuer le bruit.

















&lt;figure  id=&#34;figure-matrice-b-de-létape-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape3.png&#34; alt=&#34;Matrice B de l&amp;#39;étape 3.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice B de l&amp;rsquo;étape 3.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

Nous avons appliqué l’algorithme de Nesterov sur cette matrice test avec $\epsilon = 10^{-5}$, $\rho = 0.5$, $\alpha = 10^{-1}$, $\beta = 10$ et $\sigma = 0.15$.

















&lt;figure  id=&#34;figure-matrice-obtenue-en-appliquant-lalgorithme-de-nesterov-à-b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov_matrix.png&#34; alt=&#34;Matrice obtenue en appliquant l’algorithme de Nesterov à $B$.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice obtenue en appliquant l’algorithme de Nesterov à $B$.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

Afin de présenter les résultats obtenus par l’algorithme de Nesterov pour la matrice test, nous représentons les matrices en entrée et en sortie de l’algorithme par des tableaux de pixels où chaque pixel a une couleur qui dépend de la valeur du coefficient auquel il est associé.

Les résultats obtenus sont très satisfaisants. En effet, la matrice obtenue par l’algorithme ressemble très fortement à la matrice sans bruit initiale.

&amp;nbsp;

# Application à l’analyse de taux d’intérêts

Nous avons ensuite appliqué notre algorithme à une matrice de covariance obtenue empiriquement à partir de données de variations de taux d’intérêts.

|      |      |
|------|------|
|















&lt;figure  id=&#34;figure-matrice-de-covariance-empirique-bruitée&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/empirique.png&#34; alt=&#34;Matrice de covariance empirique bruitée.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice de covariance empirique bruitée.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-matrice-de-covariance-sélectionnée&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/selection.png&#34; alt=&#34;Matrice de covariance sélectionnée.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice de covariance sélectionnée.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

Après avoir appliqué l’algorithme, la matrice sélectionnée présente une structure globalement diagonale par blocs. Pour interpréter cette structure,  on utilise une représentation graphique où les nœuds sont les différents actifs et on trace une arête entre deux nœuds s’ils sont corrélés (i.e. de covariance non nulle).

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/illustration.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

Ainsi, nous obtenons deux clusters qui montrent que les swaps sont corrélés en fonction de leur maturité. On trouve dans le cluster du haut, des maturités de 3, 6 et 9 mois et dans celui du bas, des maturités de 1 à 30 ans. Ces différentes maturités correspondent à de différents marchés et de différents besoins de financement.

&amp;nbsp;

# Analyse de la méthode présentée

L’estimation de variance-covariance par méthode sparse est un outil puissant et générique pour gérer les contraintes et garder une certaine tractabilité au niveau des données. Cette méthode aide à l’extraction d’informations fines à partir de données bruitées, lesquelles données n’auraient pas pu être analysées correctement sans avoir enlevé la contribution du bruit.

&amp;nbsp;

---&gt;
</description>
    </item>
    
  </channel>
</rss>
