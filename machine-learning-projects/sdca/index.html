<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.6.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Michaël Karpe" />

  
  
  
    
  
  <meta name="description" content="An alternative to Stochastic Gradient Descent for high-dimensional data" />

  
  <link rel="alternate" hreflang="en-us" href="https://michaelkarpe.github.io/machine-learning-projects/sdca/" />

  
  
  
    <meta name="theme-color" content="#3f51b5" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.9c4c13a48bae91f201c0af6081455e43.css" />

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  




<script async src="https://www.googletagmanager.com/gtag/js?id=G-695H905YSX"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-695H905YSX', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  


  




  
  
  

  
  

  
  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu942ec82bbfdd4b8fc9bf90d8cd76fd06_20106_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu942ec82bbfdd4b8fc9bf90d8cd76fd06_20106_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://michaelkarpe.github.io/machine-learning-projects/sdca/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@wowchemy" />
    <meta property="twitter:creator" content="@wowchemy" />
  
  <meta property="og:site_name" content="Michaël Karpe" />
  <meta property="og:url" content="https://michaelkarpe.github.io/machine-learning-projects/sdca/" />
  <meta property="og:title" content="Stochastic Dual Coordinate Ascent | Michaël Karpe" />
  <meta property="og:description" content="An alternative to Stochastic Gradient Descent for high-dimensional data" /><meta property="og:image" content="https://michaelkarpe.github.io/machine-learning-projects/sdca/featured.png" />
    <meta property="twitter:image" content="https://michaelkarpe.github.io/machine-learning-projects/sdca/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2018-06-15T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2018-06-15T00:00:00&#43;00:00">
  

  



  

  


  <title>Stochastic Dual Coordinate Ascent | Michaël Karpe</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="6b14331ed22f20c36f1e275bf06e668a" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.1ee5462d74c6c0de1f8881b384ecc58d.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Michaël Karpe</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Michaël Karpe</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#kaggle-competitions"><span>Kaggle Competitions</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#computer-vision-projects"><span>Computer Vision</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#machine-learning-projects"><span>Machine Learning</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#optimization-projects"><span>Optimization</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#programming-projects"><span>Programming</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#quantitative-finance-projects"><span>Quantitative Finance</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  





















  
  


<div class="article-container pt-3">
  <h1>Stochastic Dual Coordinate Ascent</h1>

  
  <p class="page-subtitle">An alternative to Stochastic Gradient Descent for high-dimensional data</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Michaël Karpe</span>, <span >
      Guillaume Desforges</span>, <span >
      Matthieu Roux</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jun 15, 2018
  </span>
  

  

  

  
  
  
  

  
  

</div>

  




<div class="btn-links mb-3">
  
  








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="/machine-learning-projects/sdca/SDCA_Report.pdf" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/MichaelKarpe/ponts-paristech-projects/tree/master/machine-learning/sdca" target="_blank" rel="noopener">
  Code
</a>













  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header" href="/machine-learning-projects/sdca/SDCA_Poster.pdf" >
    Poster</a>


</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 579px; max-height: 412px;">
  <div style="position: relative">
    <img src="/machine-learning-projects/sdca/featured_hu0c86a79d963a92130d8685506fa7b492_86479_720x2500_fit_q75_h2_lanczos_3.webp" width="579" height="412" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p> </p>
<h1 id="introduction">Introduction</h1>
<p>In machine learning, the process of fitting a model to the data requires to solve an optimization problem.
The difficulty resides in the fact that this optimization quickly becomes very complex when dealing with real problems.
The Stochastic Gradient Descent (SGD) is a very popular algorithm to solve those problems because it has good convergence guaranties.
Yet, the SGD does not have a good stopping criteria, and its solutions are often not accurate enough.</p>
<p>The Stochastic Dual Coordinate Ascent (SDCA) tries to solve the optimization problem by solving its dual problem.
Instead of optimizing the weights, we optimize a dual variable from which we can compute the weights and thus solve the former.
This method can give good results for specific problems : for instance, solving the dual problem of the SVM has proven to be effective and to give interesting results, with a linear convergence in some cases.</p>
<p>In this report, we compile the key theoretical points necessary to have a global understanding of the SDCA.
First we introduce the SDCA and its principles.
We then present the machine learning problem our report focuses on, and we study computational performances of the method by trying to apply SDCA on concrete problems. Finally we conclude on SDCA strengths and weaknesses.</p>
<p> </p>
<h1 id="purpose-of-the-report-a-new-sgd-like-method">Purpose of the report: a new SGD-like method</h1>
<h2 id="difference-between-sgd-and-sdca">Difference between SGD and SDCA</h2>
<p>A simple approach for solving Support Vector Machine learning is Stochastic Gradient Descent (SGD).
SGD finds an $\epsilon_P$-sub-optimal solution in time $O(1/(\lambda \epsilon_P))$.
We say that a solution $w$ is $\epsilon_P$-sub-optimal if $P(w) - P(w^{*}) \leq \epsilon_P$, where $P$ is the objective function of the primal problem.
This runtime does not depend on $n$ and therefore is favorable when $n$ is very large.
However, as explained in the studied articles, the SGD approach has several disadvantages:</p>
<ul>
<li>it does not have a clear stopping criterion</li>
<li>it tends to be too aggressive at the beginning of the optimization process, especially when $\lambda$ is very small</li>
<li>while SGD reaches a moderate accuracy quite fast, its convergence becomes rather slow when we are interested in more accurate solutions</li>
</ul>
<p>Therefore, an alternative approach is Dual Coordinate Ascent (DCA), which solves the dual problem instead of the primal problem.</p>
<p> </p>
<h2 id="formulation-of-sdca-optimization-problem">Formulation of SDCA optimization problem</h2>
<p>Let $x_1, \dots, x_n \in \mathbb{R}^d$, $\phi_1, \dots, \phi_n$ scalar convex functions, $\lambda &gt; 0$ regularization parameter. Let us focus on the following optimization problem:</p>
<p>$$\min_{w \in \mathbb{R}^d} P(w) = \left[ \dfrac{1}{n} \sum_{i=1}^n \phi_i(w^\top x_i) + \dfrac{\lambda}{2}||w||^2 \right]$$</p>
<p>with solution $w^{*} = \arg \min_{w \in \mathbb{R}^d} P(w)$.</p>
<p>Moreover, we say that a solution $w$ is $\epsilon_P$-sub-optimal if $P(w) - P(w^{*}) \leq \epsilon_P$. We analyze here the required runtime to find an $\epsilon_P$-sub-optimal solution using SDCA.</p>
<p>Let $\phi_i^{*} : \mathbb{R} \rightarrow \mathbb{R}$ be the convex conjugate of $\phi_i$ : $\phi_i^{*}(u) = \max_z (zu-\phi_i(z))$. The dual problem of &hellip; is defined as follows:</p>
<p>$$\max_{\alpha \in \mathbb{R}^n} D(\alpha) = \dfrac{1}{n} \sum_{i=1}^n -\phi_i^{*}(-\alpha_i) - \dfrac{\lambda}{2}||\dfrac{1}{\lambda n}\sum_{i=1}^n \alpha_ix_i||^2$$</p>
<p>with solution $\alpha^{*} = \arg \max_{a \in \mathbb{R}^n} D(\alpha)$.</p>
<p>Moreover, if we define $w(\alpha) = \frac{1}{\lambda n} \sum_{i=1}^n \alpha_ix_i$, thanks to classic optimization results, we then have:</p>
<p>$$w(\alpha^{*}) = w^{*}$$</p>
<p>$$P(w^{*}) = D(\alpha^{*})$$</p>
<p>We also define the duality gap as $P(w(\alpha)) - D(\alpha)$. The SDCA procedure is described in Section 1.4.</p>
<p> </p>
<h2 id="focus-on-the-logistic-regression">Focus on the logistic regression</h2>
<p>In order to fully grasp the method behind the first paper, let&rsquo;s take an example with the logistic regression. We will consider logistic regression only for binary classification. We use the following usual notations : $X \in \mathbf{X} = \mathbb{R}^p$ the random variable for the description space, and $Y \in \mathbf{Y} = \{-1, 1\}$ the random variable for the label. We recall that the model is the following :</p>
<p>$$\frac{\mathbb{P}(y=1 | X=x)}{\mathbb{P}(y=-1 |X=x)} = w^\top x, \quad w \in \mathbb{R}^p$$</p>
<p>We want to find $w$ such that it maximizes the likelihood, or log-likelihood, with a term of regularization:</p>
<p>$$\min_w C \sum_i \log\left(1 + e^{-y_iw^\top x_i}\right)  + \frac{1}{2} w^\top w$$</p>
<p>In order to get the dual problem, we rewrite it with an artificial constraint $z_i = y_iw^Tx_i$, and we have the following Lagrangian :</p>
<p>$$\mathcal{L}(w, z, \alpha) = \sum_i (C \log\left(1+z_i\right) + \alpha_i z_i) - \sum_i \alpha_i e^{-z_i} + \frac{1}{2}w^\top w$$</p>
<p>We will note $w^* = \sum_i \alpha_i y_i x_i$ and $z^*$ the variables solution of the optimization problem</p>
<p>$$\min_{w, z} \mathcal{L}(w, z, \alpha) = \mathcal{L}(w^<em>, z^</em>, \alpha) = \psi(\alpha)$$</p>
<p>In fact, it leads to the following dual problem :</p>
<p>$$\max_{\alpha} \sum_{i \in I} (-\alpha_i \log(\alpha_i) - (C-\alpha_i) \log(C - \alpha_i)) - \frac{1}{2} \alpha^\top Q\alpha$$
$$\text{s.t. } I = \{i,\ 0 &lt; \alpha_i &lt;= C \}$$
$$0 \leq \alpha_i \leq C$$
$$Q_{ij} = y_i x_i^T x_j y_j$$</p>
<p>Now we got the dual problem, we need to solve a maximization problem.
To do so, we will use in this paper the coordinate ascent method, which consist in optimizing the objective function coordinate by coordinate (or with groups of coordinates).
The SDCA algorithm is described in the next subsection.</p>
<p> </p>
<h2 id="sdca-algorithm">SDCA algorithm</h2>
















<figure  id="figure-sdca-algorithm">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/media/sdca/algorithm.png" alt="SDCA algorithm." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      SDCA algorithm.
    </figcaption></figure>
<p> </p>
<h2 id="computation-of-closed-forms">Computation of closed forms</h2>
<p>In the studied articles, SDCA is computed either for $L$-Lipschitz loss functions or for $(1/\gamma)$-smooth loss functions.
We recall that a function $\phi_i : \mathbb{R} \rightarrow \mathbb{R}$ is $L$-Lipschitz if $\forall a,b \in \mathbb{R}$, $|\phi_i(a)-\phi_i(b)| \leq L |a-b|$, and that a function $\phi_i : \mathbb{R} \rightarrow \mathbb{R}$ is $(1/\gamma)$-smooth if it is differentiable and its derivative is (1/$\gamma)$-Lipschitz.
Moreover, if $\phi_i$ is $(1/\gamma)$-smooth, then $\phi_i^{*}$ is $\gamma$-strongly convex.
The different loss functions used are described in the table below.
For experimentation, we mainly focused on log loss and square loss.</p>
<p> </p>
<h2 id="algorithm-termination">Algorithm termination</h2>
<p>For the sake of simplicity, the studied articles consider the following assumptions: $\forall i, ||x_i|| \leq 1$, $\forall (i,a), \phi_i(a) \geq 0$ and $\forall i, \phi_i(0) \leq 1$.
Under these assumptions, we have the following theorem:</p>
<p><strong>Theorem</strong> Consider Procedure SDCA with $\alpha^{(0)} = 0$.
Assume that $\forall i, \phi_i$ is $L$-Lipschitz (resp. $(1/\gamma)$-smooth).
To obtain an expected duality gap of $\mathbb{E}[P(\overline{w})-D(\overline{\alpha})] \leq \epsilon_P$, it suffices to have a total number of iterations of
$$T \geq n + \max\left(0, \left\lceil n \log \left(\dfrac{\lambda n}{2 L^2} \right) \right\rceil \right) + \dfrac{20 L^2}{\lambda \epsilon_P} \quad \left( \text{resp. } T &gt; \left(n + \dfrac{1}{\lambda \gamma} \right) \log \left[ \dfrac{1}{(T-T_0)\epsilon_P} \left(n + \dfrac{1}{\lambda \gamma} \right) \right] \right)$$</p>
<p> </p>
<h1 id="experiments">Experiments</h1>
<h2 id="implementation">Implementation</h2>
<p>We implemented :</p>
<ul>
<li><em>Estimator</em> objects that can fit, predict and score themselves : logistic loss and square loss</li>
<li><em>Optimizer</em> objects used for fitting : SGD and SDCA</li>
<li>projections : polynomial and gaussian</li>
<li>some data utilities</li>
</ul>
<p> </p>
<h2 id="description-of-the-chosen-data-sets">Description of the chosen data sets</h2>
<p>We used our implementation on :</p>
<ul>
<li><em>Arrhythmia</em> : <a href="https://archive.ics.uci.edu/ml/datasets/Arrhythmia" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/Arrhythmia</a></li>
<li><em>Adult</em> : <a href="https://archive.ics.uci.edu/ml/datasets/adult" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets/adult</a></li>
<li>some other data sets available on <em>scikit-learn</em>: <em>Labeled Faced Wild</em>, <em>Forest covertypes</em></li>
</ul>
<p>While the Arrhythmia data set has $452$ instances, which is quite low, it has $279$ features, which is quite high.
On the other hand, the Adult data set has $48,842$ instances but only $14$ features.</p>
<p>The Arrhythmia data set will help us to check the properties of SDCA when there are high-dimensional features.
The Adult data set will help us to compare the SGD and SDCA when there are a large number of instances.</p>
<p> </p>
<h2 id="use-of-closed-forms-and-numerical-issues">Use of closed forms and numerical issues</h2>
<p>In this report, we used the closed form presented above.
The closed form for the logistic regression gave us numerous numerical issues.
On some cases, we can end up with catastrophic cancellations due to either the $\log$ or the $\exp$.</p>
<p>A solution that is proposed by another study is to optimize a sub-problem with a modified Newton algorithm for each iteration, and thus avoid catastrophic cancellations. We implemented this modified Newton algorithm and tried to use it for the logistic regression on the data sets described above, but of course computation time was incredibly long comparing to the use of closed forms.</p>
<p> </p>
<h2 id="choice-of-algorithm-termination-option">Choice of algorithm termination option</h2>
<p>Because of the stochastic behavior of the algorithm, the output is very sensitive to the iteration at which it stops.
Indeed, coefficients vary suddenly, and the convergence is not really monotonous : at some point, it is uncertain whether the loss improves or not.</p>
<p>There are essentially two ways of taking this into account.
The first method is to stop at a random step, which actually yields good results.
The second method consists in averaging the last $\alpha^{(t)}$ obtained by the algorithm, making sure that the local variations of $\alpha$ are corrected.</p>
<p>Another way to stop the algorithm is to use the duality gap. However, as this theorem presents a sufficient condition for the total number of iterations, this number is much higher than the real total number of iterations needed to have an acceptable duality gap.</p>
<p>Considering this analysis, we decided to choose the average output option and to set manually the number of iterations needed for our experimentation. As explained in the studied articles, we can note that this stopping time $T_0$ can be chosen between $1$ to $T$, and is generally chosen equal to $T/2$. However, in practice, these parameters are not required as the duality gap is used to terminate the algorithm.</p>
<p> </p>
<h2 id="choice-of-hyperparameters">Choice of hyperparameters</h2>
<p>The SGD has two hyperparameters $c$ and $eps$ while the SDCA has only one hyperparameter $c$.
In order to compare the algorithms, we chose to select the best hyperparameters for each optimizer and for each data set using a validation procedure with a learning set and a validation set.
On every data set, for each hyperparameter, we computed the accuracy after a given number of epochs for a range of values and a certain validation set, and plotted them. We selected the following hyperparameter values :</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>SGD $c$</th>
<th>SGD $eps$</th>
<th>SDCA $c$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arrhythmia</td>
<td>$10^3$</td>
<td>$10^{-5}$</td>
<td>$10^{-1}$</td>
</tr>
<tr>
<td>Adults</td>
<td>$10^4$</td>
<td>$5 \cdot 10^{-6}$</td>
<td>$5 \cdot 10^{-2}$</td>
</tr>
</tbody>
</table>
<p> </p>
<h2 id="stopping-time">Stopping time</h2>
<p>With such data sets and hyper parameters, we compute the sufficient stopping time for a dual gap lower than $10^{-3}$.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Sufficient stopping time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arrhythmia</td>
<td>401,549</td>
</tr>
<tr>
<td>Adults</td>
<td>629,840</td>
</tr>
</tbody>
</table>
<p>These values perfectly illustrate the explanation about the sufficient stopping time condition described previously. In practice, only some tens of thousands, or even less, are sufficient to have a good convergence.</p>
<p> </p>
<h2 id="comparison-between-sgd-and-sdca-on-used-data-sets">Comparison between SGD and SDCA on used data sets</h2>
<p>We fit a logistic regression model on the data sets with the hyper parameters detailed above.
On each data set, we used $85$% of the data for training and $15$% of the data for testing.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>















<figure  id="figure-evolution-of-the-loss-during-the-learning-for-the-sgd-on-the-arrhythmia-dataset">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/media/sdca/arrhythmia_sgd.png" alt="Evolution of the loss during the learning for the **SGD** on the **Arrhythmia** dataset." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      Evolution of the loss during the learning for the <strong>SGD</strong> on the <strong>Arrhythmia</strong> dataset.
    </figcaption></figure></td>
<td>















<figure  id="figure-evolution-of-the-loss-during-the-learning-for-the-sdca-on-the-adults-dataset">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/media/sdca/arrhythmia_sdca.png" alt="Evolution of the loss during the learning for the **SDCA** on the **Adults** dataset." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      Evolution of the loss during the learning for the <strong>SDCA</strong> on the <strong>Adults</strong> dataset.
    </figcaption></figure></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>















<figure  id="figure-evolution-of-the-loss-during-the-learning-for-the-sgd-on-the-arrhythmia-dataset">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/media/sdca/adults_sgd.png" alt="Evolution of the loss during the learning for the **SGD** on the **Arrhythmia** dataset." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      Evolution of the loss during the learning for the <strong>SGD</strong> on the <strong>Arrhythmia</strong> dataset.
    </figcaption></figure></td>
<td>















<figure  id="figure-evolution-of-the-loss-during-the-learning-for-the-sdca-on-the-adults-dataset">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/media/sdca/adults_sdca.png" alt="Evolution of the loss during the learning for the **SDCA** on the **Adults** dataset." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      Evolution of the loss during the learning for the <strong>SDCA</strong> on the <strong>Adults</strong> dataset.
    </figcaption></figure></td>
</tr>
</tbody>
</table>
<p>We can see that after a consequent number of iterations, the accuracy of the estimator trained with the SDCA stops to vary, while the accuracy of the one trained with the SGD continues to vary and reaches better accuracy levels. In practice, it is highly probable that the SDCA gets trapped in a local minimum. Indeed, the structure itself of the algorithm makes it impossible to escape.</p>
<p>While the SGD can perform slight jumps thanks to the learning rate $eps$, the SDCA only optimizes along one coordinate. If it is trapped into a local minimum, it cannot vary anymore.</p>
<p>In our experiment, on the one hand the SGD has a better accuracy than the SDCA. On the other hand, the convergence of the SDCA is much clearer.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>















<figure  id="figure-evolution-of-the-accuracy-during-the-learning-for-the-sgd-and-the-sdca-on-the-arrhythmia-dataset">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/media/sdca/arrhythmia.png" alt="Evolution of the accuracy during the learning for the SGD and the SDCA on the **Arrhythmia** dataset." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      Evolution of the accuracy during the learning for the SGD and the SDCA on the <strong>Arrhythmia</strong> dataset.
    </figcaption></figure></td>
<td>















<figure  id="figure-evolution-of-the-accuracy-during-the-learning-for-the-sgd-and-the-sdca-on-the-adults-dataset">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/media/sdca/adults.png" alt="Evolution of the accuracy during the learning for the SGD and the SDCA on the **Adults** dataset." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      Evolution of the accuracy during the learning for the SGD and the SDCA on the <strong>Adults</strong> dataset.
    </figcaption></figure></td>
</tr>
</tbody>
</table>
<p> </p>
<h1 id="conclusion">Conclusion</h1>
<p>In this report, we summarized most of what is needed to understand the SDCA : its goal, its theoretical framework and its algorithm.
While our implementation of the SDCA for logistic regression seems to work, it did not yield better performance than SGD for our experiments.</p>
<p>On the other hand, the SGD can keep fluctuating when the SDCA really converges.
Depending on the problem, it can be a real advantage.
Other tracks need to be investigated in order to improve the performance of the SDCA, such as the resolution of numerical issues for some losses or the use of the SDCA on other data sets.</p>
<p> </p>
<h3 id="references">References</h3>
<p>This article is based on two main studies:</p>
<ul>
<li>
<p><em>Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</em> (S. Shalev-Shwartz and T. Zhang, 2013) from <em><a href="http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf" target="_blank" rel="noopener">http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf</a></em> was our main interest. This paper compiles many theoretical results on the SDCA and gives a clear algorithm.</p>
</li>
<li>
<p><em>Dual Coordinate Descent Methods for Logistic Regression and Maximum Entropy Models</em> (H.-F. Yu, F.-L. Huang, C.-J. Lin, 2011) from <em><a href="https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</a></em> gives interesting insight for the logistic regression case, with a modified Newton method for each iteration step instead of the approximation of the closed form, which helps against the numerical issues.</p>
</li>
</ul>
<p> </p>

    </div>

    







<div class="share-box">
  <ul class="share">
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://michaelkarpe.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu66194438bc6e14d260870deead871271_609448_270x270_fill_q75_lanczos_center.jpg" alt="Michaël Karpe"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://michaelkarpe.github.io/">Michaël Karpe</a></h5>
      <h6 class="card-subtitle">Machine Learning Scientist</h6>
      <p class="card-text">Machine Learning Scientist at Next Gate Tech. MEng in Industrial Engineering &amp; Operations Research, FinTech Concentration at University of California, Berkeley. Diplôme d&rsquo;Ingénieur (MSc) in Applied Mathematics &amp; Computer Science, Machine Learning &amp; Computer Vision Concentration at Ecole des Ponts ParisTech, France.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/michaelkarpe/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/MichaelKarpe" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=2yQ3TvgAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.kaggle.com/mika30" target="_blank" rel="noopener">
        <i class="fab fa-kaggle"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    




  
    




  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  

  

  

  
  






  
  <p class="powered-by copyright-license-text">
    © Michaël Karpe 2023. All rights reserved.
  </p>
  




  <p class="powered-by">
    
    
    
      
      
      
      
      
      

    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>








  
  


<script src="/en/js/wowchemy.min.54dd6e4d8f2e4b1d098381b57f18dd83.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>
















</body>
</html>
