<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Michaël Karpe</title>
    <link>https://michaelkarpe.github.io/</link>
      <atom:link href="https://michaelkarpe.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Michaël Karpe</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 10 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://michaelkarpe.github.io/media/icon_hu942ec82bbfdd4b8fc9bf90d8cd76fd06_20106_512x512_fill_lanczos_center_3.png</url>
      <title>Michaël Karpe</title>
      <link>https://michaelkarpe.github.io/</link>
    </image>
    
    <item>
      <title>Multi-Agent Reinforcement Learning in a Realistic Limit Order Book Market Simulation</title>
      <link>https://michaelkarpe.github.io/publication/karpe2020multiagent/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/publication/karpe2020multiagent/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An overall view of key problems in algorithmic trading and recent progress</title>
      <link>https://michaelkarpe.github.io/publication/karpe2020overall/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/publication/karpe2020overall/</guid>
      <description></description>
    </item>
    
    <item>
      <title>★ NYC Taxi Trip Duration Prediction</title>
      <link>https://michaelkarpe.github.io/kaggle-competitions/nyc-taxi/</link>
      <pubDate>Sun, 10 May 2020 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/kaggle-competitions/nyc-taxi/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How long does it take to get from the Bronx to Staten Island in an NYC yellow cab?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the &amp;ldquo;NYC Taxi Trip Duration Prediction&amp;rdquo; challenge of my UC Berkeley &amp;ldquo;Applications in Data Analysis&amp;rdquo; course, I built a machine learning model to predict the duration of an NYC taxi trip as soon as you enter the taxi! As the winning model of the challenge, this model predicts the duration of such a trip with a root mean square error of less than 4.5 minutes.&lt;/p&gt;
&lt;p&gt;If you are interested in machine learning and want to know how to build a powerful model on a simple regression problem (or if you just like machine learning memes), click on the &amp;ldquo;Code&amp;rdquo; button on top of this webpage!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>★ Madness at Home and on the Court</title>
      <link>https://michaelkarpe.github.io/kaggle-competitions/march-madness/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/kaggle-competitions/march-madness/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&amp;ldquo;Wow, we won a Kaggle competition!&amp;rdquo; That&amp;rsquo;s pretty much the reaction Emilien Etchevers, Kieran Janin, Remi Le Thai, Haley Wohlever and I got when the results of the &lt;em&gt;Google Cloud &amp;amp; NCAA March Madness Analytics competition&lt;/em&gt; were announced.&lt;/p&gt;
&lt;p&gt;In this Kaggle competition, we aimed to understand the factors contributing to the entertainment, or madness, of a college basketball match using two sources: objective data on NCAA teams&amp;rsquo; previous performances, and subjective data on fans&amp;rsquo; reactions to games. For the purposes of this project, two dependent variables related to the madness were explored: predictability in March Madness matches, and the corresponding distribution of viewers&amp;rsquo; sentiment.&lt;/p&gt;
&lt;p&gt;Click on the “Code” button on top of this webpage to discover our analysis!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Dual Coordinate Ascent</title>
      <link>https://michaelkarpe.github.io/machine-learning-projects/sdca/</link>
      <pubDate>Fri, 15 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/machine-learning-projects/sdca/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In machine learning, the process of fitting a model to the data requires to solve an optimization problem.
The difficulty resides in the fact that this optimization quickly becomes very complex when dealing with real problems.
The Stochastic Gradient Descent (SGD) is a very popular algorithm to solve those problems because it has good convergence guaranties.
Yet, the SGD does not have a good stopping criteria, and its solutions are often not accurate enough.&lt;/p&gt;
&lt;p&gt;The Stochastic Dual Coordinate Ascent (SDCA) tries to solve the optimization problem by solving its dual problem.
Instead of optimizing the weights, we optimize a dual variable from which we can compute the weights and thus solve the former.
This method can give good results for specific problems : for instance, solving the dual problem of the SVM has proven to be effective and to give interesting results, with a linear convergence in some cases.&lt;/p&gt;
&lt;p&gt;In this report, we compile the key theoretical points necessary to have a global understanding of the SDCA.
First we introduce the SDCA and its principles.
We then present the machine learning problem our report focuses on, and we study computational performances of the method by trying to apply SDCA on concrete problems. Finally we conclude on SDCA strengths and weaknesses.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;purpose-of-the-report-a-new-sgd-like-method&#34;&gt;Purpose of the report: a new SGD-like method&lt;/h1&gt;
&lt;h2 id=&#34;difference-between-sgd-and-sdca&#34;&gt;Difference between SGD and SDCA&lt;/h2&gt;
&lt;p&gt;A simple approach for solving Support Vector Machine learning is Stochastic Gradient Descent (SGD).
SGD finds an $\epsilon_P$-sub-optimal solution in time $O(1/(\lambda \epsilon_P))$.
We say that a solution $w$ is $\epsilon_P$-sub-optimal if $P(w) - P(w^{*}) \leq \epsilon_P$, where $P$ is the objective function of the primal problem.
This runtime does not depend on $n$ and therefore is favorable when $n$ is very large.
However, as explained in the studied articles, the SGD approach has several disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it does not have a clear stopping criterion&lt;/li&gt;
&lt;li&gt;it tends to be too aggressive at the beginning of the optimization process, especially when $\lambda$ is very small&lt;/li&gt;
&lt;li&gt;while SGD reaches a moderate accuracy quite fast, its convergence becomes rather slow when we are interested in more accurate solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, an alternative approach is Dual Coordinate Ascent (DCA), which solves the dual problem instead of the primal problem.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;formulation-of-sdca-optimization-problem&#34;&gt;Formulation of SDCA optimization problem&lt;/h2&gt;
&lt;p&gt;Let $x_1, \dots, x_n \in \mathbb{R}^d$, $\phi_1, \dots, \phi_n$ scalar convex functions, $\lambda &amp;gt; 0$ regularization parameter. Let us focus on the following optimization problem:&lt;/p&gt;
&lt;p&gt;$$\min_{w \in \mathbb{R}^d} P(w) = \left[ \dfrac{1}{n} \sum_{i=1}^n \phi_i(w^\top x_i) + \dfrac{\lambda}{2}||w||^2 \right]$$&lt;/p&gt;
&lt;p&gt;with solution $w^{*} = \arg \min_{w \in \mathbb{R}^d} P(w)$.&lt;/p&gt;
&lt;p&gt;Moreover, we say that a solution $w$ is $\epsilon_P$-sub-optimal if $P(w) - P(w^{*}) \leq \epsilon_P$. We analyze here the required runtime to find an $\epsilon_P$-sub-optimal solution using SDCA.&lt;/p&gt;
&lt;p&gt;Let $\phi_i^{*} : \mathbb{R} \rightarrow \mathbb{R}$ be the convex conjugate of $\phi_i$ : $\phi_i^{*}(u) = \max_z (zu-\phi_i(z))$. The dual problem of &amp;hellip; is defined as follows:&lt;/p&gt;
&lt;p&gt;$$\max_{\alpha \in \mathbb{R}^n} D(\alpha) = \dfrac{1}{n} \sum_{i=1}^n -\phi_i^{*}(-\alpha_i) - \dfrac{\lambda}{2}||\dfrac{1}{\lambda n}\sum_{i=1}^n \alpha_ix_i||^2$$&lt;/p&gt;
&lt;p&gt;with solution $\alpha^{*} = \arg \max_{a \in \mathbb{R}^n} D(\alpha)$.&lt;/p&gt;
&lt;p&gt;Moreover, if we define $w(\alpha) = \frac{1}{\lambda n} \sum_{i=1}^n \alpha_ix_i$, thanks to classic optimization results, we then have:&lt;/p&gt;
&lt;p&gt;$$w(\alpha^{*}) = w^{*}$$&lt;/p&gt;
&lt;p&gt;$$P(w^{*}) = D(\alpha^{*})$$&lt;/p&gt;
&lt;p&gt;We also define the duality gap as $P(w(\alpha)) - D(\alpha)$. The SDCA procedure is described in Section 1.4.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;focus-on-the-logistic-regression&#34;&gt;Focus on the logistic regression&lt;/h2&gt;
&lt;p&gt;In order to fully grasp the method behind the first paper, let&amp;rsquo;s take an example with the logistic regression. We will consider logistic regression only for binary classification. We use the following usual notations : $X \in \mathbf{X} = \mathbb{R}^p$ the random variable for the description space, and $Y \in \mathbf{Y} = \{-1, 1\}$ the random variable for the label. We recall that the model is the following :&lt;/p&gt;
&lt;p&gt;$$\frac{\mathbb{P}(y=1 | X=x)}{\mathbb{P}(y=-1 |X=x)} = w^\top x, \quad w \in \mathbb{R}^p$$&lt;/p&gt;
&lt;p&gt;We want to find $w$ such that it maximizes the likelihood, or log-likelihood, with a term of regularization:&lt;/p&gt;
&lt;p&gt;$$\min_w C \sum_i \log\left(1 + e^{-y_iw^\top x_i}\right)  + \frac{1}{2} w^\top w$$&lt;/p&gt;
&lt;p&gt;In order to get the dual problem, we rewrite it with an artificial constraint $z_i = y_iw^Tx_i$, and we have the following Lagrangian :&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}(w, z, \alpha) = \sum_i (C \log\left(1+z_i\right) + \alpha_i z_i) - \sum_i \alpha_i e^{-z_i} + \frac{1}{2}w^\top w$$&lt;/p&gt;
&lt;p&gt;We will note $w^* = \sum_i \alpha_i y_i x_i$ and $z^*$ the variables solution of the optimization problem&lt;/p&gt;
&lt;p&gt;$$\min_{w, z} \mathcal{L}(w, z, \alpha) = \mathcal{L}(w^&lt;em&gt;, z^&lt;/em&gt;, \alpha) = \psi(\alpha)$$&lt;/p&gt;
&lt;p&gt;In fact, it leads to the following dual problem :&lt;/p&gt;
&lt;p&gt;$$\max_{\alpha} \sum_{i \in I} (-\alpha_i \log(\alpha_i) - (C-\alpha_i) \log(C - \alpha_i)) - \frac{1}{2} \alpha^\top Q\alpha$$
$$\text{s.t. } I = \{i,\ 0 &amp;lt; \alpha_i &amp;lt;= C \}$$
$$0 \leq \alpha_i \leq C$$
$$Q_{ij} = y_i x_i^T x_j y_j$$&lt;/p&gt;
&lt;p&gt;Now we got the dual problem, we need to solve a maximization problem.
To do so, we will use in this paper the coordinate ascent method, which consist in optimizing the objective function coordinate by coordinate (or with groups of coordinates).
The SDCA algorithm is described in the next subsection.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;sdca-algorithm&#34;&gt;SDCA algorithm&lt;/h2&gt;
















&lt;figure  id=&#34;figure-sdca-algorithm&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/sdca/algorithm.png&#34; alt=&#34;SDCA algorithm.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      SDCA algorithm.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;computation-of-closed-forms&#34;&gt;Computation of closed forms&lt;/h2&gt;
&lt;p&gt;In the studied articles, SDCA is computed either for $L$-Lipschitz loss functions or for $(1/\gamma)$-smooth loss functions.
We recall that a function $\phi_i : \mathbb{R} \rightarrow \mathbb{R}$ is $L$-Lipschitz if $\forall a,b \in \mathbb{R}$, $|\phi_i(a)-\phi_i(b)| \leq L |a-b|$, and that a function $\phi_i : \mathbb{R} \rightarrow \mathbb{R}$ is $(1/\gamma)$-smooth if it is differentiable and its derivative is (1/$\gamma)$-Lipschitz.
Moreover, if $\phi_i$ is $(1/\gamma)$-smooth, then $\phi_i^{*}$ is $\gamma$-strongly convex.
The different loss functions used are described in the table below.
For experimentation, we mainly focused on log loss and square loss.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;algorithm-termination&#34;&gt;Algorithm termination&lt;/h2&gt;
&lt;p&gt;For the sake of simplicity, the studied articles consider the following assumptions: $\forall i, ||x_i|| \leq 1$, $\forall (i,a), \phi_i(a) \geq 0$ and $\forall i, \phi_i(0) \leq 1$.
Under these assumptions, we have the following theorem:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; Consider Procedure SDCA with $\alpha^{(0)} = 0$.
Assume that $\forall i, \phi_i$ is $L$-Lipschitz (resp. $(1/\gamma)$-smooth).
To obtain an expected duality gap of $\mathbb{E}[P(\overline{w})-D(\overline{\alpha})] \leq \epsilon_P$, it suffices to have a total number of iterations of
$$T \geq n + \max\left(0, \left\lceil n \log \left(\dfrac{\lambda n}{2 L^2} \right) \right\rceil \right) + \dfrac{20 L^2}{\lambda \epsilon_P} \quad \left( \text{resp. } T &amp;gt; \left(n + \dfrac{1}{\lambda \gamma} \right) \log \left[ \dfrac{1}{(T-T_0)\epsilon_P} \left(n + \dfrac{1}{\lambda \gamma} \right) \right] \right)$$&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;We implemented :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Estimator&lt;/em&gt; objects that can fit, predict and score themselves : logistic loss and square loss&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Optimizer&lt;/em&gt; objects used for fitting : SGD and SDCA&lt;/li&gt;
&lt;li&gt;projections : polynomial and gaussian&lt;/li&gt;
&lt;li&gt;some data utilities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;description-of-the-chosen-data-sets&#34;&gt;Description of the chosen data sets&lt;/h2&gt;
&lt;p&gt;We used our implementation on :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Arrhythmia&lt;/em&gt; : &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Arrhythmia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Arrhythmia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Adult&lt;/em&gt; : &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/adult&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://archive.ics.uci.edu/ml/datasets/adult&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;some other data sets available on &lt;em&gt;scikit-learn&lt;/em&gt;: &lt;em&gt;Labeled Faced Wild&lt;/em&gt;, &lt;em&gt;Forest covertypes&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the Arrhythmia data set has $452$ instances, which is quite low, it has $279$ features, which is quite high.
On the other hand, the Adult data set has $48,842$ instances but only $14$ features.&lt;/p&gt;
&lt;p&gt;The Arrhythmia data set will help us to check the properties of SDCA when there are high-dimensional features.
The Adult data set will help us to compare the SGD and SDCA when there are a large number of instances.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;use-of-closed-forms-and-numerical-issues&#34;&gt;Use of closed forms and numerical issues&lt;/h2&gt;
&lt;p&gt;In this report, we used the closed form presented above.
The closed form for the logistic regression gave us numerous numerical issues.
On some cases, we can end up with catastrophic cancellations due to either the $\log$ or the $\exp$.&lt;/p&gt;
&lt;p&gt;A solution that is proposed by another study is to optimize a sub-problem with a modified Newton algorithm for each iteration, and thus avoid catastrophic cancellations. We implemented this modified Newton algorithm and tried to use it for the logistic regression on the data sets described above, but of course computation time was incredibly long comparing to the use of closed forms.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;choice-of-algorithm-termination-option&#34;&gt;Choice of algorithm termination option&lt;/h2&gt;
&lt;p&gt;Because of the stochastic behavior of the algorithm, the output is very sensitive to the iteration at which it stops.
Indeed, coefficients vary suddenly, and the convergence is not really monotonous : at some point, it is uncertain whether the loss improves or not.&lt;/p&gt;
&lt;p&gt;There are essentially two ways of taking this into account.
The first method is to stop at a random step, which actually yields good results.
The second method consists in averaging the last $\alpha^{(t)}$ obtained by the algorithm, making sure that the local variations of $\alpha$ are corrected.&lt;/p&gt;
&lt;p&gt;Another way to stop the algorithm is to use the duality gap. However, as this theorem presents a sufficient condition for the total number of iterations, this number is much higher than the real total number of iterations needed to have an acceptable duality gap.&lt;/p&gt;
&lt;p&gt;Considering this analysis, we decided to choose the average output option and to set manually the number of iterations needed for our experimentation. As explained in the studied articles, we can note that this stopping time $T_0$ can be chosen between $1$ to $T$, and is generally chosen equal to $T/2$. However, in practice, these parameters are not required as the duality gap is used to terminate the algorithm.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;choice-of-hyperparameters&#34;&gt;Choice of hyperparameters&lt;/h2&gt;
&lt;p&gt;The SGD has two hyperparameters $c$ and $eps$ while the SDCA has only one hyperparameter $c$.
In order to compare the algorithms, we chose to select the best hyperparameters for each optimizer and for each data set using a validation procedure with a learning set and a validation set.
On every data set, for each hyperparameter, we computed the accuracy after a given number of epochs for a range of values and a certain validation set, and plotted them. We selected the following hyperparameter values :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;SGD $c$&lt;/th&gt;
&lt;th&gt;SGD $eps$&lt;/th&gt;
&lt;th&gt;SDCA $c$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Arrhythmia&lt;/td&gt;
&lt;td&gt;$10^3$&lt;/td&gt;
&lt;td&gt;$10^{-5}$&lt;/td&gt;
&lt;td&gt;$10^{-1}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adults&lt;/td&gt;
&lt;td&gt;$10^4$&lt;/td&gt;
&lt;td&gt;$5 \cdot 10^{-6}$&lt;/td&gt;
&lt;td&gt;$5 \cdot 10^{-2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;stopping-time&#34;&gt;Stopping time&lt;/h2&gt;
&lt;p&gt;With such data sets and hyper parameters, we compute the sufficient stopping time for a dual gap lower than $10^{-3}$.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Sufficient stopping time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Arrhythmia&lt;/td&gt;
&lt;td&gt;401,549&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adults&lt;/td&gt;
&lt;td&gt;629,840&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These values perfectly illustrate the explanation about the sufficient stopping time condition described previously. In practice, only some tens of thousands, or even less, are sufficient to have a good convergence.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;comparison-between-sgd-and-sdca-on-used-data-sets&#34;&gt;Comparison between SGD and SDCA on used data sets&lt;/h2&gt;
&lt;p&gt;We fit a logistic regression model on the data sets with the hyper parameters detailed above.
On each data set, we used $85$% of the data for training and $15$% of the data for testing.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-loss-during-the-learning-for-the-sgd-on-the-arrhythmia-dataset&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/sdca/arrhythmia_sgd.png&#34; alt=&#34;Evolution of the loss during the learning for the **SGD** on the **Arrhythmia** dataset.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the loss during the learning for the &lt;strong&gt;SGD&lt;/strong&gt; on the &lt;strong&gt;Arrhythmia&lt;/strong&gt; dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-loss-during-the-learning-for-the-sdca-on-the-adults-dataset&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/sdca/arrhythmia_sdca.png&#34; alt=&#34;Evolution of the loss during the learning for the **SDCA** on the **Adults** dataset.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the loss during the learning for the &lt;strong&gt;SDCA&lt;/strong&gt; on the &lt;strong&gt;Adults&lt;/strong&gt; dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-loss-during-the-learning-for-the-sgd-on-the-arrhythmia-dataset&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/sdca/adults_sgd.png&#34; alt=&#34;Evolution of the loss during the learning for the **SGD** on the **Arrhythmia** dataset.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the loss during the learning for the &lt;strong&gt;SGD&lt;/strong&gt; on the &lt;strong&gt;Arrhythmia&lt;/strong&gt; dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-loss-during-the-learning-for-the-sdca-on-the-adults-dataset&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/sdca/adults_sdca.png&#34; alt=&#34;Evolution of the loss during the learning for the **SDCA** on the **Adults** dataset.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the loss during the learning for the &lt;strong&gt;SDCA&lt;/strong&gt; on the &lt;strong&gt;Adults&lt;/strong&gt; dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can see that after a consequent number of iterations, the accuracy of the estimator trained with the SDCA stops to vary, while the accuracy of the one trained with the SGD continues to vary and reaches better accuracy levels. In practice, it is highly probable that the SDCA gets trapped in a local minimum. Indeed, the structure itself of the algorithm makes it impossible to escape.&lt;/p&gt;
&lt;p&gt;While the SGD can perform slight jumps thanks to the learning rate $eps$, the SDCA only optimizes along one coordinate. If it is trapped into a local minimum, it cannot vary anymore.&lt;/p&gt;
&lt;p&gt;In our experiment, on the one hand the SGD has a better accuracy than the SDCA. On the other hand, the convergence of the SDCA is much clearer.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-accuracy-during-the-learning-for-the-sgd-and-the-sdca-on-the-arrhythmia-dataset&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/sdca/arrhythmia.png&#34; alt=&#34;Evolution of the accuracy during the learning for the SGD and the SDCA on the **Arrhythmia** dataset.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the accuracy during the learning for the SGD and the SDCA on the &lt;strong&gt;Arrhythmia&lt;/strong&gt; dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-accuracy-during-the-learning-for-the-sgd-and-the-sdca-on-the-adults-dataset&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/sdca/adults.png&#34; alt=&#34;Evolution of the accuracy during the learning for the SGD and the SDCA on the **Adults** dataset.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the accuracy during the learning for the SGD and the SDCA on the &lt;strong&gt;Adults&lt;/strong&gt; dataset.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this report, we summarized most of what is needed to understand the SDCA : its goal, its theoretical framework and its algorithm.
While our implementation of the SDCA for logistic regression seems to work, it did not yield better performance than SGD for our experiments.&lt;/p&gt;
&lt;p&gt;On the other hand, the SGD can keep fluctuating when the SDCA really converges.
Depending on the problem, it can be a real advantage.
Other tracks need to be investigated in order to improve the performance of the SDCA, such as the resolution of numerical issues for some losses or the use of the SDCA on other data sets.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;This article is based on two main studies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization&lt;/em&gt; (S. Shalev-Shwartz and T. Zhang, 2013) from &lt;em&gt;&lt;a href=&#34;http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf&lt;/a&gt;&lt;/em&gt; was our main interest. This paper compiles many theoretical results on the SDCA and gives a clear algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Dual Coordinate Descent Methods for Logistic Regression and Maximum Entropy Models&lt;/em&gt; (H.-F. Yu, F.-L. Huang, C.-J. Lin, 2011) from &lt;em&gt;&lt;a href=&#34;https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf&lt;/a&gt;&lt;/em&gt; gives interesting insight for the logistic regression case, with a modified Newton method for each iteration step instead of the approximation of the closed form, which helps against the numerical issues.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image Segmentation: Mean Shift &amp; Normalized Cut</title>
      <link>https://michaelkarpe.github.io/computer-vision-projects/segmentation/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/computer-vision-projects/segmentation/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;mean-shift&#34;&gt;Mean Shift&lt;/h1&gt;
&lt;p&gt;The Mean Shift algorithm is a &lt;em&gt;non-parametric&lt;/em&gt; technique whose aim is to find local maxima in a &lt;em&gt;high-dimensional data distribution&lt;/em&gt; without computing the latter. Therefore, the main issue is how to efficiently estimate a density function given a set of samples.&lt;/p&gt;
&lt;p&gt;The simpliest way is to smooth the data. A common technique to smooth data is to compute a &lt;em&gt;kernel density estimation&lt;/em&gt; $f(\mathbf{x})$:&lt;/p&gt;
&lt;p&gt;$$f(x) = \sum_{i \in \mathcal{I}} K(\mathbf{x}-\mathbf{x_i}) = \sum_{i \in \mathcal{I}} k\left(\dfrac{||\mathbf{x}-\mathbf{x_i}||^2}{h^2}\right)$$&lt;/p&gt;
&lt;p&gt;where $(x_i)_{i \in \mathcal{I}}$ are the input samples, $k$ the kernel function and $h$ the kernel width. Then, we can find $f(x)$ maxima with usual optimization techniques (e.g. gradient ascent).&lt;/p&gt;
&lt;p&gt;However, $f(x)$ computation can have a too high complexity in high dimensional spaces. Thus, mean shift becomes useful. The algorithm uses a technique called &lt;em&gt;multiple restart gradient descent&lt;/em&gt;, starting from $y_0$ and iterating under the following procedure (where $G$ is associated with the kernel function $g(r)=-k&amp;rsquo;(r)$):&lt;/p&gt;
&lt;p&gt;$$\mathbf{y}_{k+1} = \mathbf{y}_k + \mathbf{m}(\mathbf{y}_k)$$&lt;/p&gt;
&lt;p&gt;$$\text{with} \quad \mathbf{m}(\mathbf{x}) = \dfrac{\sum_{i \in \mathcal{I}} \mathbf{x_i}G(\mathbf{x}-\mathbf{x_i})}{\sum_{i \in \mathcal{I}} G(\mathbf{x}-\mathbf{x_i})} - \mathbf{x} \quad \text{called the mean shift vector}$$&lt;/p&gt;
















&lt;figure  id=&#34;figure-one-dimensional-visualization-of-the-kernel-density-estimate-its-derivative-and-a-mean-shift&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/segmentation/meanshift.png&#34; alt=&#34;One-dimensional visualization of the kernel density estimate, its derivative, and a mean shift.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      One-dimensional visualization of the kernel density estimate, its derivative, and a mean shift.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;It has been proven that this algorithm converges if the kernel $k(r)$ is monotonically decreasing. Two kernels commonly used for the mean shift algorithm are the &lt;em&gt;Gaussian kernel&lt;/em&gt;, and the &lt;em&gt;Epanechnikov kernel&lt;/em&gt;, whose formula is $k_{E}(r) = \max(0,1-r)$. Therefore, the simpliest way to apply mean shift algorithm is to use the above gradient procedure at every input point $x_i$, in order to find all local maxima.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;image segmentation&lt;/em&gt;, mean shift algorithm is generally used taking into account spatial coordinates and color of pixels, as with the bilateral filter, through a kernel of the form:&lt;/p&gt;
&lt;p&gt;$$K(\mathbf{x_i}) = k\left(\dfrac{||\mathbf{x_r}||^2}{h_r^2}\right)k\left(\dfrac{||\mathbf{x_s}||^2}{h_s^2}\right)$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{x_s} = (x,y)$ are the spatial coordinates (spatial domain), $\mathbf{x_r}$ is the color value (range domain), $h_s$ (resp. $h_r$) the spatial (resp. range) bandwith.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;normalized-cut&#34;&gt;Normalized Cut&lt;/h1&gt;
&lt;p&gt;The Normalized Cut algorithm is an efficient way to segment an image. This algorithm is based on a graph representation of the image: pixels are vertices and weights (edges) depend on the image (brightness, intensity, distance or whatever can be useful to segment the image). The vertices could be just a subset of pixels like points of interest.&lt;/p&gt;
&lt;p&gt;Once the graph is computed, the problem is to cut vertices into two disjoint subsets $A$ and $B$ such that weights from A to B, $cut(A, B) = \sum_{u \in A, v\in B} w(u, v)$ is minimal. Unfortunately, algorithms tend to make unbalanced sets ($cut(A, B)$ is smaller if $A$ contains only one element). The idea for this algorithm is to \textit{normalize the cut}:&lt;/p&gt;
&lt;p&gt;$$N_{cut}(A, B) = \frac{cut(A, B)}{assoc(A, V)} + \frac{cut(A, B)}{assoc(B, V)}$$&lt;/p&gt;
&lt;p&gt;where $assoc(A, V) = \sum_{u \in A, t \in V} w(u, t)$.&lt;/p&gt;
&lt;p&gt;It has been proven in &lt;em&gt;reference 1&lt;/em&gt; that the &lt;em&gt;normalized cut&lt;/em&gt; is equivalent to find the eigenvector with the second smallest eigenvalue for:&lt;/p&gt;
&lt;p&gt;$$(D - W)x = \lambda Dx$$&lt;/p&gt;
&lt;p&gt;where, if $N = |V|$, $W \in \mathbb{R}^{N\times N}$ is the weight matrix and $D \in \mathbb{R}^{N\times N}$ is the diagonal matrix where $D(i, i) = \sum_j w(i, j)$. Signs of the second eigenvector $x$ decide on the cut ($i \in A$ iff $x(i) &amp;gt; 0$).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;our-results&#34;&gt;Our results:&lt;/h2&gt;
&lt;p&gt;We decided to test this algorithm. Firstly, we worked on a set of points in $\mathbb{R}^2$ and we made a graph where vertices are points and weights are $w(x,y) = ||x - y||_2^{-1}$. We obtained the segmentation shown in the left figure below, which give good results (the algorithm did not know the colors / the labels of the points). Then, we decided to work on artificial images and we found two main issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can we make efficiently (in Python) the graph from the image?&lt;/li&gt;
&lt;li&gt;How can we speed up the algorithm (because the complexity is $O(N^3)$ where $N = |V|$, $N = 10^6$ for a $1000\times 1000$ image)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To solve the first issue, we decided to make a graph only based on colors, i.e. $w(x, y) = np.abs(I(x) - I(y))$. To solve the second one, we used a &lt;em&gt;sparsed matrix&lt;/em&gt;. We obtained great results for two artificial black and white images but results are awful for real images. Therefore the idea is to change our graph representation.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-results-for-the-cut-of-two-gaussians&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/segmentation/cut1.png&#34; alt=&#34;Results for the cut of two gaussians.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Results for the cut of two gaussians.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-perfect-results-for-an-artificial-and-complex-bw-picture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/segmentation/cut2.png&#34; alt=&#34;Perfect results for an artificial and complex B&amp;amp;W picture.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Perfect results for an artificial and complex B&amp;amp;W picture.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-failure-with-a-real-photograph-coins&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/segmentation/cut3.png&#34; alt=&#34;Failure with a real photograph (coins).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Failure with a real photograph (coins).
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Shi, J., &amp;amp; Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8), 888-905.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Szeliski, R. (2010). Computer vision: algorithms and applications. Springer Science &amp;amp; Business Media.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>★ Credit Card Fraud Detection</title>
      <link>https://michaelkarpe.github.io/machine-learning-projects/outlier/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/machine-learning-projects/outlier/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;brief-description-of-the-project&#34;&gt;Brief description of the project&lt;/h1&gt;
&lt;p&gt;During the Spring semester of my second year at &lt;em&gt;École des Ponts ParisTech&lt;/em&gt; (from February 2018 to May 2018), I worked with three other students on a credit card fraud detection project for &lt;em&gt;Société Générale&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The aim of this project was to realize a comparative study between different outlier detection algorithms to select the one that had the best performances on a credit card fraud dataset. After some research, we decided to focus our study on the following algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Robust Estimator of Covariance&lt;/li&gt;
&lt;li&gt;Local Outlier Factor&lt;/li&gt;
&lt;li&gt;One-class SVM&lt;/li&gt;
&lt;li&gt;Isolation Forest&lt;/li&gt;
&lt;li&gt;Autoencoder&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code is available on my GitHub but has been left uncleaned.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;related-project&#34;&gt;Related project&lt;/h1&gt;
&lt;p&gt;In June 2019, Robin Teuwens shared a kernel on the famous Kaggle &lt;em&gt;Credit Card Fraud Dataset&lt;/em&gt;, entitled &lt;em&gt;Fraud Detection as a Cost Optimization Problem&lt;/em&gt; (Teuwens changed the name of kernel in August) and proposing &lt;a href=&#34;https://www.kaggle.com/robinteuwens/precision-vs-recall-optimizing-fraud-costs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an approach very similar to ours&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Indeed, the image at the beginning of this article shows profits and costs we had chosen for the cost function we wanted to minimize for the &lt;em&gt;Societe Generale&lt;/em&gt; project, according to the predicted and the real label of a given transaction.&lt;/p&gt;
&lt;p&gt;In November 2019, &lt;a href=&#34;https://www.kaggle.com/mika30/improving-fraud-detection-for-cost-minimization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;I made some changes to Teuwens&amp;rsquo; kernel&lt;/a&gt; to improve his results, switching the logistic regression to gradient boosted trees algorithms and SMOTE oversampling to ADASYN oversampling.&lt;/p&gt;
&lt;p&gt;If I have some time in the next months (but it&amp;rsquo;s unlikely as I switched to new projects), I would like to evaluate the performance of autoencoders on this latest kernel, as we used them for the &lt;em&gt;Societe Generale&lt;/em&gt; project and as Teuwens also proposed &lt;a href=&#34;https://www.kaggle.com/robinteuwens/anomaly-detection-with-auto-encoders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a kernel using autoencoders&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>★ Lloyd Iteration Convergence</title>
      <link>https://michaelkarpe.github.io/computer-vision-projects/lloyd/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/computer-vision-projects/lloyd/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;relocation-vectors&#34;&gt;Relocation vectors&lt;/h1&gt;
&lt;h2 id=&#34;calculation&#34;&gt;Calculation&lt;/h2&gt;
&lt;p&gt;The visualization of the relocation vectors is obtained by plotting the segments between the generators before and after Lloyd&amp;rsquo;s iteration. The generators of the cells in the previous iteration have been drawn in red.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-relocation-vectors&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q2_relocation.png&#34; alt=&#34;Visualization of relocation vectors.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of relocation vectors.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_uniform_image.png&#34; alt=&#34;Visualization of the cells after convergence.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;convergence&#34;&gt;Convergence&lt;/h2&gt;
&lt;p&gt;For the plotting of the convergence curves, we saved the different values in .txt files and processed the data in Python language, with the matplotlib.pyplot library. We can see that this evolution is globally decreasing, with an increase in energy when the cells &amp;ldquo;unblock&amp;rdquo; after having converged to a local minimum of energy.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-in-logarithmic-scale-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-a-uniform-initialization&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_uniform_length.png&#34; alt=&#34;Evolution in logarithmic scale of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for a **uniform** initialization.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution in logarithmic scale of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for a &lt;strong&gt;uniform&lt;/strong&gt; initialization.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-in-logarithmic-scale-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-an-initialization-in-one-corner&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_coin_length.png&#34; alt=&#34;Evolution in logarithmic scale of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for an initialization **in one corner**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution in logarithmic scale of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for an initialization &lt;strong&gt;in one corner&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-in-logarithmic-scale-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-an-initialization-on-a-line&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_line_length.png&#34; alt=&#34;Evolution in logarithmic scale of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for an initialization **on a line**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution in logarithmic scale of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for an initialization &lt;strong&gt;on a line&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;By zooming the first convergence curve between $30$ and $60$ iterations and plotting it in real scale (and not in logarithmic scale as on the different images), it becomes clear that the convergence of the mean length is quadratic.&lt;/p&gt;
















&lt;figure  id=&#34;figure-visualization-of-the-sampling-of-the-square-domain-with-n--10-000-points&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_regression.png&#34; alt=&#34;Visualization of the sampling of the square domain with $N = 10 000$ points.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the sampling of the square domain with $N = 10 000$ points.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;energy-distribution-by-cell&#34;&gt;Energy distribution by cell&lt;/h1&gt;
&lt;h2 id=&#34;calculation-1&#34;&gt;Calculation&lt;/h2&gt;
&lt;p&gt;The global energy of a domain partition in $N_c$ cells $\mathcal{C}_i$ is given by the following formula :&lt;/p&gt;
&lt;p&gt;$$E_{dom} = \sum_{i=1}^{N_c} E_{cell} = \sum_{i=1}^{N_c} \int_{x \in \mathcal{C}_i} ||x-x_i||^2 dx$$&lt;/p&gt;
&lt;p&gt;To carry out the calculation of integrals, we proceed by Monte-Carlo method, by sampling a number $N$ of points $x_j$ on the whole domain and by approximating the integral by a discrete sum :&lt;/p&gt;
&lt;p&gt;$$E_{cell} = \int_{x \in \mathcal{C}&lt;em&gt;i} ||x-x_i||^2 dx = \sum&lt;/em&gt;{x_j \in \mathcal{C}_i} ||x_j-x_i||^2$$&lt;/p&gt;
&lt;p&gt;A representation of this sampling is shown for $N = 10,000$. In order to cover the entire domain sufficiently, we have chosen $N = 100,000$.&lt;/p&gt;
















&lt;figure  id=&#34;figure-visualization-of-cells-after-convergence-in-an-hourglass-shaped-domain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q4_sampling.png&#34; alt=&#34;Visualization of cells after convergence in an hourglass-shaped domain.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of cells after convergence in an hourglass-shaped domain.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;convergence-1&#34;&gt;Convergence&lt;/h2&gt;
&lt;p&gt;Still in the square domain with an initialization of $10$ points, the evolution of the energy of the cells is represented below. For the $3$ initializations tested, we can see that the cells converge to a close energy after about thirty iterations. This convergence is fast for uniform and one-line initializations, with a slight energy correction at about $23$ iterations for the uniform initialization, due to the cells getting stuck (which we find with the growth of the average norm of the relocation vectors around the same number of iterations).&lt;/p&gt;
&lt;p&gt;For initialization in a corner, convergence is slower because more iterations are needed to allow the $10$ generators to occupy the entire domain space. However, there is no major energy jump due to a blockage of cells between them.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-a-uniform-initialization&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_uniform_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for a **uniform** initialization.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for a &lt;strong&gt;uniform&lt;/strong&gt; initialization.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-an-initialization-in-a-corner&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_coin_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for an initialization **in a corner**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for an initialization &lt;strong&gt;in a corner&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-an-initialization-on-a-line&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_line_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for an initialization **on a line**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for an initialization &lt;strong&gt;on a line&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;domain-influence&#34;&gt;Domain influence&lt;/h2&gt;
&lt;p&gt;The shape of the domain greatly influences the convergence results. For example, for the circular domain, which is a &amp;ldquo;regular&amp;rdquo; domain in the sense that the edges do not have &amp;ldquo;corners&amp;rdquo; or abrupt breaks in direction (the edges are well continuous and driftable), convergence occurs without any problem after about ten iterations and is not characterized by any particular jumps. The results for the circular domain are shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-cells-after-convergence-in-a-circular-domain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_circle_image.png&#34; alt=&#34;Visualization of cells after convergence in a circular domain.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of cells after convergence in a circular domain.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-relocation-vectors-in-a-circular-domain-as-a-function-of-the-number-of-lloyds-iterations-for-uniform-initialization&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_circle_length.png&#34; alt=&#34;Evolution of the mean norm of relocation vectors in a circular domain as a function of the number of Lloyd&amp;#39;s iterations for uniform initialization.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of relocation vectors in a circular domain as a function of the number of Lloyd&amp;rsquo;s iterations for uniform initialization.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-cell-energy-in-a-circular-domain-as-a-function-of-the-number-of-lloyds-iterations-for-uniform-initialization&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_circle_energy.png&#34; alt=&#34;Evolution of the mean cell energy in a circular domain as a function of the number of Lloyd&amp;#39;s iterations for uniform initialization.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean cell energy in a circular domain as a function of the number of Lloyd&amp;rsquo;s iterations for uniform initialization.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;For the other areas provided, more original behaviour can be observed, due to the same observations as those made previously. For the L-shape, the inner corner of the L will cause some increases in average length, as the cells &amp;ldquo;crossing&amp;rdquo; the corner move quickly at once to adapt to the shape, and convergence is quite slow relative to the square and circular shapes.&lt;/p&gt;
&lt;p&gt;For the key-shaped domain, convergence is also slower due to the complexity of the domain, and medium-length jumps are more important due to this complexity, because cells move a lot when a &amp;ldquo;corner&amp;rdquo; is crossed and filled by the cells.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-cells-after-convergence-in-a-l-domain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_lform_image.png&#34; alt=&#34;Visualization of cells after convergence in a **L** domain.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of cells after convergence in a &lt;strong&gt;L&lt;/strong&gt; domain.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-cells-after-convergence-in-a-domain-containing-a-cross&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_cross_image.png&#34; alt=&#34;Visualization of cells after convergence in a domain **containing a cross**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of cells after convergence in a domain &lt;strong&gt;containing a cross&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-cells-after-convergence-in-a-domain-in-the-form-of-a-key&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_key_image.png&#34; alt=&#34;Visualization of cells after convergence in a domain **in the form of a key**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of cells after convergence in a domain &lt;strong&gt;in the form of a key&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-in-a-l-domain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_lform_length.png&#34; alt=&#34;Evolution of the mean norm of relocation vectors as a function of the number of Lloyd&amp;#39;s iterations in a **L** domain.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations in a &lt;strong&gt;L&lt;/strong&gt; domain.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-norm-of-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-in-a-domain-containing-a-cross&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_cross_length.png&#34; alt=&#34;Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;#39;s iterations in a domain **containing a cross**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations in a domain &lt;strong&gt;containing a cross&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-norm-of-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-in-a-domain-in-the-form-of-a-key&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_key_length.png&#34; alt=&#34;Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;#39;s iterations in a domain **in the form of a key**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations in a domain &lt;strong&gt;in the form of a key&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-cell-energy-as-a-function-of-the-number-of-lloyds-iterations-in-a-l-domain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_lform_energy.png&#34; alt=&#34;Evolution of the average cell energy as a function of the number of Lloyd&amp;#39;s iterations in a **L** domain.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average cell energy as a function of the number of Lloyd&amp;rsquo;s iterations in a &lt;strong&gt;L&lt;/strong&gt; domain.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-in-a-domain-containing-a-cross&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_cross_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations in a domain **containing a cross**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations in a domain &lt;strong&gt;containing a cross&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-in-a-domain-in-the-form-of-a-key&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_key_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations in a domain **in the form of a key**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations in a domain &lt;strong&gt;in the form of a key&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;For hourglass-shaped domains (hard-coded in C++ code according to a parameter corresponding to the size of the bottleneck), this difficulty for the Lloyd&amp;rsquo;s iteration to cross corners or narrow passages is clearly highlighted. Indeed, we can see that, in general, the narrower the bottleneck, the longer the cells take to cross the bottleneck, and thus the greater the average length rise when the bottleneck is crossed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Initialization:&lt;/strong&gt; The observation on all domains leads us to think that for fairly simple and regular domains, a uniform initialization is quite efficient. For more complex domain forms, it is advisable to initialize the algorithm with more generators at the irregular edges of the domain, or at places with narrow passages.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-in-an-hourglass-shaped-domain-with-bottleneck-width-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass001_image.png&#34; alt=&#34;Visualization of the cells after convergence in an hourglass-shaped domain with bottleneck width **1**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence in an hourglass-shaped domain with bottleneck width &lt;strong&gt;1&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-in-an-hourglass-shaped-domain-with-bottleneck-width-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass003_image.png&#34; alt=&#34;Visualization of the cells after convergence in an hourglass-shaped domain with bottleneck width **3**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence in an hourglass-shaped domain with bottleneck width &lt;strong&gt;3&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-in-an-hourglass-shaped-domain-with-bottleneck-width-5&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass005_image.png&#34; alt=&#34;Visualization of the cells after convergence in an hourglass-shaped domain with bottleneck width **5**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence in an hourglass-shaped domain with bottleneck width &lt;strong&gt;5&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-norm-of-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-an-hourglass-domain-with-bottleneck-width-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass001_length.png&#34; alt=&#34;Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for an hourglass domain with bottleneck width **1**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for an hourglass domain with bottleneck width &lt;strong&gt;1&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-norm-of-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-an-hourglass-domain-with-bottleneck-width-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass003_length.png&#34; alt=&#34;Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for an hourglass domain with bottleneck width **3**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for an hourglass domain with bottleneck width &lt;strong&gt;3&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-norm-of-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-an-hourglass-domain-with-bottleneck-width-5&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass005_length.png&#34; alt=&#34;Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for an hourglass domain with bottleneck width **5**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average norm of relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for an hourglass domain with bottleneck width &lt;strong&gt;5&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-cell-energy-as-a-function-of-the-number-of-lloyds-iterations-for-an-hourglass-domain-with-bottleneck-width-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass001_energy.png&#34; alt=&#34;Evolution of the average cell energy as a function of the number of Lloyd&amp;#39;s iterations for an hourglass domain with bottleneck width **1**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average cell energy as a function of the number of Lloyd&amp;rsquo;s iterations for an hourglass domain with bottleneck width &lt;strong&gt;1&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-cell-energy-as-a-function-of-the-number-of-lloyds-iterations-for-an-hourglass-domain-with-bottleneck-width-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass003_energy.png&#34; alt=&#34;Evolution of the average cell energy as a function of the number of Lloyd&amp;#39;s iterations for an hourglass domain with bottleneck width **3**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average cell energy as a function of the number of Lloyd&amp;rsquo;s iterations for an hourglass domain with bottleneck width &lt;strong&gt;3&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-cell-energy-as-a-function-of-the-number-of-lloyds-iterations-for-an-hourglass-domain-with-bottleneck-width-5&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass005_energy.png&#34; alt=&#34;Evolution of the average cell energy as a function of the number of Lloyd&amp;#39;s iterations for an hourglass domain with bottleneck width **5**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average cell energy as a function of the number of Lloyd&amp;rsquo;s iterations for an hourglass domain with bottleneck width &lt;strong&gt;5&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;overshooting-and-temporal-inertia&#34;&gt;Overshooting and temporal inertia&lt;/h1&gt;
&lt;h2 id=&#34;overshooting&#34;&gt;Overshooting&lt;/h2&gt;
&lt;p&gt;Overshooting involves moving the generators not to the centres of mass of the cells, but slightly beyond. Informally, we consider the displacement vector $v_N$ at the $N$ iteration of a classical Lloyd&amp;rsquo;s iteration, and instead of moving the generator according to $v_N$, we move it according to $v_{N+1} = (1+\alpha)v_N$, where $\alpha \in \mathbb{R}^{+}$.&lt;/p&gt;
&lt;p&gt;There are three classes of behaviour of the Lloyd&amp;rsquo;s iteration according to the value of the overshooting parameter (determined approximately by numerical testing, the limit values of these behaviours change notably according to the shape of the domain and the initialization of the generators) for the square domain and for a uniform initialization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\alpha \in [0,1]$ : the overshooting parameter is quite small: it speeds up the convergence of Lloyd&amp;rsquo;s iteration&amp;hellip;&lt;/li&gt;
&lt;li&gt;$\alpha \approx [1, 1 + \epsilon]$ : the overshooting parameter is a bit too big : the iteration leads to a situation where $(1+\alpha)v_{N+1} = -(1+\alpha)v_N$, which causes the generators to alternate between two configurations&lt;/li&gt;
&lt;li&gt;$\alpha &amp;gt; &amp;gt; 1+\epsilon$: the overshooting parameter is much too large: Lloyd&amp;rsquo;s iteration diverges and behaves chaotically.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-with-an-overshooting-parameter-with-an-overshooting-parameter-alpha--05&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting050_image.png&#34; alt=&#34;Visualization of the cells after convergence with an overshooting parameter with an overshooting parameter $\alpha = 0.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence with an overshooting parameter with an overshooting parameter $\alpha = 0.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-with-an-overshooting-parameter-with-an-overshooting-parameter-alpha--11&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting110_image.png&#34; alt=&#34;Visualization of the cells after convergence with an overshooting parameter with an overshooting parameter $\alpha = 1.1$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence with an overshooting parameter with an overshooting parameter $\alpha = 1.1$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-with-an-overshooting-parameter-with-an-overshooting-parameter-alpha--15&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting150_image.png&#34; alt=&#34;Visualization of the cells after convergence with an overshooting parameter with an overshooting parameter $\alpha = 1.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence with an overshooting parameter with an overshooting parameter $\alpha = 1.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-an-overshooting-parameter-alpha--05&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting050_length.png&#34; alt=&#34;Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for a square domain with an overshooting parameter $\alpha = 0.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with an overshooting parameter $\alpha = 0.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-an-overshooting-parameter-alpha--11&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting110_length.png&#34; alt=&#34;Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for a square domain with an overshooting parameter $\alpha = 1.1$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with an overshooting parameter $\alpha = 1.1$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-an-overshooting-parameter-alpha--15&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting150_length.png&#34; alt=&#34;Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for a square domain with an overshooting parameter $\alpha = 1.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with an overshooting parameter $\alpha = 1.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-an-overshooting-parameter-alpha--05&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting050_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for a square domain with an overshooting parameter $\alpha = 0.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with an overshooting parameter $\alpha = 0.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-an-overshooting-parameter-alpha--11&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting110_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for a square domain with an overshooting parameter $\alpha = 1.1$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with an overshooting parameter $\alpha = 1.1$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-an-overshooting-parameter-alpha--15&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting150_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for a square domain with an overshooting parameter $\alpha = 1.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with an overshooting parameter $\alpha = 1.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Thus, if the parameter is too large, the Lloyd&amp;rsquo;s iteration may not converge. To prevent this problem, we can impose a rather small $\alpha$ parameter (at least strictly less than $1$, less than $0.5$ if we want to be sure to converge even in particular cases).&lt;/p&gt;
&lt;p&gt;Another risk is that if the $\alpha$ parameter is too large, Lloyd&amp;rsquo;s iteration will take the generators out of the domain. To overcome this problem, we could use the &lt;em&gt;inside&lt;/em&gt; function of &lt;em&gt;cdt.h&lt;/em&gt; file, to apply overshooting to the relocation of a generator only if the new generator is inside the domain.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;temporal-inertia&#34;&gt;Temporal inertia&lt;/h2&gt;
&lt;p&gt;Temporal inertia consists in keeping in memory the centers of mass of the previous iterations, and moving the generators with a linear combination of the previous $n$ displacement vectors. Informally, we move a generator at iteration $N$ according to the vector $v_{N+1} = \sum_{i=N-n}^N w_i v_i$, where the $w_i$ are weights given by the user to the relocation vectors of the previous iterations.&lt;/p&gt;
&lt;p&gt;There are three classes of Lloyd&amp;rsquo;s iteration behaviour according to the value of the different coefficients (classes determined approximately by numerical testing, the limit values of these behaviours change in particular according to the shape of the domain and the initialization of the generators) for the square domain and for a uniform initialization, with a number $n = 5$ of displacement vectors taken into account:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_{i=N-n}^N w_i \leq 1$: the previous relocation vectors are relatively little taken into account: convergence is accelerated because the generators move faster, since the first iterations concern larger displacements&lt;/li&gt;
&lt;li&gt;$\sum_{i=N-n}^N w_i \approx 1 + \epsilon$: the previous relocation vectors are a bit too much taken into account, and the generators move a bit too much: their positions alternate between $2$ configurations (oscillating behavior)&lt;/li&gt;
&lt;li&gt;$\sum_{i=N-n}^N w_i &amp;gt; &amp;gt; 1+\epsilon$: the first relocation vectors are much too much taken into account, and the addition of vectors with too large coefficients produces larger and larger vectors: the Lloyd&amp;rsquo;s iteration diverges and the generators leave the domain&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--015&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_015_image.png&#34; alt=&#34;Visualization of the cells after convergence with temporal inertia of the last 5 displacements, with weights $w_i = 0.15$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence with temporal inertia of the last 5 displacements, with weights $w_i = 0.15$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--030&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_030_image.png&#34; alt=&#34;Visualization of the cells after convergence with temporal inertia of the last 5 displacements, with weights $w_i = 0.30$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence with temporal inertia of the last 5 displacements, with weights $w_i = 0.30$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-visualization-of-the-cells-after-convergence-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--050&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_050_image.png&#34; alt=&#34;Visualization of the cells after convergence with temporal inertia of the last 5 displacements, with weights $w_i = 0.50$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualization of the cells after convergence with temporal inertia of the last 5 displacements, with weights $w_i = 0.50$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--015&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_015_length.png&#34; alt=&#34;Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.15$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.15$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--030&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_030_length.png&#34; alt=&#34;Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.30$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.30$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-mean-norm-of-the-relocation-vectors-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--050&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_050_length.png&#34; alt=&#34;Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;#39;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.50$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the mean norm of the relocation vectors as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.50$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--015&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_015_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.15$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.15$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--030&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_030_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.30$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.30$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-evolution-of-the-average-energy-of-the-cells-as-a-function-of-the-number-of-lloyds-iterations-for-a-square-domain-with-temporal-inertia-of-the-last-5-displacements-with-weights-w_i--050&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_050_energy.png&#34; alt=&#34;Evolution of the average energy of the cells as a function of the number of Lloyd&amp;#39;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.50$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution of the average energy of the cells as a function of the number of Lloyd&amp;rsquo;s iterations for a square domain with temporal inertia of the last 5 displacements, with weights $w_i = 0.50$.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Convergence is faster when the coefficients are not taken too large. Indeed, taking into account the &amp;ldquo;large&amp;rdquo; vectors of previous displacements allows the generators to move faster.&lt;/p&gt;
&lt;p&gt;However, similarly to the overshooting parameter being too large, too large coefficients cause the algorithm to diverge (and even with small coefficients, convergence with slight oscillations can be seen in the middle image above). To overcome this, we impose small coefficients, or we check that the generators stay inside the domain with &lt;em&gt;inside&lt;/em&gt; parameter.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

# Vecteurs de relocalisation

## Calcul

La visualisation des vecteurs de relocalisation s&#39;obtient en traçant les segments entre les générateurs avant et après itération de Lloyd. Le résultat obtenu est présenté Figure $1$, où l&#39;on a tracé en rouge les générateurs des cellules à l&#39;itération précédente.

|      |      |
|------|------|
|















&lt;figure  id=&#34;figure-visualisation-des-vecteurs-de-relocalisation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q2_relocation.png&#34; alt=&#34;Visualisation des vecteurs de relocalisation.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des vecteurs de relocalisation.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_uniform_image.png&#34; alt=&#34;Visualisation des cellules après convergence.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

Pour le tracé des courbes de convergence, nous avons sauvegardé les différentes valeurs dans des fichiers .txt et traité les données en langage Python, avec la bibliothèque matplotlib.pyplot. Les résultats obtenus sont présentés en Figure $2$. On constate que cette évolution est globalement décroissance, avec une augmentation de l&#39;énergie lorsque les cellules se &#34;débloquent&#34; après avoir convergé vers un minimum local d&#39;énergie.

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-en-échelle-logarithmique-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-uniforme&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_uniform_length.png&#34; alt=&#34;Evolution en échelle logarithmique de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation **uniforme**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution en échelle logarithmique de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation &lt;strong&gt;uniforme&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-en-échelle-logarithmique-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-dans-un-coin&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_coin_length.png&#34; alt=&#34;Evolution en échelle logarithmique de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation **dans un coin**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution en échelle logarithmique de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation &lt;strong&gt;dans un coin&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-en-échelle-logarithmique-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-sur-une-ligne&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_line_length.png&#34; alt=&#34;Evolution en échelle logarithmique de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation **sur une ligne**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution en échelle logarithmique de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation &lt;strong&gt;sur une ligne&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

En zoomant la première courbe de convergence entre $30$ et $60$ itérations et en la traçant en échelle réelle (et non pas en échelle logarithmique comme sur les différentes images), il apparaît clairement que la convergence de la longueur moyenne est quadratique (Figure $3$).

















&lt;figure  id=&#34;figure-visualisation-de-léchantillonnage-du-domaine-carré-avec-n--10-000-points&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_regression.png&#34; alt=&#34;Visualisation de l&amp;#39;échantillonnage du domaine carré avec $N = 10 000$ points.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation de l&amp;rsquo;échantillonnage du domaine carré avec $N = 10 000$ points.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

# Distribution des énergies par cellule

## Calcul

[//]: # (L&#39;énergie globale d&#39;une partition de domaine en $N_c$ cellules $\mathcal{C}_i$ est donnée par la formule suivante :)

$$E_{dom} = \sum_{i=1}^{N_c} E_{cell} = \sum_{i=1}^{N_c} \int_{x \in \mathcal{C}_i} ||x-x_i||^2 dx$$

Pour réaliser le calcul des intégrales, on procède par méthode de Monte-Carlo, en échantillonnant un nombre $N$ de points $x_j$ sur l&#39;ensemble du domaine et en approximant l&#39;intégrale par une somme discrète :

$$E_{cell} = \int_{x \in \mathcal{C}_i} ||x-x_i||^2 dx = \sum_{x_j \in \mathcal{C}_i} ||x_j-x_i||^2$$

Une représentation de cet échantillonnage est réalisée en Figure $4$ pour $N = 10 000$. Pour assez couvrir entièrement le domaine, nous avons choisi $N = 100 000$.

















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-en-forme-de-sablier&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q4_sampling.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine en forme de sablier.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine en forme de sablier.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

## Convergence

Toujours dans le domaine carré avec une initialisation de $10$ points, l&#39;évolution de l&#39;énergie des cellules est représentée Figure $5$. Pour les $3$ initialisations testées, on constate que les cellules convergent vers une énergie proche après une trentaine d&#39;itérations. Cette convergence est rapide pour les initialisations uniforme et sur une ligne, avec une légère correction d&#39;énergie à environ 23 itérations pour l&#39;initialisation uniforme, en raison des cellules qui se bloquaient (ce que l&#39;on retrouve avec la croissance de la norme moyenne des vecteurs de relocalisation autour du même nombre d&#39;itérations).

Pour l&#39;initialisation dans un coin, la convergence est plus lente car plus d&#39;itérations sont nécessaires pour permettre aux $10$ générateurs d&#39;occuper l&#39;espace entier du domaine. Cependant, on ne constate pas de saut majeur d&#39;énergie qui serait dû à un blocage de cellules entre elles.

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-uniforme&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_uniform_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation **uniforme**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation &lt;strong&gt;uniforme&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-dans-un-coin&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_coin_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation **dans un coin**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation &lt;strong&gt;dans un coin&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-sur-une-ligne&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q3_line_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation **sur une ligne**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation &lt;strong&gt;sur une ligne&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

## Influence du domaine

La forme du domaine influence beaucoup les résultats de convergence. Pour le domaine circulaire par exemple, qui est un domaine &#34;régulier&#34; dans le sens où les bords ne présentent pas de &#34;coins&#34; ou de rupture brutale de direction (les bords sont bien continus et dérivables), la convergence se fait sans problème au bout d&#39;une dizaine d&#39;itérations et n&#39;est pas caractérisée par des sauts particuliers. Les résultats pour le domaine circulaire sont présentés Figure $6$.

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-circulaire&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_circle_image.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine circulaire.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine circulaire.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-évolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-dans-un-domaine-circulaire-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-uniforme&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_circle_length.png&#34; alt=&#34;Évolution de la norme moyenne des vecteurs de relocalisation dans un domaine circulaire en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation uniforme.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Évolution de la norme moyenne des vecteurs de relocalisation dans un domaine circulaire en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation uniforme.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-évolution-de-lénergie-moyenne-des-cellules-dans-un-domaine-circulaire-en-fonction-du-nombre-ditérations-de-lloyd-pour-une-initialisation-uniforme&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_circle_energy.png&#34; alt=&#34;Évolution de l&amp;#39;énergie moyenne des cellules dans un domaine circulaire en fonction du nombre d&amp;#39;itérations de Lloyd pour une initialisation uniforme.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Évolution de l&amp;rsquo;énergie moyenne des cellules dans un domaine circulaire en fonction du nombre d&amp;rsquo;itérations de Lloyd pour une initialisation uniforme.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

Pour les autres domaines fournis, on constate des comportements plus originaux, dus aux mêmes observations que formulées précédemment. Pour la forme en L, le coin intérieur du L va provoquer quelques augmentations de la longueur moyenne, les cellules &#34;franchissant&#34; le coin se déplaçant rapidement d&#39;un coup pour s&#39;adapter à la forme, et la convergence est assez lente relativement aux formes carrée et circulaire.

Pour le domaine en forme de clé, la convergence est aussi plus lente en raison de la complexité du domaine, et les sauts de longueur moyenne sont plus importants en raison de cette complexité, car les cellules se déplacent beaucoup lorsqu&#39;un &#34;coin&#34; est franchi et comblé par les cellules. L&#39;ensemble des résultats est présenté dans les Figures $7$ à $9$.

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-en-l&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_lform_image.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine **en L**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine &lt;strong&gt;en L&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-comportant-une-croix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_cross_image.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine **comportant une croix**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine &lt;strong&gt;comportant une croix&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-en-forme-de-clé&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_key_image.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine **en forme de clé**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine &lt;strong&gt;en forme de clé&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-dans-un-domaine-en-l&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_lform_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd dans un domaine **en L**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd dans un domaine &lt;strong&gt;en L&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-dans-un-domaine-comportant-une-croix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_cross_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd dans un domaine **comportant une croix**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd dans un domaine &lt;strong&gt;comportant une croix&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-dans-un-domaine-en-forme-de-clé&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_key_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd dans un domaine **en forme de clé**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd dans un domaine &lt;strong&gt;en forme de clé&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-dans-un-domaine-en-l&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_lform_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd dans un domaine **en L**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd dans un domaine &lt;strong&gt;en L&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-dans-un-domaine-comportant-une-croix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_cross_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd dans un domaine **comportant une croix**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd dans un domaine &lt;strong&gt;comportant une croix&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-dans-un-domaine-en-forme-de-clé&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_key_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd dans un domaine **en forme de clé**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd dans un domaine &lt;strong&gt;en forme de clé&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

Pour les domaines en forme de sablier (codées en dur dans le code C++ en fonction d&#39;un paramètre correspondant à la taille du goulet d&#39;étranglement), cette difficulté pour l&#39;itération de Lloyd à franchir des coins ou des passages étroits est nettement mise en évidence. On constate en effet que, de façon générale, plus le goulet d&#39;étranglement est étroit, plus les cellules mettent du temps à franchir le goulet d&#39;étranglement, et donc plus la remontée de longueur moyenne est importante lorsqu&#39;il est franchi (Figure $11$).

**Initialisation:** L&#39;observation sur l&#39;ensemble des domaines nous amène à penser que pour des domaines assez simples et réguliers, une initialisation uniforme est assez efficace. Pour des formes de domaines plus complexes, il convient d&#39;initialiser l&#39;algorithme avec plus de générateurs aux bords irréguliers du domaine, ou aux endroits présentant des passages étroits.

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-en-forme-de-sablier-avec-goulet-détranglement-de-largeur-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass001_image.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine en forme de sablier avec goulet d&amp;#39;étranglement de largeur **1**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine en forme de sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;1&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-en-forme-de-sablier-avec-goulet-détranglement-de-largeur-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass003_image.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine en forme de sablier avec goulet d&amp;#39;étranglement de largeur **3**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine en forme de sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;3&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-dans-un-domaine-en-forme-de-sablier-avec-goulet-détranglement-de-largeur-5&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass005_image.png&#34; alt=&#34;Visualisation des cellules après convergence dans un domaine en forme de sablier avec goulet d&amp;#39;étranglement de largeur **5**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence dans un domaine en forme de sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;5&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-en-sablier-avec-goulet-détranglement-de-largeur-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass001_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;#39;étranglement de largeur **1**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;1&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-en-sablier-avec-goulet-détranglement-de-largeur-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass003_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;#39;étranglement de largeur **3**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;3&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-en-sablier-avec-goulet-détranglement-de-largeur-5&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass005_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;#39;étranglement de largeur **5**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;5&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-en-sablier-avec-goulet-détranglement-de-largeur-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass001_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;#39;étranglement de largeur **1**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;1&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-en-sablier-avec-goulet-détranglement-de-largeur-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass003_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;#39;étranglement de largeur **3**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;3&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-en-sablier-avec-goulet-détranglement-de-largeur-5&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q5_sandglass005_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;#39;étranglement de largeur **5**.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine en sablier avec goulet d&amp;rsquo;étranglement de largeur &lt;strong&gt;5&lt;/strong&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

# Dépassement des centres (overshooting) et inertie temporelle

## Overshooting

L&#39;overshooting consiste à déplacer les générateurs non pas aux centres de masse des cellules, mais un peu au-delà. Informatiquement, on considère le vecteur de déplacement $v_N$ à l&#39;itération $N$ d&#39;une itération de Lloyd classique, et au lieu de déplacer le générateur selon $v_N$, on le déplace selon $v_{N+1} = (1+\alpha)v_N$, où $\alpha \in \mathbb{R}^{+}$.

On distingue trois classes de comportement de l&#39;itération de Lloyd selon la valeur du paramètre d&#39;overshooting (déterminées de façon approximative en testant numériquement, les valeurs limites de ces comportements changent notamment selon la forme du domaine et l&#39;initialisation des générateurs) pour le domaine carré et pour une initialisation uniforme (Figures $13$ à $15$) :

$\alpha \in [0,1]$ : la paramètre d&#39;overshooting est assez petit : il permet d&#39;accélérer la convergence de l&#39;itération de Lloyd
- $\alpha \approx [1, 1 + \epsilon]$ : le paramètre d&#39;overshooting est un peu trop grand : l&#39;itération aboutit à une situation où $(1+\alpha)v_{N+1} = -(1+\alpha)v_N$, ce qui entraîne l&#39;alternance des générateurs entre deux configurations
- $\alpha &gt; &gt; 1+\epsilon$ : le paramètre d&#39;overshooting est beaucoup trop grand : l&#39;itération de Lloyd diverge et a un comportement chaotique

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-avec-un-paramètre-dovershooting-avec-un-paramètre-dovershooting-alpha--05&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting050_image.png&#34; alt=&#34;Visualisation des cellules après convergence avec un paramètre d&amp;#39;overshooting avec un paramètre d&amp;#39;overshooting $\alpha = 0.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence avec un paramètre d&amp;rsquo;overshooting avec un paramètre d&amp;rsquo;overshooting $\alpha = 0.5$.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-avec-un-paramètre-dovershooting-avec-un-paramètre-dovershooting-alpha--11&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting110_image.png&#34; alt=&#34;Visualisation des cellules après convergence avec un paramètre d&amp;#39;overshooting avec un paramètre d&amp;#39;overshooting $\alpha = 1.1$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence avec un paramètre d&amp;rsquo;overshooting avec un paramètre d&amp;rsquo;overshooting $\alpha = 1.1$.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-avec-un-paramètre-dovershooting-avec-un-paramètre-dovershooting-alpha--15&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting150_image.png&#34; alt=&#34;Visualisation des cellules après convergence avec un paramètre d&amp;#39;overshooting avec un paramètre d&amp;#39;overshooting $\alpha = 1.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence avec un paramètre d&amp;rsquo;overshooting avec un paramètre d&amp;rsquo;overshooting $\alpha = 1.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-un-paramètre-dovershooting-alpha--05&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting050_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;#39;overshooting $\alpha = 0.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;rsquo;overshooting $\alpha = 0.5$.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-un-paramètre-dovershooting-alpha--11&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting110_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;#39;overshooting $\alpha = 1.1$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;rsquo;overshooting $\alpha = 1.1$.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-un-paramètre-dovershooting-alpha--15&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting150_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;#39;overshooting $\alpha = 1.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;rsquo;overshooting $\alpha = 1.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-un-paramètre-dovershooting-alpha--05&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting050_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;#39;overshooting $\alpha = 0.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;rsquo;overshooting $\alpha = 0.5$.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-un-paramètre-dovershooting-alpha--11&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting110_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;#39;overshooting $\alpha = 1.1$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;rsquo;overshooting $\alpha = 1.1$.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-un-paramètre-dovershooting-alpha--15&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q7_overshooting150_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;#39;overshooting $\alpha = 1.5$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec un paramètre d&amp;rsquo;overshooting $\alpha = 1.5$.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

Ainsi, si le paramètre est trop important, l&#39;itération de Lloyd risque de ne pas converger. Pour prévenir ce problème, on peut imposer un paramètre $\alpha$ assez petit (au moins strictement inférieur à 1, inférieur à $0.5$ si l&#39;on veut être sûr de converger y compris dans des cas particuliers).

Un autre risque est que si le paramètre $\alpha$ est trop grand, l&#39;itération de Lloyd fasse sortir les générateurs du domaine. Pour pallier ce problème informatiquement, on pourrait utiliser la fonction inside de cdt.h, pour appliquer l&#39;overshooting à la relocalisation d&#39;un générateur seulement si le nouveau générateur est bien à l&#39;intérieur (inside) du domaine.

&amp;nbsp;

## Inertie temporelle

L&#39;inertie temporelle consiste à garder en mémoire les centres de masse des itérations précédentes, et à déplacer les générateurs avec une combinaison linéaire des $n$ vecteurs déplacements précédents. Informatiquement, on déplace un générateur à l&#39;itération $N$ selon le vecteur $v_{N+1} = \sum_{i=N-n}^N w_i v_i$, où les $w_i$ sont des poids données par l&#39;utilisateur aux vecteurs de relocalisation des itérations précédentes.

On distingue trois classes de comportement de l&#39;itération de Lloyd selon la valeur des différents coefficients (classes déterminées de façon approximative en testant numériquement, les valeurs limites de ces comportements changent notamment selon la forme du domaine et l&#39;initialisation des générateurs) pour le domaine carré et pour une initialisation uniforme, avec un nombre $n = 5$ de vecteurs déplacements pris en compte (Figures $16$ à $18$):

- $\sum_{i=N-n}^N w_i \leq 1$ : les précédents vecteurs de relocalisation sont relativement peu pris en compte : la convergence est accélérée car les générateurs se déplacent plus vite, puisque les premières itérations concernent des déplacements plus importants
- $\sum_{i=N-n}^N w_i \approx 1 + \epsilon$ : les précédents vecteurs de relocalisation sont un peu trop pris en compte, et les générateurs se déplacent un peu trop : leurs positions alternent entre 2 configurations (comportement oscillant)
- $\sum_{i=N-n}^N w_i &gt; &gt; 1+\epsilon$ : les premiers vecteurs de relocalisation sont beaucoup trop pris en compte, et l&#39;addition des vecteurs avec de trop grands coefficients produit des vecteurs de plus en plus grands : l&#39;itération de Lloyd diverge et les générateurs sortent du domaine.

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--015&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_015_image.png&#34; alt=&#34;Visualisation des cellules après convergence avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.15$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.15$.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--030&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_030_image.png&#34; alt=&#34;Visualisation des cellules après convergence avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.30$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.30$.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-visualisation-des-cellules-après-convergence-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--050&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_050_image.png&#34; alt=&#34;Visualisation des cellules après convergence avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.50$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Visualisation des cellules après convergence avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.50$.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--015&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_015_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.15$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.15$.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--030&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_030_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.30$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.30$.
    &lt;/figcaption&gt;&lt;/figure&gt;| 















&lt;figure  id=&#34;figure-evolution-de-la-norme-moyenne-des-vecteurs-de-relocalisation-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--050&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_050_length.png&#34; alt=&#34;Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.50$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de la norme moyenne des vecteurs de relocalisation en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.50$.
    &lt;/figcaption&gt;&lt;/figure&gt;|

|      |      |      |
|------|------|------|
|















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--015&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_015_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.15$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.15$.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--030&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_030_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.30$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.30$.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-evolution-de-lénergie-moyenne-des-cellules-en-fonction-du-nombre-ditérations-de-lloyd-pour-un-domaine-carré-avec-inertie-temporelle-des-5-derniers-déplacements-avec-des-poids-w_i--050&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/lloyd/q8_inertia_050_energy.png&#34; alt=&#34;Evolution de l&amp;#39;énergie moyenne des cellules en fonction du nombre d&amp;#39;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.50$.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Evolution de l&amp;rsquo;énergie moyenne des cellules en fonction du nombre d&amp;rsquo;itérations de Lloyd pour un domaine carré avec inertie temporelle des 5 derniers déplacements, avec des poids $w_i = 0.50$.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

La convergence est plus rapide lorsque les coefficients ne sont pas pris trop grands. En effet, la prise de comptes des &#34;grands&#34; vecteurs de déplacements précédents permet aux générateurs de se déplacer plus vite.

Cependant, de façon analogue au paramètre d&#39;overshooting trop grand, des coefficients trop grands provoquent une divergence de l&#39;algorithme (et même avec des coefficients petits, on constate une convergence avec de légères oscillations sur l&#39;image de gauche en Figure $17$). Pour pallier cela, on impose des coefficients petits, ou on vérifie que les générateurs restent à l&#39;intérieur du domaine avec le paramètre *inside*.

&amp;nbsp;

---&gt;
</description>
    </item>
    
    <item>
      <title>★ Automated Text Matching</title>
      <link>https://michaelkarpe.github.io/optimization-projects/textmatching/</link>
      <pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/optimization-projects/textmatching/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;presentation-of-the-project&#34;&gt;Presentation of the project&lt;/h1&gt;
&lt;p&gt;In the field of translationology, there are many approaches and algorithms for making translations from one language to another. Each method has its advantages and disadvantages, and a compromise between several methods is usually needed to achieve an efficient translation algorithm.&lt;/p&gt;
&lt;p&gt;We are interested here in an algorithm for aligning texts in different languages based on dynamic time warping (DTW). Such an algorithm does not require either prior bilingual resources or knowledge of similarities between the languages studied.&lt;/p&gt;
&lt;p&gt;It can thus work on any language pair, especially on languages for which few linguistic resources are available. On the other hand, its accuracy in translation is low, so we will focus mainly on aligning paragraphs rather than words.&lt;/p&gt;
&lt;p&gt;The approach used in Kim Gerdes&amp;rsquo; algorithm, although simple at first glance, is no less original. It consists in working on the occurrences and position of words in the texts under consideration using the DTW.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;paragraph-alignment&#34;&gt;Paragraph alignment&lt;/h1&gt;
&lt;p&gt;The application of DTW in a translation system can make bad associations, if one tries to be too precise, due to different syntaxes within languages. The algorithm proposed by Gerdes therefore uses the sum of the word alignments to align paragraphs, thus eliminating possible spurious signals due to erroneous word associations.&lt;/p&gt;
&lt;p&gt;Length-based alignment is an alignment method, not based on the DTW, which allows a first alignment of paragraphs based solely on the length of the paragraphs.&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/alignbylength.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;jaro-winkler-distance&#34;&gt;Jaro-Winkler distance&lt;/h2&gt;
&lt;p&gt;To complete the DTW distance, we use the Jaro-Winkler syntax distance between $2$ strings $d_w$ , depending on the Jaro distance $d_j$:&lt;/p&gt;
&lt;p&gt;$$ d_w = d_j+lp(1-d_j) \qquad \text{ with } \qquad d_j = \frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m-t}{m}\right) $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$|s_{i}|$ is the length of the string $s_{i}$&lt;/li&gt;
&lt;li&gt;$m$ is the number of corresponding characters&lt;/li&gt;
&lt;li&gt;$t$ is the number of transpositions&lt;/li&gt;
&lt;li&gt;$l$ is the length of the common prefix (maximum $4$ characters)&lt;/li&gt;
&lt;li&gt;$p$ is a coefficient that allows to favor strings with a common prefix (Winkler proposes for value $p = 0.1$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;dynamic-time-warping-dtw&#34;&gt;Dynamic Time Warping (DTW)&lt;/h2&gt;
&lt;p&gt;In general, DTW algorithms seek to find the optimal monotonic (non-intersecting) alignment of two sequences of variable length. Monotonic means that the order of the sequence positions before and after DTW is respected, but that the distance between these positions may change. \newline&lt;/p&gt;
&lt;p&gt;DTW algorithms are dynamic programming algorithms. For our paragraph alignment, it is applied on recency vectors $(p_1, p_2-p_1, \dots, p_n-p_{n-1}, 1-p_n)$ where $p_i$ is the position of the $i$ occurrence (in text fraction) of the considered $p$ word.&lt;/p&gt;
&lt;p&gt;$$ W_{i+1,j+1} = \left| r_1-r_2 \right| + \min\left(W_{i,j+1}, W_{i+1,j}, W_{i,j}\right) $$&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/dtw.PNG&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;integration-in-an-alignment-system&#34;&gt;Integration in an alignment system&lt;/h3&gt;
&lt;p&gt;We describe here, in a synthetic way, the global algorithm performing the automatic matching of texts in different languages without prior knowledge, including the DTW algorithm explained above. This algorithm, described in detail by Kim Gerdes, can be synthesized in $4$ main steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reading and cleaning of texts: suppression of parasitic signals (accents, extra spaces, punctuation&amp;hellip;)&lt;/li&gt;
&lt;li&gt;Construction of hash tables associating words with their different characteristics (number of occurrences, appearance indices&amp;hellip;)&lt;/li&gt;
&lt;li&gt;Calculation for each of the texts of the internal cognates of the language (Jaro-Winkler distance), significant words or groups of words (frequent and well distributed in the text).&lt;/li&gt;
&lt;li&gt;Application of the DTW algorithm for the alignment of words, groups of words and paragraphs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;execution-of-the-alignment-system&#34;&gt;Execution of the alignment system&lt;/h1&gt;
&lt;p&gt;The alignment system described above was developed in C++ and applied to the Universal Declaration of Human Rights (UDHR) in the following languages: English, French, Spanish, German, Russian. We present here the results obtained for the French-English language pair.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;French&lt;/th&gt;
&lt;th&gt;English&lt;/th&gt;
&lt;th&gt;DTW&lt;/th&gt;
&lt;th&gt;French&lt;/th&gt;
&lt;th&gt;English&lt;/th&gt;
&lt;th&gt;DTW&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;respect&lt;/td&gt;
&lt;td&gt;respect&lt;/td&gt;
&lt;td&gt;0.0039&lt;/td&gt;
&lt;td&gt;nations&lt;/td&gt;
&lt;td&gt;nations&lt;/td&gt;
&lt;td&gt;0.0228&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ces&lt;/td&gt;
&lt;td&gt;these&lt;/td&gt;
&lt;td&gt;0.0046&lt;/td&gt;
&lt;td&gt;conscience&lt;/td&gt;
&lt;td&gt;conscience&lt;/td&gt;
&lt;td&gt;0.0245&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;déclaration&lt;/td&gt;
&lt;td&gt;declaration&lt;/td&gt;
&lt;td&gt;0.0070&lt;/td&gt;
&lt;td&gt;dignité&lt;/td&gt;
&lt;td&gt;dignity&lt;/td&gt;
&lt;td&gt;0.0306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sans&lt;/td&gt;
&lt;td&gt;without&lt;/td&gt;
&lt;td&gt;0.0107&lt;/td&gt;
&lt;td&gt;religion&lt;/td&gt;
&lt;td&gt;religion&lt;/td&gt;
&lt;td&gt;0.0326&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;considérant&lt;/td&gt;
&lt;td&gt;whereas&lt;/td&gt;
&lt;td&gt;0.0115&lt;/td&gt;
&lt;td&gt;éducation&lt;/td&gt;
&lt;td&gt;education&lt;/td&gt;
&lt;td&gt;0.0375&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nationalité&lt;/td&gt;
&lt;td&gt;nationality&lt;/td&gt;
&lt;td&gt;0.0116&lt;/td&gt;
&lt;td&gt;contre&lt;/td&gt;
&lt;td&gt;against&lt;/td&gt;
&lt;td&gt;0.0383&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;libertés&lt;/td&gt;
&lt;td&gt;freedoms&lt;/td&gt;
&lt;td&gt;0.0124&lt;/td&gt;
&lt;td&gt;article&lt;/td&gt;
&lt;td&gt;article&lt;/td&gt;
&lt;td&gt;0.0545&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;famille&lt;/td&gt;
&lt;td&gt;family&lt;/td&gt;
&lt;td&gt;0.0150&lt;/td&gt;
&lt;td&gt;droits&lt;/td&gt;
&lt;td&gt;rights&lt;/td&gt;
&lt;td&gt;0.0714&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;société&lt;/td&gt;
&lt;td&gt;society&lt;/td&gt;
&lt;td&gt;0.0152&lt;/td&gt;
&lt;td&gt;présente&lt;/td&gt;
&lt;td&gt;declaration&lt;/td&gt;
&lt;td&gt;0.0849&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;unies&lt;/td&gt;
&lt;td&gt;united&lt;/td&gt;
&lt;td&gt;0.0213&lt;/td&gt;
&lt;td&gt;religion&lt;/td&gt;
&lt;td&gt;property&lt;/td&gt;
&lt;td&gt;0.0979&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;analysis-of-the-results&#34;&gt;Analysis of the results&lt;/h2&gt;
&lt;p&gt;After applying the DTW algorithm to different languages, we found that word matching was effective when the DTW value returned for a word pair was less than $0.1$ (this value having been normalized to be between $0$ and 1).&lt;/p&gt;
&lt;p&gt;For the French and English language pair, we find erroneous associations for DTW values greater than $0.08$, notably due to the different syntax of the languages (see below). It is therefore necessary to include the Jaro-Winkler distance in the alignment system to correct this problem.&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/piege.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Once the Jaro-Winkler distance is included in the alignment system, we can combine these word associations to proceed with the UDHR paragraph association. The UDHR consists of $89$ paragraphs, for a total of $2,106$ words and $11,663$ characters.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-déclaration-universelle-des-droits-de-lhomme-dudh&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/dudh.jpg&#34; alt=&#34;Déclaration Universelle des Droits de l&amp;#39;Homme (DUDH).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;83%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Déclaration Universelle des Droits de l&amp;rsquo;Homme (DUDH).
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-universal-declaration-of-human-rights-udhr&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/udhr.jpg&#34; alt=&#34;Universal Declaration of Human Rights (UDHR).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Universal Declaration of Human Rights (UDHR).
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Reciprocal paragraph associations (dashes on vertical lines) are represented by solid arrows, while dashed arrows represent one-way associations.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-alignment-by-length&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/alignlongueur.png&#34; alt=&#34;Alignment by length.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Alignment by length.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-alignment-with-dtw&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/alignDTW.png&#34; alt=&#34;Alignment with DTW.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Alignment with DTW.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;alignment-system-limits&#34;&gt;Alignment system limits&lt;/h2&gt;
&lt;p&gt;The main limitation of the developed alignment system lies in the accuracy of the alignment, limited to a paragraph scale. The DTW algorithm alone is not sufficient to achieve an efficient alignment, and the addition of a syntax distance such as Jaro-Winkler is necessary to improve the results. The results obtained remain very satisfying.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Kim Gerdes. &lt;em&gt;L’alignement pour les pauvres : Adapter la bonne métrique pour unalgorithme dynamique de dilatation temporelle pour l’alignement sans ressources de corpus bilingues.&lt;/em&gt; 9e Journées internationales d’Analyse statistique des Données Textuelles, Mars 2008.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Meinard Müller. &lt;em&gt;Information retrieval for music and motion.&lt;/em&gt; Chapter 4 : Dynamic Time Warping, Septembre 2007.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

&amp;nbsp;

# Présentation du projet

Dans le domaine de la traductologie, il existe de nombreuses approches et de nombreux algorithmes pour réaliser des traductions d&#39;une langue à une autre. Chaque méthode possède ses avantages et ses inconvénients, et il convient généralement d&#39;utiliser un compromis entre plusieurs méthodes pour obtenir un algorithme de traduction efficace.

Nous nous intéressons ici à un algorithme d&#39;alignement de textes en langues différentes basé sur la déformation (ou dilatation) temporelle dynamique (ou *Dynamic Time Warping, DTW*). Un tel algorithme n&#39;a besoin ni de ressources bilingues préalables, ni de connaissances de similarités entre les langues étudiées.

Il peut ainsi fonctionner sur n&#39;importe quel couple de langues, et notamment sur des langues pour lesquelles on possède peu de ressources linguistiques. En revanche, sa précision dans la traduction est faible ; ainsi nous nous concentrerons principalement sur l&#39;alignement de paragraphes plutôt que de mots.

L&#39;approche employée dans l&#39;algorithme de Kim Gerdes, bien que simple à première vue, n&#39;en est pas moins originale. Elle consiste à travailler sur les occurrences et la position des mots dans les textes considérés en employant le DTW.

&amp;nbsp;

# Alignement de paragraphes

L&#39;application du DTW dans un système de traduction peut réaliser de mauvaises associations, si l&#39;on cherche à être trop précis, en raison de syntaxes différentes au sein des langues. Il s&#39;agit donc, dans l&#39;algorithme proposé par Gerdes, de se servir de la somme des alignements de mots pour procéder à l&#39;alignement de paragraphes, et ainsi faire disparaître les éventuels signaux parasites dus à des associations erronées de mots.

L&#39;alignement par longueur est une méthode d&#39;alignement, non basée sur le DTW, qui permet de réaliser un premier alignement de paragraphes uniquement basé sur la longueur des paragraphes.

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/alignbylength.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

## Distance de Jaro-Winkler

Pour compléter la distance DTW, on utilise la distance syntaxique de Jaro-Winkler entre $2$ chaînes de caractères $d_w$, fonction de la distance de Jaro $d_j$ :

$$ d_w = d_j+lp(1-d_j) \qquad \text{ avec } \qquad d_j = \frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m-t}{m}\right) $$

- $|s_{i}|$ est la longueur de la chaîne de caractères $s_{i}$
- $m$ est le nombre de caractères correspondants
- $t$ est le nombre de transpositions
- $l$ est la longueur du préfixe commun (maximum $4$ caractères)
- $p$ est un coefficient qui permet de favoriser les chaînes avec un préfixe commun (Winkler propose pour valeur $p = 0.1$)

&amp;nbsp;

## Dynamic Time Warping (DTW)

De façon générale, les algorithmes DTW &#34;cherchent à trouver l’alignement monotone (sans croisement) optimal de deux séquences de longueur variable.&#34; Par monotone, on entend que l&#39;ordre des positions des séquences avant et après DTW est respecté, mais que l&#39;écart entre ces positions peut changer. \newline

Les algorithmes DTW sont des algorithmes de programmation dynamique. Pour notre alignement de paragraphes, il est appliqué sur des vecteurs de récence $(p_1, p_2-p_1, \dots, p_n-p_{n-1}, 1-p_n)$ où $p_i$ est la position de l&#39;occurrence $i$ (en fraction de texte) du mot $p$ considéré.

$$ W_{i+1,j+1} = \left| r_1-r_2 \right| + \min\left(W_{i,j+1}, W_{i+1,j}, W_{i,j}\right) $$

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/dtw.PNG&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

### Intégration dans un système d&#39;alignement

Nous décrivons ici, de façon synthétique, l&#39;algorithme global réalisant la mise en correspondance automatique de textes en langues différentes sans connaissances préalables, incluant l&#39;algorithme DTW expliqué ci-dessus. Cet algorithme, décrit en détails par Kim Gerdes, peut être synthétisé en $4$ grandes étapes :

- Lecture et nettoyage des textes : suppression des signaux parasites (accents, espaces en trop, ponctuation...)
- Construction des tables de hachage associant les mots à leurs différentes caractéristiques (nombre d&#39;occurrences, indices d&#39;apparition...)
- Calcul pour chacun des textes des cognats internes à la langue (distance de Jaro-Winkler), des mots ou groupes de mots significatifs (fréquents et bien répartis dans le texte)
- Application de l&#39;algorithme DTW pour l&#39;alignement des mots, des groupes de mots et des paragraphes

&amp;nbsp;

# Exécution du système d&#39;alignement

Le système d&#39;alignement décrit précédemment a été développé en langage C++ et appliqué sur la Déclaration Universelle des Droits de l&#39;Homme (DUDH ou *Universal Declaration of Human Rights, UDHR*) dans les langues suivantes : français, anglais, espagnol, allemand, russe. Nous présentons ici les résultats obtenus pour le couple de langues français-anglais.

| Français    | Anglais     | DTW    | Français    | Anglais     | DTW    |
|-------------|-------------|--------|-------------|-------------|--------|
| respect     | respect     | 0.0039 | nations     | nations     | 0.0228 |
| ces         | these       | 0.0046 | conscience  | conscience  | 0.0245 |
| déclaration | declaration | 0.0070 | dignité     | dignity     | 0.0306 |
| sans        | without     | 0.0107 | religion    | religion    | 0.0326 |
| considérant | whereas     | 0.0115 | éducation   | education   | 0.0375 |
| nationalité | nationality | 0.0116 | contre      | against     | 0.0383 |
| libertés    | freedoms    | 0.0124 | article     | article     | 0.0545 |
| famille     | family      | 0.0150 | droits      | rights      | 0.0714 |
| société     | society     | 0.0152 | présente    | declaration | 0.0849 |
| unies       | united      | 0.0213 | religion    | property    | 0.0979 |

&amp;nbsp;

## Analyse des résultats

Après application de l&#39;algorithme DTW sur différentes langues, nous avons pu constater que la mise de correspondance de mots était efficace lorsque la valeur de DTW renvoyée pour un couple de mots était inférieure à $0,1$ (celle-ci ayant été normalisée pour être comprise entre $0$ et 1).

Pour le couple de langues français et anglais, on constate des associations erronées pour à partir d&#39;une valeur de DTW supérieure à $0,08$, notamment en raison de la syntaxe différente des langues (voir ci-dessous). Il convient donc d&#39;inclure la distance de Jaro-Winkler dans le système d&#39;alignement pour corriger ce problème.

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/piege.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Une fois la distance de Jaro-Winkler incluse dans le système d&#39;alignement, nous pouvons combiner ces associations de mots pour procéder à l&#39;association des paragraphes de la DUDH. La DUDH est constituée de $89$ paragraphes, pour un total de $2$ $106$ mots et $11$ $663$ caractères.

|      |      |
|------|------|
|















&lt;figure  id=&#34;figure-déclaration-universelle-des-droits-de-lhomme-dudh&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/dudh.jpg&#34; alt=&#34;Déclaration Universelle des Droits de l&amp;#39;Homme (DUDH).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;83%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Déclaration Universelle des Droits de l&amp;rsquo;Homme (DUDH).
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-universal-declaration-of-human-rights-udhr&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/udhr.jpg&#34; alt=&#34;Universal Declaration of Human Rights (UDHR).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Universal Declaration of Human Rights (UDHR).
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

Les associations réciproques de paragraphes (tirets sur les traits verticaux) sont représentées par des flèches pleines, tandis que les flèches en pointillés représentent les associations dans un seul sens.

|      |      |
|------|------|
|















&lt;figure  id=&#34;figure-alignement-par-longueur&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/alignlongueur.png&#34; alt=&#34;Alignement par longueur.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Alignement par longueur.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-alignement-avec-dtw&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/textmatching/alignDTW.png&#34; alt=&#34;Alignement avec DTW.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Alignement avec DTW.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

## Limites du système d&#39;alignement

La limite principale du système d&#39;alignement développé réside dans la précision de l&#39;alignement, limitée à une échelle de paragraphes. L&#39;algorithme DTW seul n&#39;est pas suffisant pour réaliser un alignement efficace, et l&#39;ajout d&#39;une distance syntaxique comme Jaro-Winkler est nécessaire pour améliorer les résultats. Les résultats obtenus sont très satisfaisants malgré tout.

&amp;nbsp;

### Références

1. Kim Gerdes. *L’alignement pour les pauvres : Adapter la bonne métrique pour unalgorithme dynamique de dilatation temporelle pour l’alignement sans ressources de corpus bilingues.* 9e Journées internationales d’Analyse statistique des Données Textuelles, Mars 2008.

2. Meinard Müller. *Information retrieval for music and motion.* Chapter 4 : Dynamic Time Warping, Septembre 2007.

&amp;nbsp;

---&gt;
</description>
    </item>
    
    <item>
      <title>Covariance estimation by sparse method</title>
      <link>https://michaelkarpe.github.io/quantitative-finance-projects/covariance/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/quantitative-finance-projects/covariance/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;presentation-of-the-project&#34;&gt;Presentation of the project&lt;/h1&gt;
&lt;p&gt;Determining the dependency structures between different assets or risk factors is at the heart of multidimensional financial modelling problems. For example, when considering a portfolio of assets, Markowitz theory states that it is important that the portfolio be diversified and therefore that the assets be as uncorrelated as possible.&lt;/p&gt;
&lt;p&gt;Mathematically, the natural variable to model this dependence structure is the covariance matrix. Indeed, in the context of Gaussian variables, covariance is sufficient to describe correlation structures (which is not the case for other distributions). Moreover, although the focus is on covariance and the criterion studied is based on Gaussian modelling, the information obtained is actually richer and can be applied to other distributions.&lt;/p&gt;
&lt;p&gt;However, the empirical data that we exploit are affected by noise that biases the estimation and conventional methods then provide poor results.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;purpose-and-method-of-resolution&#34;&gt;Purpose and method of resolution&lt;/h1&gt;
&lt;p&gt;The aim is therefore to obtain a noise-free covariance matrix, called sparse due to a large number of zero coefficients, from a noisy covariance matrix. This problem can be written as an optimization problem in which one seeks to maximize the log-likelihood of the solution by penalizing the number of zeros in the inverse covariance matrix. In addition, we impose constraints on its eigenvalues to ensure that the matrix is positive, and to further limit the solution, given the information that we would have a priori on the problem. In the Gaussian framework, the latter is formulated as follows:&lt;/p&gt;
&lt;p&gt;$$\text{max} \ f(X) := \log \det X - &amp;lt; \Sigma, X &amp;gt; - \rho \ Card(X)$$
$$\text{s.c.} \ \alpha I_n \leq X \leq \beta I_n$$
$$\Sigma \in S_n^{+}, \ X \in S_n$$
$$\rho, \ \alpha, \ \beta &amp;gt; 0$$&lt;/p&gt;
&lt;p&gt;However, this problem is described as NP-difficult, which means that it cannot be solved by computer in a reasonable time - one reason for this is the non-convexity of the objective function. It is therefore necessary to transform it. For this, convex relaxation methods are applied to the initial problem. These methods make it possible to put the problem into a form for which numerical solving algorithms exist. The algorithm used here is the algorithm of Nesterov (2005).&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;nesterovs-algorithm&#34;&gt;Nesterov&amp;rsquo;s algorithm&lt;/h2&gt;
&lt;p&gt;Nesterov&amp;rsquo;s algorithm is based on recent and efficient optimization methods to determine the extremum of a function $\phi(y)$ which is close to the function $f(X)$. By reducing the difference between the exact solution and the approximated solution by successive iterations (see figure below), we obtain a result very close to the exact solution.&lt;/p&gt;
















&lt;figure  id=&#34;figure-illustration-of-the-nesterov-algorithm-with-fx-exact-function-phiy-approximate-function&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov.png&#34; alt=&#34;Illustration of the Nesterov algorithm with $f(x)$ exact function, $\phi(y)$ approximate function.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Illustration of the Nesterov algorithm with $f(x)$ exact function, $\phi(y)$ approximate function.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;checking-the-algorithm&#34;&gt;Checking the algorithm&lt;/h1&gt;
&lt;p&gt;In order to make sure that the algorithm is properly implemented, we tested it on a very simple noisy matrix. It was constructed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: An $A$ matrix is constructed whose diagonal coefficients are equal to 1 and of which a few non-diagonal coefficients (i.e. a negligible number compared to the size of the chosen matrix), randomly drawn according to a uniform probability, are equal to $1$ or $-1$ with equiprobability. The randomly drawn non-diagonal coefficients are copied symmetrically with respect to the diagonal so as to obtain a diagonal matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the resulting matrix is not invertible, the procedure is repeated. The matrix finally created corresponds to the inverse of a covariance matrix without noise but with some non-zero covariances, i.e. only a few variables are correlated.&lt;/p&gt;
















&lt;figure  id=&#34;figure-matrix-a-from-step-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape1.png&#34; alt=&#34;Matrix $A$ from step 1.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrix $A$ from step 1.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step 2: A second $V$ matrix whose coefficients follow a uniform law on $[-1, 1]$ is constructed and then symmetrized. This matrix corresponds to the noise.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Step 3: We constitute a covariance matrix $B$ noise by summing the inverse of $A$ with $\sigma V$ where $\sigma$ allows to intensify or to attenuate the noise.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
















&lt;figure  id=&#34;figure-matrix-b-from-step-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape3.png&#34; alt=&#34;Matrix $B$ from step 3.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrix $B$ from step 3.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;We applied Nesterov&amp;rsquo;s algorithm to this test matrix with $\epsilon = 10^{-5}$, $\rho = 0.5$, $\alpha = 10^{-1}$, $\beta = 10$ and $\sigma = 0.15$.&lt;/p&gt;
















&lt;figure  id=&#34;figure-matrix-obtained-by-applying-nesterovs-algorithm-to-b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov_matrix.png&#34; alt=&#34;Matrix obtained by applying Nesterov&amp;#39;s algorithm to $B$.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrix obtained by applying Nesterov&amp;rsquo;s algorithm to $B$.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;In order to present the results obtained by the Nesterov algorithm for the test matrix, we represent the matrices at the input and output of the algorithm by arrays of pixels where each pixel has a color that depends on the value of the coefficient with which it is associated.&lt;/p&gt;
&lt;p&gt;The results obtained are very satisfactory. Indeed, the matrix obtained by the algorithm looks very much like the initial noiseless matrix.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;application-to-interest-rate-analysis&#34;&gt;Application to interest rate analysis&lt;/h1&gt;
&lt;p&gt;We then applied our algorithm to a covariance matrix obtained empirically from data on interest rate changes.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-noisy-empirical-covariance-matrix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/empirique.png&#34; alt=&#34;Noisy empirical covariance matrix.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Noisy empirical covariance matrix.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;















&lt;figure  id=&#34;figure-selected-covariance-matrix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/selection.png&#34; alt=&#34;Selected covariance matrix.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Selected covariance matrix.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;After applying the algorithm, the selected matrix has a globally diagonal block structure. To interpret this structure, a graphical representation is used where the nodes are the different assets and an edge is drawn between two nodes if they are correlated (i.e. of non-zero covariance).&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/illustration.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Thus, we obtain two clusters that show that swaps are correlated according to their maturity. The top cluster has maturities of $3$, $6$ and $9$ months and the bottom cluster has maturities of $1$ to $30$ years. These different maturities correspond to different markets and different financing needs.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;analysis-of-the-method-presented&#34;&gt;Analysis of the method presented&lt;/h1&gt;
&lt;p&gt;Variance-covariance estimation by sparse method is a powerful and generic tool to manage constraints and keep some traceability at the data level. This method helps to extract fine information from noisy data, which could not have been properly analyzed without removing the noise contribution.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

&amp;nbsp;

# Présentation du projet

La détermination des structures de dépendance entre différents actifs ou facteurs de risques est au cœur des problèmes de modélisation financière multidimensionnels. Par exemple, lorsque l’on considère un portefeuille d’actifs, la théorie de Markowitz énonce qu’il est important que le portefeuille soit diversifié et donc que les actifs soient le moins corrélé possible.

Mathématiquement, la variable naturelle pour modéliser cette structure de dépendance est la matrice de covariance. En effet, dans le cadre de variables gaussiennes, la covariance est suffisante à décrire les structures de corrélation (ce qui n’est pas le cas pour d’autres distributions). De plus, bien que l’on s’intéresse à la covariance et que le critère étudié se base sur une modélisation gaussienne, l’information obtenue est en fait plus riche et peut s’appliquer à d’autres distributions.

Cependant, les données empiriques que nous exploitons sont affectées par du bruit qui biaise l’estimation et les méthodes classiques fournissent alors de piètres résultats.

&amp;nbsp;

# Objectif et méthode de résolution

Le but est donc d’obtenir une matrice de covariance débarrassée du bruit, dite sparse en raison d’un grand nombre de coefficients nuls, à partir d’une matrice de covariance bruitée. Ce problème peut s’écrire comme un problème d’optimisation dans lequel on cherche à maximiser la log-vraisemblance de la solution en pénalisant le nombre de zéros dans la matrice de covariance inverse. On impose, de plus, des contraintes sur ses valeurs propres permettant d’une part de s’assurer que la matrice est positive, et d’autre part de borner davantage la solution, compte tenu d’informations qu’on aurait a priori sur le problème. Dans le cadre gaussien, ce dernier se formule ainsi :

$$\text{max} \ f(X) := \log \det X - &lt; \Sigma, X &gt; - \rho \ Card(X)$$
$$\text{s.c.} \ \alpha I_n \leq X \leq \beta I_n$$
$$\Sigma \in S_n^{+}, \ X \in S_n$$
$$\rho, \ \alpha, \ \beta &gt; 0$$

Ce problème est néanmoins qualifié de NP-difficile, ce qui signifie qu’on ne peut pas le résoudre informatiquement en temps raisonnable – cela est notamment dû à la non-convexité de la fonction objectif. Il est donc nécessaire de le transformer. Pour cela, on applique au problème initial des méthodes de relaxation convexe. Ces méthodes permettent de mettre le problème sous une forme pour laquelle il existe des algorithmes de résolution numérique. L’algorithme utilisé ici est l’algorithme de Nesterov (2005).

&amp;nbsp;

## Algorithme de Nesterov

L’algorithme de Nesterov s’inspire de méthodes d’optimisation récentes et efficaces pour déterminer l’extremum d’une fonction $\phi(y)$ qui est proche de la fonction $f(X)$. En réduisant l’écart entre la solution exacte et la solution approchée par itérations successives (voir figure ci-dessous), on obtient un résultat très proche de la solution exacte.

















&lt;figure  id=&#34;figure-illustration-de-lalgorithme-de-nesterov-avec-fx-fonction-exacte-phiy-fonction-approchée&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov.png&#34; alt=&#34;Illustration de l’algorithme de Nesterov avec $f(x)$ fonction exacte, $\phi(y)$ fonction approchée.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Illustration de l’algorithme de Nesterov avec $f(x)$ fonction exacte, $\phi(y)$ fonction approchée.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

# Vérification de l&#39;algorithme

Afin de s’assurer de la bonne implémentation de l’algorithme, nous l’avons testé sur une matrice bruitée très simple. Celle-ci a été construite de la façon suivante :

- Etape 1 : On construit une matrice $A$ dont les coefficients diagonaux sont égaux à 1 et dont quelques coefficients non diagonaux (c’est-à-dire un nombre négligeable par rapport à la taille de la matrice choisie), tirés aléatoirement selon une probabilité uniforme, sont égaux à $1$ ou $-1$ avec équiprobabilité. Les coefficients non diagonaux tirés aléatoirement sont recopiés symétriquement par rapport à la diagonale de façon à obtenir une matrice diagonale.

Si la matrice alors obtenue n’est pas inversible, on répète la procédure. La matrice finalement créée correspond à l’inverse d’une matrice de covariance sans bruit mais avec quelques covariances non nulles, c’est-à-dire que seuls quelques variables sont corrélées.

















&lt;figure  id=&#34;figure-matrice-a-de-létape-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape1.png&#34; alt=&#34;Matrice $A$ de l&amp;#39;étape 1.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice $A$ de l&amp;rsquo;étape 1.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

- Etape 2 : On construit une seconde matrice $V$ dont les coefficients suivent une loi uniforme sur $[-1, 1]$ puis on la symétrise. Cette matrice correspond au bruit.

- Etape 3 : On constitue une matrice de covariance $B$ bruitée en sommant l’inverse de $A$ avec $\sigma V$ où $\sigma$ permet d’intensifier ou d’atténuer le bruit.

















&lt;figure  id=&#34;figure-matrice-b-de-létape-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/etape3.png&#34; alt=&#34;Matrice B de l&amp;#39;étape 3.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice B de l&amp;rsquo;étape 3.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

Nous avons appliqué l’algorithme de Nesterov sur cette matrice test avec $\epsilon = 10^{-5}$, $\rho = 0.5$, $\alpha = 10^{-1}$, $\beta = 10$ et $\sigma = 0.15$.

















&lt;figure  id=&#34;figure-matrice-obtenue-en-appliquant-lalgorithme-de-nesterov-à-b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/nesterov_matrix.png&#34; alt=&#34;Matrice obtenue en appliquant l’algorithme de Nesterov à $B$.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice obtenue en appliquant l’algorithme de Nesterov à $B$.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

Afin de présenter les résultats obtenus par l’algorithme de Nesterov pour la matrice test, nous représentons les matrices en entrée et en sortie de l’algorithme par des tableaux de pixels où chaque pixel a une couleur qui dépend de la valeur du coefficient auquel il est associé.

Les résultats obtenus sont très satisfaisants. En effet, la matrice obtenue par l’algorithme ressemble très fortement à la matrice sans bruit initiale.

&amp;nbsp;

# Application à l’analyse de taux d’intérêts

Nous avons ensuite appliqué notre algorithme à une matrice de covariance obtenue empiriquement à partir de données de variations de taux d’intérêts.

|      |      |
|------|------|
|















&lt;figure  id=&#34;figure-matrice-de-covariance-empirique-bruitée&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/empirique.png&#34; alt=&#34;Matrice de covariance empirique bruitée.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice de covariance empirique bruitée.
    &lt;/figcaption&gt;&lt;/figure&gt; | 















&lt;figure  id=&#34;figure-matrice-de-covariance-sélectionnée&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/selection.png&#34; alt=&#34;Matrice de covariance sélectionnée.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Matrice de covariance sélectionnée.
    &lt;/figcaption&gt;&lt;/figure&gt;|

&amp;nbsp;

Après avoir appliqué l’algorithme, la matrice sélectionnée présente une structure globalement diagonale par blocs. Pour interpréter cette structure,  on utilise une représentation graphique où les nœuds sont les différents actifs et on trace une arête entre deux nœuds s’ils sont corrélés (i.e. de covariance non nulle).

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/covariance/illustration.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

Ainsi, nous obtenons deux clusters qui montrent que les swaps sont corrélés en fonction de leur maturité. On trouve dans le cluster du haut, des maturités de 3, 6 et 9 mois et dans celui du bas, des maturités de 1 à 30 ans. Ces différentes maturités correspondent à de différents marchés et de différents besoins de financement.

&amp;nbsp;

# Analyse de la méthode présentée

L’estimation de variance-covariance par méthode sparse est un outil puissant et générique pour gérer les contraintes et garder une certaine tractabilité au niveau des données. Cette méthode aide à l’extraction d’informations fines à partir de données bruitées, lesquelles données n’auraient pas pu être analysées correctement sans avoir enlevé la contribution du bruit.

&amp;nbsp;

---&gt;
</description>
    </item>
    
    <item>
      <title>Markovian Image Restoration</title>
      <link>https://michaelkarpe.github.io/computer-vision-projects/restoration/</link>
      <pubDate>Wed, 07 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/computer-vision-projects/restoration/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;presentation-of-the-project&#34;&gt;Presentation of the project&lt;/h1&gt;
&lt;p&gt;The acquisition of a digital image is most often accompanied by the appearance of noise, often due to imperfections in the detection, transmission or compression of the signal, or to inherent defects in the environment such as insufficient or too much lighting. The suppression of this noise is even a vital issue in several fields, including medical imaging, and the search for an effective image denoising algorithm remains a persistent challenge at the crossroads of several scientific fields: functional analysis, probability, statistics and physical sciences.&lt;/p&gt;
&lt;p&gt;In this study, we implement different algorithms for the restitution of an image containing simple grayscale patterns, noisy according to Gaussian noise. For this, we adopt a probabilistic description of the image, considering its pixels as random variables $X_i$, energy $U(X_i)$ which depends on its neighborhood. This approach allows us to consider the image as a Markov field.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;numerical-implementation-of-markov-fields&#34;&gt;Numerical implementation of Markov fields&lt;/h1&gt;
&lt;p&gt;We are now formulating the probabilistic methods that we have digitally implemented to apply them to image processing.&lt;/p&gt;
&lt;p&gt;In our study, we will consider rectangular grayscale images (0 to 255), length $w$ and height $h$. Each pixel in the image is represented by its coordinates $(i, j)$ and a value corresponding to the gray level of the pixel, which value belongs to $E = [|0, 255|]$. The total image can thus be represented by an array of values of $E$, of dimension $w \times h$.&lt;/p&gt;
&lt;p&gt;For each pixel, a click system can be defined as represented on the first image of this article. In our computer codes, the presence of a variable $c$ which will take the value $4$ or $8$ will allow us to choose if we consider $4$ or $8$-connected neighborhoods, and we will consider clicks of order $2$.&lt;/p&gt;
&lt;p&gt;The implementation of image processing algorithms has three main phases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the choice of a suitable Markov field&lt;/li&gt;
&lt;li&gt;the drawing of a configuration according to the chosen Markov field&lt;/li&gt;
&lt;li&gt;the implementation of the algorithm that converges to a correct image after a certain number of prints.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We describe these three phases in a theoretical way, then we will analyze the results obtained with the different methods.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;markov-fields-in-image-processing&#34;&gt;Markov fields in image processing&lt;/h2&gt;
&lt;p&gt;We present here the most used Markov fields in image processing as well as some variants giving better results.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;ising-model&#34;&gt;Ising Model&lt;/h3&gt;
&lt;p&gt;The Ising model is only applicable to an image with $2$ levels of gray. By an affine transformation, we can associate to these $2$ values the values of the set $E = \{-1, 1\}$. We recall the energy of this model:&lt;/p&gt;
&lt;p&gt;$$U(x) = - \sum_{c=(s,t) \in C} \beta x_s x_t - \sum_{s \in S} B x_s$$&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;potts-model&#34;&gt;Potts Model&lt;/h3&gt;
&lt;p&gt;This is a generalization of Ising&amp;rsquo;s model, adapted to a set $E$ of cardinal $N$, as $E = [|0, 255|]$. The main difference with the Ising model is that only potentials related to second-order clicks are defined. There is no energy term related to first-order clicks, corresponding to an external magnetic field. The energy of this model is :&lt;/p&gt;
&lt;p&gt;$$U(x) = \ \beta \sum_{c=(s,t) \in C} (\textbf{1}&lt;em&gt;{\{x_s \neq x_t\}} - \textbf{1}&lt;/em&gt;{\{x_s = x_t\}})$$&lt;/p&gt;
&lt;p&gt;Such a model tends to create homogenous zones the larger the $\beta$ is.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;gaussian-markovian-model&#34;&gt;Gaussian Markovian model&lt;/h3&gt;
&lt;p&gt;This model can only be used for grayscale images, which is perfectly suited to our study. We consider, here again, $4$- or $8$-axis neighborhoods and only second order clicks. The energy of this model is :&lt;/p&gt;
&lt;p&gt;$$U(x) = \beta \sum_{c=(s,t) \in C} (x_s-x_t)^2 + \alpha \sum_{s \in S} (x_s-\mu_s)^2$$&lt;/p&gt;
&lt;p&gt;For $\beta &amp;gt; 0$, the first quadratic term favors small differences in gray level, since it minimizes an energy that increases quadratically with the difference in gray levels. The second term involves a $\mu_s$ term that corresponds to a reference image. If we know an approximation of the image we want to obtain, or if we want to remain close to the initial image, this term allows the solution image $x$ to not move away from the reference image $\mu$.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;drawing-a-configuration-according-to-the-markov-field&#34;&gt;Drawing a configuration according to the Markov field&lt;/h2&gt;
&lt;p&gt;Once a Markov field has been chosen, its total energy must be minimized. This is done by pulling configurations. The general idea is to draw a random value for each pixel and to assign it this value if it allows the total energy to be reduced.&lt;/p&gt;
&lt;p&gt;The algorithms most commonly used to make these draws, the Gibbs sampler and the Metropolis algorithm, work in a similar way. The $n$ iterations, where a $s$ pixel is randomly selected and then an image-dependent random experiment is associated with $s$ at the $n-1$ iteration. We update or not $s$ depending on the result of the random experiment.&lt;/p&gt;
&lt;p&gt;Since all the pixels $s$ must be scanned a large number of times, one usually scans all the pixels line by line and from left to right without performing a random draw, to make sure that all pixels have been updated. The algorithm stops after a large number $n$ of iterations, or when there are few pixel changes for an iteration.&lt;/p&gt;
&lt;p&gt;Both algorithms are called probabilistic relaxation algorithms: relaxation because the algorithm performs successive updates of the different pixels, and probabilistic because the algorithm simulates random draws.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;gibbs-sampler&#34;&gt;Gibbs&amp;rsquo; sampler&lt;/h3&gt;
&lt;p&gt;For each of the $n$ iterations of the algorithm, we scan all the pixels. For each pixel (noted $s$) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calculation of the local probability, knowing the configuration of the neighbors $V_s$ for the image at iteration $n-1$ :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ \mathbb{P} (X_s = x_s | V_s) = \dfrac{\exp (-U_s(x_s | V_s))}{\sum_{a_s \in A_s}{\exp (-U_s(a_s | V_s))}}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Updating of the site by random draw according to the law $\mathbb{P}(X_s = x_s | V_s)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;metropolis-algorithm&#34;&gt;Metropolis algorithm&lt;/h3&gt;
&lt;p&gt;For each of the $n$ iterations of the algorithm, we scan all the pixels. For each pixel (noted $s$) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random draw of $\lambda$ in $E$ according to a uniform law on $E$:&lt;/li&gt;
&lt;li&gt;Calculation of the energy variation if the value of $s$, $x_s^{(n-1)}$, is replaced by $\lambda$:
$$ \Delta U = U_s(\lambda | V_s^{(n-1)}) - U_s(x_s^{(n-1)} | V_s^{(n-1)}) $$&lt;/li&gt;
&lt;li&gt;If $ \Delta U \leq $0, then we update the pixel: $x_s^{(n)} = \lambda$&lt;/li&gt;
&lt;li&gt;Otherwise, update to the probability of success $p = \exp (- \Delta U)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;implementation-of-the-image-restoration-algorithm&#34;&gt;Implementation of the image restoration algorithm&lt;/h2&gt;
&lt;p&gt;After implementing a configuration pull algorithm, it is necessary to implement an algorithm converging to a solution image of the total energy minimization problem. Two algorithms are mainly used in Markov field image processing.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;simulated-annealing&#34;&gt;Simulated annealing&lt;/h3&gt;
&lt;p&gt;Simulated annealing is a classical method of energy minimization frequently used in physics. In image processing, the algorithm consists of $n$ iterations during which configuration prints are made. However, these prints now depend only on the configuration energy, but also on a quantity $T^{(n)}$ which measures the degree of randomness introduced in these prints, called temperature, and which decreases with each iteration. Starting from a fairly large temperature $T^{(0)}$ and the image to be processed, the algorithm is as follows:&lt;/p&gt;
&lt;p&gt;For each iteration $n$,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Printing a configuration by replacing the energies $U(x)$ by the quantities $U(x) / T^{(n)}$ for pixel update prints&lt;/li&gt;
&lt;li&gt;Temperature decrease in logarithmic decay: $T^{(n)} &amp;gt; \dfrac{c}{\log(2+n)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Logarithmic decay is necessary to obtain the convergence in probability of the algorithm to the image that minimizes energy. In practice, for complex or large images, this decay is too slow and it is preferable to use a linear or quadratic decay, which can cause convergence to only a local minimum of energy. However, the size ($200 \times 200$ pixels) and the simplicity of the images we process allow us to use this logarithmic decay. Slight differences between logarithmic and linear decay have been observed, which is consistent with the notions of local and global energy minimum.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;iterated-conditional-modes&#34;&gt;Iterated conditional modes&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;Iterated Conditional Modes (ICM)&lt;/em&gt; method consists in testing &lt;em&gt;all&lt;/em&gt; shades of gray for each pixel and updating with the configuration that allows the most important energy decrease. Even if all shades of grey are tested at each iteration, the absence of the probabilistic character (present in the simulated annealing) allows the ICM to converge much faster. On the other hand, we do not necessarily converge towards the global minimum.&lt;/p&gt;
&lt;p&gt;The ICM algorithm is as follows: For each of the $n$ iterations of the algorithm, we scan all the pixels. For each pixel (noted $s$) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\forall \lambda \in E$, calculating the energy variation if the value of $s$, $x_s^{(n-1)}$, is replaced by $\lambda$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$ \Delta U = U_s(\lambda | V_s^{(n-1)}) - U_s(x_s^{(n-1)} | V_s^{(n-1)}) $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the U delta is $0$, then update the pixel with a label that minimizes $\Delta U$ : $x_s^{(n)} = \lambda$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;practical-results-and-discussion&#34;&gt;Practical results and discussion&lt;/h1&gt;
&lt;p&gt;Before we could restore images, we had to deteriorate them. To do this, we used a Gaussian noise with an amplitude of $50$, simulating a Gaussian random variable and modifying the value of the pixels in the image by adding the value of the variable if this addition allows the pixel value to remain within $E = [|0, 255|]$.&lt;/p&gt;
&lt;p&gt;The figure below shows the image we studied, as well as the same image scrambled with a Gaussian noise of amplitude $50$. This image is composed of a white background (pixels of value $x_s = 255$), a black square ($x_s = 0$), a dark gray star ($x_s = 70$), a gray heart ($x_s = 140$) and a light gray circle ($x_s = 210$).&lt;/p&gt;
















&lt;figure  id=&#34;figure-initial-image-without-noise-left-and-with-gaussian-noise-of-amplitude-50-right&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures1.png&#34; alt=&#34;Initial image without noise (left) and with Gaussian noise of amplitude 50 (right).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Initial image without noise (left) and with Gaussian noise of amplitude 50 (right).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;To quantitatively evaluate the image restoration, we consider the &lt;em&gt;signal-to-noise ratio (SNR)&lt;/em&gt;, which is expressed in decibels (dB) and is given by :&lt;/p&gt;
&lt;p&gt;$$SNR = 10 \log \left(\dfrac{\sum_{s \in S} x_s^2}{\sum_{s \in S} (y_s-x_s)^2} \right)$$&lt;/p&gt;
&lt;p&gt;where $x_s$ is the value of the $s$ pixel of the original noiseless image, and $y_s$ is the value of the $s$ pixel of the noiseless image. The greater the SNR, the less noise deteriorates the initial image. To know if the noise removal is effective, we will have to compare the values with the SNR obtained with the noisy image. The table below summarizes the SNR for a Gaussian noise of amplitude $50$.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Amplitude&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;63.9516&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;45.2884&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;31.9855&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;20.3203&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Remember that we consider grayscale images, because the computing time is too long for color images ($E = [|0, 255|]^3$). Even for very simple grayscale images, the results obtained are far from perfect, which justifies limiting our study to this type of images.&lt;/p&gt;
&lt;p&gt;There are several methods of Markov field image processing and each of these methods has its own parameters. To perform image processing, we must choose the values given to these parameters. The table below summarizes all the parameters with the default choices made, which are the choices that gave the best results when executing our algorithms.&lt;/p&gt;
&lt;p&gt;The main differences in our results lie in the choice of the minimization algorithm and in the choice of the potential model.&lt;/p&gt;
&lt;p&gt;We are going to apply to the scrambled image the different algorithms described above, for different parameter values. We will first study the Potts model, then the Markovian-Gaussian model.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;first-series-of-tests&#34;&gt;First series of tests&lt;/h2&gt;
&lt;p&gt;At first, we consider that we know nothing about the image we need to obtain. We implement the algorithms described previously and execute them.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;potts-model-1&#34;&gt;Potts Model&lt;/h3&gt;
&lt;h4 id=&#34;with-simulated-annealing&#34;&gt;With simulated annealing&lt;/h4&gt;
&lt;p&gt;Given the size of the image, a scan of the noisy image shows that all shades of gray are present in the image: $E = [|0, 255|]$. We then run the simulated annealing with a Metropolis algorithm that pulls $\lambda \in E$.&lt;/p&gt;
















&lt;figure  id=&#34;figure-image-processed-by-simulated-annealing-and-potts-model-for-beta-in-50-100-500-from-left-to-right&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures10.png&#34; alt=&#34;Image processed by simulated annealing and Potts model for $\beta \in \\{50, 100, 500\\}$ (from left to right).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by simulated annealing and Potts model for $\beta \in \{50, 100, 500\}$ (from left to right).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$\beta$&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;2.73104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;8.48355&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;11.3079&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;11.6187&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that the results improve when $\beta$ increases, with a saturation phenomenon for $\beta$ large ($\beta \approx 10^4$). However, the results are worse than the noisy image itself! The image being very noisy, the algorithm tends to replace the pixels by any value between $0$ and $255$, which does not allow to unclutter the image.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h4 id=&#34;with-icm&#34;&gt;With ICM&lt;/h4&gt;
















&lt;figure  id=&#34;figure-image-processed-by-icm-and-potts-model-for-1-2-and-4-iterations-from-left-to-right&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures3.png&#34; alt=&#34;Image processed by ICM and Potts model for 1, 2 and 4 iterations (from left to right).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by ICM and Potts model for 1, 2 and 4 iterations (from left to right).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Itérations&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;29.2689&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;27.3294&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;25.5964&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;23.1687&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are very good after a few iterations. Even after only one iteration, the noise has almost completely disappeared. However, the ICM tends to crop the figures, resulting in an SNR that decreases as the number of iterations increases. The SNR does not change when changing $\beta$, so $\beta$ does not seem to have any influence on the result.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;gaussian-markovian-model-1&#34;&gt;Gaussian Markovian model&lt;/h3&gt;
&lt;p&gt;The Gaussian Markovian model is now being considered, and both energy minimization algorithms are being tested with and without attachment to the initial data.&lt;/p&gt;
&lt;p&gt;We find that the Markovian Gaussian model blurs the images. Indeed, it tends to perform a kind of local average. Since the image is blurred by noise that takes on all values of $E = [|0, 255|]$, the averaging homogenizes the different grey areas into an average grey. Thus, even though the table below shows that the SNR is better than in the blurred image, the visual rendering and the disappearance of the noise in the white area are not good.&lt;/p&gt;
















&lt;figure  id=&#34;figure-image-processed-by-simulated-annealing-top-and-icm-bottom-and-markovian-gaussian-model-without-left-and-with-right-data-attachment&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures4.png&#34; alt=&#34;Image processed by simulated annealing (top) and ICM (bottom) and Markovian Gaussian model without (left) and with (right) data attachment.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by simulated annealing (top) and ICM (bottom) and Markovian Gaussian model without (left) and with (right) data attachment.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Méthode&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Annealing without attachment&lt;/td&gt;
&lt;td&gt;35.2715&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Annealing with attachment&lt;/td&gt;
&lt;td&gt;38.9796&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ICM without attachment&lt;/td&gt;
&lt;td&gt;38.0792&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ICM with attachment&lt;/td&gt;
&lt;td&gt;17.2107&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This first series of tests shows that, except for the ICM with Potts model, the image restoration results are poor. For the simulated annealing with Potts, the poor results are explained by the random drawing on the $255$ shades of grey, whereas in the initial un-noiseed image, only $5$ shades are present.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;second-series-of-tests&#34;&gt;Second series of tests&lt;/h2&gt;
&lt;p&gt;Configuration drawings are now made only among the shades present in the initial unclouded image: $\lambda \in E = \{0, 70, 140, 210, 255\}$.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;potts-model-2&#34;&gt;Potts model&lt;/h3&gt;
&lt;h4 id=&#34;with-simulated-annealing-1&#34;&gt;With simulated annealing&lt;/h4&gt;
















&lt;figure  id=&#34;figure-image-processed-by-simulated-annealing-and-potts-model-for-beta-in-5-25-35-50-100-500-from-left-to-right-then-top-to-bottom&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures2.png&#34; alt=&#34;Image processed by simulated annealing and Potts model for $\beta \in \\{5, 25, 35, 50, 100, 500\\}$ (from left to right then top to bottom).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by simulated annealing and Potts model for $\beta \in \{5, 25, 35, 50, 100, 500\}$ (from left to right then top to bottom).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$\beta$&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;3.38126&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;3.18447&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;td&gt;14.8631&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;22.8984&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;23.9018&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;25.9764&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are much better than when nothing is known about the initial image, even if the SNR remains lower than that of the noisy image. We can see here the importance of the $\beta$ parameter: the larger $\beta$ is, the larger the size of the homogeneous areas increases. However, there is always a saturation phenomenon for large $\beta$. The image converges towards a state close to the initial state.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h4 id=&#34;with-icm-1&#34;&gt;With ICM&lt;/h4&gt;
&lt;p&gt;The results become excellent: the image obtained is almost the initial image and the SNR is better than for the noisy image. The SNR also shows that $\beta$ still has no influence.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;markovian-gaussian-model&#34;&gt;Markovian Gaussian model&lt;/h3&gt;
&lt;p&gt;The Gaussian Markovian model also gives much better results. The results are slightly better for ICM than for simulated annealing. The image obtained when the attachment is added to the initial data, including the correction of the adverse effects of the ICM at the edges, is extremely close to the initial noise-free image.&lt;/p&gt;
















&lt;figure  id=&#34;figure-image-processed-by-simulated-annealing-top-and-icm-bottom-and-markovian-gaussian-model-without-left-and-with-right-attached-to-the-data-and-knowing-the-shades-of-grey-present-in-the-initial-unblurred-image&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures5.png&#34; alt=&#34;Image processed by simulated annealing (top) and ICM (bottom) and Markovian Gaussian model without (left) and with (right) attached to the data and knowing the shades of grey present in the initial unblurred image.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by simulated annealing (top) and ICM (bottom) and Markovian Gaussian model without (left) and with (right) attached to the data and knowing the shades of grey present in the initial unblurred image.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Méthode&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Annealing without attachment&lt;/td&gt;
&lt;td&gt;40.3498&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Annealing with attachment&lt;/td&gt;
&lt;td&gt;44.4864&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ICM without attachment&lt;/td&gt;
&lt;td&gt;43.9946&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ICM with attachment&lt;/td&gt;
&lt;td&gt;44.7768&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h4 id=&#34;with-simulated-annealing-2&#34;&gt;With simulated annealing&lt;/h4&gt;
&lt;p&gt;The influence of $\beta$ and the saturation phenomenon are preserved: the greater the $\beta$, the better the restoration. The SNR is better than for the noisy image.&lt;/p&gt;
&lt;p&gt;Modifications of the $\alpha$ parameter have been performed on the tests with data attachment, however the results are already very good for $\alpha$ small, so $\alpha$ does not seem to have a big influence. It is recalled that in theory, the larger $\alpha$ is, the greater the attachment to the initial data.&lt;/p&gt;
















&lt;figure  id=&#34;figure-image-processed-by-simulated-annealing-and-gaussian-markovian-model-without-top-and-with-bottom-data-attachment-for-beta-in-1-10-25-left-to-right&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures11.png&#34; alt=&#34;Image processed by simulated annealing and Gaussian Markovian model without (top) and with (bottom) data attachment for $\beta \in \\{1, 10, 25\\}$ (left to right).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by simulated annealing and Gaussian Markovian model without (top) and with (bottom) data attachment for $\beta \in \{1, 10, 25\}$ (left to right).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$\beta$&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;40.7834&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;40.0753&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;40.0668&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5000&lt;/td&gt;
&lt;td&gt;39.6827&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$\beta$&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;32.5507&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;38.6730&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;43.5519&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5000&lt;/td&gt;
&lt;td&gt;43.4116&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h4 id=&#34;with-icm-2&#34;&gt;With ICM&lt;/h4&gt;
&lt;p&gt;We make the same observations as with the simulated annealing. The best SNR among all the tests is obtained here, for $\beta = 20$.&lt;/p&gt;
















&lt;figure  id=&#34;figure-image-processed-by-icm-and-markovian-gaussian-model-with-data-attachment-for-beta-in-1-10-25-from-left-to-right&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures8.png&#34; alt=&#34;Image processed by ICM and Markovian Gaussian model with data attachment for $\beta \in \\{1, 10, 25\\}$ (from left to right).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by ICM and Markovian Gaussian model with data attachment for $\beta \in \{1, 10, 25\}$ (from left to right).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$\beta$&lt;/th&gt;
&lt;th&gt;SNR (dB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;19.7154&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;39.8086&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;46.9805&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;46.474&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;43.4296&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5000&lt;/td&gt;
&lt;td&gt;43.3621&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;limitations-of-markovian-methods&#34;&gt;Limitations of Markovian methods&lt;/h2&gt;
&lt;p&gt;The image processed in the first series of tests had separate shapes with distant shades of grey. We are now studying an image composed of a gray scale gradient to show the limitations of Markov methods.&lt;/p&gt;
















&lt;figure  id=&#34;figure-initial-image-without-noise-left-and-with-gaussian-noise-of-amplitude-50-right&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/cercles1.png&#34; alt=&#34;Initial image without noise (left) and with Gaussian noise of amplitude 50 (right).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Initial image without noise (left) and with Gaussian noise of amplitude 50 (right).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Simulated annealing methods and ICM with $\beta$ large, Gaussian Markovian model with data attachment and knowledge of the initial grades are applied to the image as these are the choices that have given the best results so far. The results are shown in below.&lt;/p&gt;
&lt;p&gt;It can be seen that the simulated annealing with Potts&amp;rsquo; model gives a result similar to the one of the image with the different shapes for $\beta = 25$. When $\beta$ is increased, the result does not improve and remains close to that.&lt;/p&gt;
&lt;p&gt;ICM with Potts tends to deteriorate the edges as seen in the first series of tests. However, as the shades of grey are close together, this causes some of them to disappear.&lt;/p&gt;
&lt;p&gt;The Gaussian Markovian model gives very good results whatever the potential. The SNR obtained is double that of the noisy image, which is even better than the results for the image with the different shapes.&lt;/p&gt;
&lt;p&gt;To conclude this study and to show the limits of Markovian methods on close shades, we tested the different algorithms on a photo with the $255$ shades of grey. The method giving the best SNR is the Markovian Gaussian model with data attachment, although this model results in blurring of the image.&lt;/p&gt;
















&lt;figure  id=&#34;figure-image-processed-by-simulated-annealing-left-and-icm-right-for-the-potts-model-top-and-the-markovian-gaussian-model-with-data-attachment-bottom-for-beta-large&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/cercles2.png&#34; alt=&#34;Image processed by simulated annealing (left) and ICM (right) for the Potts model (top) and the Markovian-Gaussian model with data attachment (bottom) for $\beta$ large.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image processed by simulated annealing (left) and ICM (right) for the Potts model (top) and the Markovian-Gaussian model with data attachment (bottom) for $\beta$ large.
    &lt;/figcaption&gt;&lt;/figure&gt;
















&lt;figure  id=&#34;figure-initial-noise-free-image-left-with-gaussian-noise-of-amplitude-50-middle-and-image-processed-by-markovian-gaussian-model-with-data-attachment-right-for-beta-large&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/resultatslenna.png&#34; alt=&#34;Initial noise-free image (left), with Gaussian noise of amplitude $50$ (middle) and image processed by Markovian Gaussian model with data attachment (right) for $\beta$ large.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Initial noise-free image (left), with Gaussian noise of amplitude $50$ (middle) and image processed by Markovian Gaussian model with data attachment (right) for $\beta$ large.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Denoising an image is an important step in many advanced fields such as medicine or cartography. Our project presents a method for denoising black and white images based on Markov field theory and Ising and Potts physical models. The theoretical study of the Markov model highlights the need to choose the right minimization approach, as well as the sampling algorithm.&lt;/p&gt;
&lt;p&gt;We were confronted with the problem of calibrating different parameters. The results show the efficiency of the ICM algorithm with the potentials from the Gaussian Markovian model, which, compared to other models, presents a better restitution of the deteriorated image. The better the information is known about the original image, the better the restoration.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

&amp;nbsp;

# Présentation du projet

L&#39;acquisition d&#39;une image numérique est le plus souvent accompagnée de l&#39;apparition de bruit, souvent dû à une imperfection au niveau de la détection, de la transmission ou de la compression du signal, ou encore à des défauts inhérents à l&#39;environnement comme la présence d&#39;éclairage insuffisant ou trop prononcé. La suppression de ce bruit est même un enjeu vital dans plusieurs domaines, notamment l&#39;imagerie médicale, et la recherche d&#39;un algorithme efficace de débruitage d&#39;image demeure un défi persistant, à la croisée de plusieurs domaines scientifiques : analyse fonctionelle, probabilités, statistiques et sciences physiques.

Dans cette étude, nous implémentons différents algorithmes de restitution d&#39;une image contenant des motifs simples, en niveaux de gris, bruitée selon un bruit gaussien. Pour cela, nous adoptons une description probabiliste de l&#39;image, considérant ses pixels comme des variables aléatoires $X_i$, d&#39;énergie $U(X_i)$ qui dépend de son voisinage. Cette approche permet d&#39;envisager l&#39;image comme un champ de Markov.

&amp;nbsp;

# Implémentation numérique des champs de Markov

Nous formulons à présent les méthodes probabilistes que nous avons implémentées numériquement pour les appliquer au traitement d&#39;images.

Dans le cadre de notre étude, nous considérerons des images rectangulaires en niveau de gris (de 0 à 255), de longueur $w$ et de hauteur $h$. Chaque pixel de l&#39;image est représenté par ses coordonnées $(i,j)$ et une valeur correspondant au niveau de gris du pixel, cette valeur appartenant à $E = [|0, 255|]$. L&#39;image totale peut donc être représentée par un tableau de valeurs de E, de dimension $w \times h$.

Pour chaque pixel, on peut définir un système de cliques tel qu&#39;expliqué en *(2.1.2)*. Dans nos codes informatiques, la présence d&#39;une variable $c$ qui prendra la valeur $4$ ou $8$ permettra de choisir si l&#39;on considère des voisinages 4 ou 8-connexe, et on considèrera des cliques d&#39;ordre 2.

L&#39;implémentation des algorithmes de traitement d&#39;image comporte trois phases principales :

- le choix d&#39;un champ de Markov adapté
- le tirage d&#39;une configuration selon le champ de Markov choisi
- l&#39;implémentation de l&#39;algorithme qui converge vers une image correcte après un certain nombre de tirages.

Nous décrivons ces trois phases de façon théorique, puis nous analyserons les résultats obtenus avec les différentes méthodes.

&amp;nbsp;

## Champs de Markov en traitement d&#39;image

Nous présentons ici les champs de Markov les plus utilisés en traitement d&#39;image ainsi que quelques variantes donnant de meilleurs résultats.

&amp;nbsp;

### Modèle d&#39;Ising :

Comme nous l&#39;avons indiqué en \textbf{(2.2)}, ce modèle n&#39;est applicable qu&#39;à une image comprenant 2 niveaux de gris. Par une transformation affine, on peut associer à ces 2 valeurs les valeurs de l&#39;ensemble $E = \{-1, 1\}$. On rappelle l&#39;énergie de ce modèle :

$$U(x) = - \sum_{c=(s,t) \in C} \beta x_s x_t - \sum_{s \in S} B x_s$$

&amp;nbsp;

### Modèle de Potts :

Il s&#39;agit de la généralisation du modèle d&#39;Ising, adaptée à un ensemble $E$ de cardinal N, comme $E = [|0, 255|]$. La différence principale avec le modèle d&#39;Ising est que seuls les potentiels liés aux cliques d&#39;ordre 2 sont définis. Il n&#39;y a pas de terme d&#39;énergie lié aux cliques d&#39;ordre 1, correspondant à un champ magnétique externe. L&#39;énergie de ce modèle est :

$$U(x) = \ \beta \sum_{c=(s,t) \in C} (\textbf{1}_{\{x_s \neq x_t\}} - \textbf{1}_{\{x_s = x_t\}})$$

Un tel modèle tend à créer des zones homogènes de taille d&#39;autant plus grande que $\beta$ est grand.

&amp;nbsp;

### Modèle markovien gaussien :

Ce modèle n&#39;est utilisable que pour les images en niveaux de gris, ce qui est parfaitement adapté à notre étude. On considère, ici encore, des voisinages 4 ou 8-connexes et seulement des cliques d&#39;ordre 2. L&#39;énergie de ce modèle est :

$$U(x) = \beta \sum_{c=(s,t) \in C} (x_s-x_t)^2 + \alpha \sum_{s \in S} (x_s-\mu_s)^2$$

Pour $\beta &gt; 0$, le premier terme quadratique favorise les faibles différences de niveaux de gris, puisqu&#39;il s&#39;agit de minimiser une énergie qui augmente de façon quadratique avec l&#39;écart en niveaux de gris. Le second terme fait intervenir un terme $\mu_s$ qui correspond à une image de référence. Si l&#39;on connaît une approximation de l&#39;image que l&#39;on veut obtenir, ou si l&#39;on veut rester proche de l&#39;image initiale, ce terme permet à l&#39;image solution $x$ de ne pas s&#39;éloigner de l&#39;image de référence $\mu$.

&amp;nbsp;

## Tirage d&#39;une configuration selon le champ de Markov

Après le choix d&#39;un champ de Markov, il faut procéder à la minimisation de son énergie totale. On procède par tirage de configurations. L&#39;idée générale est de tirer pour chaque pixel une valeur aléatoire et de lui attribuer cette valeur si elle permet la diminution de l&#39;énergie totale.

Les algorithmes les plus utilisés pour réaliser ces tirages, l&#39;échantillonneur de Gibbs et l&#39;algorithme de Metropolis, fonctionnent de façon similaire. On procède à $n$ itérations, où l&#39;on choisit aléatoirement un pixel $s$ puis on associe à $s$ une expérience aléatoire en fonction de l&#39;image à l&#39;itération $n-1$. On met à jour ou non $s$ selon le résultat de l&#39;expérience aléatoire.

Comme il faut balayer l&#39;ensemble des pixels $s$ un grand nombre de fois, on balaye généralement l&#39;ensemble des pixels ligne par ligne et de gauche à droite sans réaliser de tirage aléatoire, pour être sûr que tous les pixels ont été soumis à des mises à jour. L&#39;algorithme cesse après un grand nombre $n$ d&#39;itérations, ou lorsqu&#39;il y a peu de changements de pixels pour une itération.

Pour ces deux algorithmes, on parle d&#39;algorithme de relaxation probabiliste : relaxation car l&#39;algorithme réalise des mises à jour successives des différents pixels, et probabiliste car l&#39;algorithme simule des tirages aléatoires.

Voici le contenu des algorithmes.

&amp;nbsp;

### L&#39;échantillonneur de Gibbs :

Pour chacune des $n$ itérations de l&#39;algorithme, on balaye l&#39;ensemble des pixels. Pour chaque pixel (noté $s$) :

- Calcul de la probabilité locale, connaissant la configuration des voisins $V_s$ pour l&#39;image à l&#39;itération $n-1$ :

$$ \mathbb{P} (X_s = x_s | V_s) = \dfrac{\exp (-U_s(x_s | V_s))}{\sum_{a_s \in A_s}{\exp (-U_s(a_s | V_s))}}$$

- Mise à jour du site par tirage aléatoire selon la loi $\mathbb{P}(X_s = x_s | V_s)$

&amp;nbsp;

### L&#39;algorithme de Metropolis :

Pour chacune des $n$ itérations de l&#39;algorithme, on balaye l&#39;ensemble des pixels. Pour chaque pixel (noté $s$) :

- Tirage aléatoire de $\lambda$ dans $E$ selon une loi uniforme sur $E$:
- Calcul de la variation d&#39;énergie si la valeur de $s$, $x_s^{(n-1)}$, est remplacée par $\lambda$ :
    $$ \Delta U = U_s(\lambda | V_s^{(n-1)}) - U_s(x_s^{(n-1)} | V_s^{(n-1)}) $$
- Si $ \Delta U \leq 0$, alors on met à jour le pixel : $x_s^{(n)} = \lambda$
- Sinon, on met à jour selon la probabilité de succès $p = \exp (- \Delta U)$

&amp;nbsp;

## Implémentation de l&#39;algorithme de restauration d&#39;image

Après avoir implémenté un algorithme de tirage de configuration, il faut implémenter un algorithme convergeant vers une image solution du problème de minimisation de l&#39;énergie totale. Deux algorithmes sont principalement utilisés dans le cadre du traitement d&#39;image par champs de Markov.

&amp;nbsp;

### Le recuit simulé :

Le recuit simulé est une méthode classique de minimisation d&#39;énergie fréquemment utilisée en physique. En traitement d&#39;image, l&#39;algorithme consiste en $n$ itérations au cours desquelles on réalise des tirages de configuration tels que décrits en *(2.3.2)*. Cependant, ces tirages ne dépendent plus que de l&#39;énergie de configuration, mais aussi d&#39;une quantité $T^{(n)}$ qui mesure le degré d&#39;aléatoire introduit dans ces tirages, qu&#39;on nomme température et qui décroît à chaque itération. Partant d&#39;une température $T^{(0)}$ assez grande et de l&#39;image à traiter, l&#39;algorithme est le suivant :

Pour chaque itération $n$,

- Tirage d&#39;une configuration en remplaçant les énergies $U(x)$ par les quantités $U(x) / T^{(n)}$ pour les tirages de mise à jour des pixels
- Diminution de la température selon une décroissance logarithmique : $T^{(n)} &gt; \dfrac{c}{\log(2+n)}$

La décroissance logarithmique est nécessaire pour obtenir la convergence en probabilité de l&#39;algorithme vers l&#39;image qui minimise l&#39;énergie. En pratique, pour des images complexes ou grandes, cette décroissance est trop lente et on préfère utiliser une décroissance linéaire ou quadratique, ce qui peut provoquer une convergence vers un minimum seulement local de l&#39;énergie. Cependant, la taille ($200 \times 200$ pixels) et la simplicité des images que nous traitons nous autorise à utiliser cette décroissance logarithmique. On a observé de légères différences entre la décroissance logarithmique et la décroissance linéaire, ce qui est cohérent avec les notions de minimum local et minimum global de l&#39;énergie.

&amp;nbsp;

### Les modes conditionnels itérés :

La méthode *Iterated Conditional Mode (ICM)* consiste à tester *toutes* les nuances de gris pour chaque pixel et à mettre à jour avec la configuration qui permet la diminution d&#39;énergie la plus importante. Même si l&#39;on teste à chaque itération toutes les nuances de gris, l&#39;absence du caractère probabiliste (présent dans le recuit simulé) permet à l&#39;ICM de converger bien plus rapidement. En revanche, on ne converge pas forcément vers le minimum global.

L&#39;algorithme ICM est le suivant : Pour chacune des $n$ itérations de l&#39;algorithme, on balaye l&#39;ensemble des pixels. Pour chaque pixel (noté $s$) :

- $\forall \lambda \in E$, calcul de la variation d&#39;énergie si la valeur de $s$, $x_s^{(n-1)}$, est remplacée par $\lambda$ :

$$ \Delta U = U_s(\lambda | V_s^{(n-1)}) - U_s(x_s^{(n-1)} | V_s^{(n-1)}) $$

- Si $ \Delta U \leq 0$, alors mise à jour le pixel avec $\lambda$ qui minimise $ \Delta U$ : $x_s^{(n)} = \lambda$

&amp;nbsp;

# Résultats pratiques et discussion

Avant de pouvoir restaurer des images, il nous a fallu les détériorer. Pour cela, nous avons utilisé un bruit gaussien d&#39;amplitude 50. Il s&#39;agit de simuler une variable aléatoire gaussienne et de modifier la valeur des pixels de l&#39;image en ajoutant la valeur de la variable si cet ajout permet à la valeur du pixel de rester dans $E = [|0, 255|]$.

La figure ci-dessous montre l&#39;image que nous avons étudiée, ainsi que la même image brouillée avec un bruit gaussien d&#39;amplitude 50. Cette image est composée d&#39;un fond blanc (pixels de valeur $x_s = 255$), d&#39;un carré noir ($x_s = 0$), d&#39;une étoile gris foncé ($x_s = 70$), d&#39;un coeur gris ($x_s = 140$) et d&#39;un rond gris clair ($x_s = 210$).

















&lt;figure  id=&#34;figure-image-initiale-sans-bruit-à-gauche-et-avec-bruit-gaussien-damplitude-50-à-droite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures1.png&#34; alt=&#34;Image initiale sans bruit (à gauche) et avec bruit gaussien d’amplitude 50 (à droite).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image initiale sans bruit (à gauche) et avec bruit gaussien d’amplitude 50 (à droite).
    &lt;/figcaption&gt;&lt;/figure&gt;

Pour évaluer quantitativement la restauration d&#39;image, nous considérons le rapport signal sur bruit *(signal-to-noise ratio, SNR)*, qui s&#39;exprime en décibels (dB) et est donné par :

$$SNR = 10 \log \left(\dfrac{\sum_{s \in S} x_s^2}{\sum_{s \in S} (y_s-x_s)^2} \right)$$

où $x_s$ est la valeur du pixel $s$ de l&#39;image initiale non bruitée, et $y_s$ de même pour l&#39;image débruitée. Plus SNR est grand, moins le bruit détériore l&#39;image initiale. Pour savoir si le débruitage est efficace, il nous faudra comparer les valeurs avec le SNR obtenu avec l&#39;image bruitée. Le tableau 1 rescense notamment le SNR pour un bruit gaussien d&#39;amplitude 50.

| Amplitude | SNR (dB) |
|-----------|----------|
| 10        | 63.9516  |
| 25        | 45.2884  |
| 50        | 31.9855  |
| 100       | 20.3203  |

On rappelle qu&#39;on considère des images en niveaux de gris, car le temps de calcul est trop important pour des images en couleur ($E = [|0, 255|]^3$). Même pour des images très simples en niveaux de gris, les résultats obtenus sont loin d&#39;être parfaits, ce qui justifie de limiter notre étude à ce type d&#39;images.

Comme expliqué en *(2.3}*, il existe plusieurs méthodes de traitement d&#39;image par champs de Markov et chacune de ces méthodes possède ses propres paramètres. Pour réaliser un traitement d&#39;image, nous devons choisir les valeurs données à ces paramètres. Le tableau ci-dessous rescense l&#39;ensemble des paramètres avec les choix par défaut effectués, qui sont les choix qui ont donné les meilleurs résultats lors de l&#39;exécution de nos algorithmes.

Les principales différences dans nos résultats résident dans le choix de l&#39;algorithme de minimisation et dans le choix du modèle de potentiel.

On va appliquer à l&#39;image brouillée les différents algorithmes décrits précédemment, pour différentes valeurs des paramètres. On va d&#39;abord étudier le modèle de Potts, puis le modèle markovien gaussien.

&amp;nbsp;

## Première série d&#39;essais

On considère dans un premier temps que l&#39;on ne connaît rien de l&#39;image que l&#39;on doit obtenir. On implémente les algorithmes tels que décrits en *(2.3)* et on les exécute.

&amp;nbsp;

### Le modèle de Potts

#### Avec recuit simulé :

Étant donné la taille de l&#39;image, un balayage de l&#39;image bruitée montre que toutes les nuances de gris sont présentes sur l&#39;image : $E = [|0, 255|]$. On exécute alors le recuit simulé avec un algorithme de Metropolis qui tire $\lambda \in E$.

















&lt;figure  id=&#34;figure-image-traitée-par-recuit-simulé-et-modèle-de-potts-pour-beta-in-50-100-500-de-gauche-à-droite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures10.png&#34; alt=&#34;Image traitée par recuit simulé et modèle de Potts pour $\beta \in \\{50, 100, 500\\}$ (de gauche à droite).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par recuit simulé et modèle de Potts pour $\beta \in \{50, 100, 500\}$ (de gauche à droite).
    &lt;/figcaption&gt;&lt;/figure&gt;

| $\beta$   | SNR (dB) |
|-----------|----------|
| 50        | 2.73104  |
| 100       | 8.48355  |
| 500       | 11.3079  |
| 10000     | 11.6187  |

On constate que les résultats s&#39;améliorent lorsque $\beta$ augmente, avec un phénomène de saturation pour $\beta$ grand ($\beta \approx 10^4$). Cependant, les résultats sont pires que l&#39;image bruitée elle-même ! L&#39;image étant très bruitée, l&#39;algorithme tend à remplacer les pixels par n&#39;importe quelle valeur entre 0 et 255, ce qui ne permet pas de débruiter l&#39;image.

&amp;nbsp;

#### Avec ICM :

















&lt;figure  id=&#34;figure-image-traitée-par-icm-et-modèle-de-potts-pour-1-2-et-4-itérations-de-gauche-à-droite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures3.png&#34; alt=&#34;Image traitée par ICM et modèle de Potts pour 1, 2 et 4 itérations (de gauche à droite).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par ICM et modèle de Potts pour 1, 2 et 4 itérations (de gauche à droite).
    &lt;/figcaption&gt;&lt;/figure&gt;

| Itérations | SNR (dB) |
|------------|----------|
| 1          | 29.2689  |
| 2          | 27.3294  |
| 4          | 25.5964  |
| 10         | 23.1687  |

Les résultats sont très bons après quelques itérations. Même après une seule itération, le bruit a presque entièrement disparu. Toutefois, l&#39;ICM tend à rogner les figures, d&#39;où un SNR qui diminue lorsque le nombre d&#39;itérations augmente. Le SNR ne varie pas lorsqu&#39;on change $\beta$, donc  $\beta$ ne semble pas avoir d&#39;influence sur le résultat.

&amp;nbsp;

### Le modèle markovien gaussien

On considère désormais le modèle markovien gaussien, et on teste les deux algorithmes de minimisation d&#39;énergie avec et sans attache aux données initiales.

On constate que le modèle markovien gaussien floute les images. En effet, il tend à effectuer une sorte de moyenne locale. Or, l&#39;image étant brouillée par un bruit qui prend toutes les valeurs de $E = [|0, 255|]$, le moyennage homogénéise les différentes zones de gris en un gris moyen. Ainsi, même si le tableau 5 montre que le SNR est meilleur que sur l&#39;image brouillée, le rendu visuel et la disparition du bruit sur la zone blanche ne sont pas bons.

















&lt;figure  id=&#34;figure-image-traitée-par-recuit-simulé-en-haut-et-icm-en-bas-et-modèle-markovien-gaussien-sans-à-gauche-et-avec-à-droite-attache-aux-données&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures4.png&#34; alt=&#34;Image traitée par recuit simulé (en haut) et ICM (en bas) et modèle markovien gaussien sans (à gauche) et avec (à droite) attache aux données.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par recuit simulé (en haut) et ICM (en bas) et modèle markovien gaussien sans (à gauche) et avec (à droite) attache aux données.
    &lt;/figcaption&gt;&lt;/figure&gt;

| Méthode             | SNR (dB) |
|---------------------|----------|
| Recuit sans attache | 35.2715  |
| Recuit avec attache | 38.9796  |
| ICM sans attache    | 38.0792  |
| ICM avec attache    | 17.2107  |

Cette première série d&#39;essais montre que, mis à part pour l&#39;ICM avec modèle de Potts, les résultats de restauration d&#39;image sont mauvais. Pour le recuit simulé avec Potts, les mauvais résultats s&#39;expliquent par le tirage aléatoire sur les 255 nuances de gris, alors que dans l&#39;image initiale non bruitée, seules 5 nuances sont présentes.

&amp;nbsp;

## Seconde série d&#39;essais

On réalise désormais les tirages de configuration seulement parmi les nuances présentes sur l&#39;image initiale non bruitée : $\lambda \in E = \\{0, 70, 140, 210, 255\\}$.

&amp;nbsp;

### Le modèle de Potts

#### Avec recuit simulé :

















&lt;figure  id=&#34;figure-image-traitée-par-recuit-simulé-et-modèle-de-potts-pour-beta-in-5-25-35-50-100-500-de-gauche-à-droite-puis-de-haut-en-bas&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures2.png&#34; alt=&#34;Image traitée par recuit simulé et modèle de Potts pour $\beta \in \\{5, 25, 35, 50, 100, 500\\}$ (de gauche à droite puis de haut en bas).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par recuit simulé et modèle de Potts pour $\beta \in \{5, 25, 35, 50, 100, 500\}$ (de gauche à droite puis de haut en bas).
    &lt;/figcaption&gt;&lt;/figure&gt;

| $\beta$   | SNR (dB) |
|-----------|----------|
| 5         | 3.38126  |
| 25        | 3.18447  |
| 35        | 14.8631  |
| 50        | 22.8984  |
| 100       | 23.9018  |
| 500       | 25.9764  |

Les résultats sont bien meilleurs que lorsqu&#39;on ne connaît rien de l&#39;image initiale, même si le SNR reste inférieur à celui de l&#39;image bruitée. On constate ici l&#39;importance du paramètre $\beta$ : plus $\beta$ est grand, plus la taille des zones homogènes augmente. On a cependant toujours un phénomène de saturation pour $\beta$ grand. L&#39;image converge vers un état proche de l&#39;état initial.

&amp;nbsp;

#### Avec ICM :

Les résultats deviennent excellents : l&#39;image obtenue est quasiment l&#39;image initiale et le SNR est meilleur que pour l&#39;image bruitée. Le SNR montre aussi que $\beta$ n&#39;a toujours pas d&#39;influence.

&amp;nbsp;

### Le modèle markovien gaussien

Le modèle markovien gaussien donne aussi de biens meilleurs résultats. Les résultats sont légèrement meilleurs pour l&#39;ICM que pour le recuit simulé. L&#39;image obtenue lorsqu&#39;on ajoute l&#39;attache aux données initiales, qui permet notamment de corriger les effets néfastes de l&#39;ICM sur les bords, est extrêmement proche de l&#39;image initiale non bruitée.

















&lt;figure  id=&#34;figure-image-traitée-par-recuit-simulé-en-haut-et-icm-en-bas-et-modèle-markovien-gaussien-sans-à-gauche-et-avec-à-droite-attache-aux-données-et-connaissant-les-nuances-de-gris-présentes-dans-limage-initiale-non-brouillée&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures5.png&#34; alt=&#34;Image traitée par recuit simulé (en haut) et ICM (en bas) et modèle markovien gaussien sans (à gauche) et avec (à droite) attache aux données et connaissant les nuances de gris présentes dans l’image initiale non brouillée.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par recuit simulé (en haut) et ICM (en bas) et modèle markovien gaussien sans (à gauche) et avec (à droite) attache aux données et connaissant les nuances de gris présentes dans l’image initiale non brouillée.
    &lt;/figcaption&gt;&lt;/figure&gt;

| Méthode             | SNR (dB) |
|---------------------|----------|
| Recuit sans attache | 40.3498  |
| Recuit avec attache | 44.4864  |
| ICM sans attache    | 43.9946  |
| ICM avec attache    | 44.7768  |

&amp;nbsp;

#### Avec recuit simulé :

L&#39;influence de $\beta$ et le phénomène de saturation sont conservés : plus $\beta$ est grand, meilleure est la restauration. Le SNR est meilleur que pour l&#39;image bruitée.

Des modifications du paramètre $\alpha$ ont été réalisées sur les tests avec attache aux données, cependant les résultats étant déjà très bons pour $\alpha$ petit, $\alpha$ ne semble pas avoir une grande influence. On rappelle qu&#39;en théorie, plus $\alpha$ est grand, plus l&#39;attache aux données initiales est importante.

















&lt;figure  id=&#34;figure-image-traitée-par-recuit-simulé-et-modèle-markovien-gaussien-sans-en-haut-et-avec-en-bas-attache-aux-données-pour-beta-in-1-10-25-de-gauche-à-droite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures11.png&#34; alt=&#34;Image traitée par recuit simulé et modèle markovien gaussien sans (en haut) et avec (en bas) attache aux données pour $\beta \in \\{1, 10, 25\\}$ (de gauche à droite).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par recuit simulé et modèle markovien gaussien sans (en haut) et avec (en bas) attache aux données pour $\beta \in \{1, 10, 25\}$ (de gauche à droite).
    &lt;/figcaption&gt;&lt;/figure&gt;

| $\beta$   | SNR (dB) |
|-----------|----------|
| 1         | 40.7834  |
| 10        | 40.0753  |
| 25        | 40.0668  |
| 5000      | 39.6827  |

| $\beta$   | SNR (dB) |
|-----------|----------|
| 1         | 32.5507  |
| 10        | 38.6730  |
| 25        | 43.5519  |
| 5000      | 43.4116  |

&amp;nbsp;

#### Avec ICM :

On réalise les mêmes constats qu&#39;avec le recuit simulé. Le meilleur SNR parmi tous les essais est obtenu ici, pour $\beta = 20$ (tableau 10).

















&lt;figure  id=&#34;figure-image-traitée-par-icm-et-modèle-markovien-gaussien-avec-attache-aux-données-pour-beta-in-1-10-25-de-gauche-à-droite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/figures8.png&#34; alt=&#34;Image traitée par ICM et modèle markovien gaussien avec attache aux données pour $\beta \in \\{1, 10, 25\\}$ (de gauche à droite).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par ICM et modèle markovien gaussien avec attache aux données pour $\beta \in \{1, 10, 25\}$ (de gauche à droite).
    &lt;/figcaption&gt;&lt;/figure&gt;

| $\beta$   | SNR (dB) |
|-----------|----------|
| 1         | 19.7154  |
| 10        | 39.8086  |
| 20        | 46.9805  |
| 25        | 46.474   |
| 1000      | 43.4296  |
| 5000      | 43.3621  |

&amp;nbsp;

## Limites des méthodes markoviennes

L&#39;image traitée dans la première série de tests comportait des formes séparées avec des nuances de gris éloignées. Nous étudions désormais une image composée d&#39;un dégradé de gris pour montrer les limites des méthodes markoviennes.

















&lt;figure  id=&#34;figure-image-initiale-sans-bruit-à-gauche-et-avec-bruit-gaussien-damplitude-50-à-droite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/cercles1.png&#34; alt=&#34;Image initiale sans bruit (à gauche) et avec bruit gaussien d’amplitude 50 (à droite).&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image initiale sans bruit (à gauche) et avec bruit gaussien d’amplitude 50 (à droite).
    &lt;/figcaption&gt;&lt;/figure&gt;

On applique à l&#39;image les méthodes de recuit simulé et l&#39;ICM avec $\beta$ grand, modèle markovien gaussien avec attache aux données et connaissance des nuances initiales car ce sont les choix qui ont donné les meilleurs résultats jusqu&#39;à présent. Les résultats sont représentés en *figure 17*.

On constate que le recuit simulé avec modèle de Potts donne un résultat analogue à celui de la *figure 12* pour $\beta = 25$. Lorsqu&#39;on augmente $\beta$, le résultat ne s&#39;améliore pas et reste proche de celui en *figure 17*.

L&#39;ICM avec Potts tend à détériorer les bords comme constaté dans la première série de tests. Toutefois, les nuances de gris étant proches, cela provoque la disparition de certaines.

Le modèle markovien gaussien donne de très bons résultats quel que soit le potentiel. Le SNR obtenu est le double de celui de l&#39;image bruitée en *figure 16*, ce qui est même meilleur que pour les résultats de la *figure 15*.

Pour conclure cette étude et montrer les limites des méthodes markoviennes sur des nuances proches, nous avons testé les différents algorithmes sur une photo avec les 255 nuances de gris. La méthode donnant le meilleur SNR est le modèle markovien gaussien avec attache aux données, bien que ce modèle ait pour conséquence de flouter l&#39;image.

















&lt;figure  id=&#34;figure-image-traitée-par-recuit-simulé-à-gauche-et-icm-à-droite-pour-le-modèle-de-potts-en-haut-et-le-modèle-markovien-gaussien-avec-attache-aux-données-en-bas-pour-beta-grand&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/cercles2.png&#34; alt=&#34;Image traitée par recuit simulé (à gauche) et ICM (à droite) pour le modèle de Potts (en haut) et le modèle markovien gaussien avec attache aux données (en bas) pour $\beta$ grand.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image traitée par recuit simulé (à gauche) et ICM (à droite) pour le modèle de Potts (en haut) et le modèle markovien gaussien avec attache aux données (en bas) pour $\beta$ grand.
    &lt;/figcaption&gt;&lt;/figure&gt;

















&lt;figure  id=&#34;figure-image-initiale-sans-bruit-à-gauche-avec-bruit-gaussien-damplitude-50-au-milieu-et-image-traitée-par-modèle-markovien-gaussien-avec-attache-aux-données-à-droite-pour-beta-grand&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/restoration/resultatslenna.png&#34; alt=&#34;Image initiale sans bruit (à gauche), avec bruit gaussien d’amplitude 50 (au milieu) et image traitée par modèle markovien gaussien avec attache aux données (à droite) pour $\beta$ grand.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Image initiale sans bruit (à gauche), avec bruit gaussien d’amplitude 50 (au milieu) et image traitée par modèle markovien gaussien avec attache aux données (à droite) pour $\beta$ grand.
    &lt;/figcaption&gt;&lt;/figure&gt;

&amp;nbsp;

# Conclusion

Débruiter une image est une étape importante dans plusieurs domaines avancés comme la médecine ou la cartographie. Notre projet présente une méthode de débruitage d&#39;images en noir et blanc reposant sur la théorie des champs de Markov et sur les modèles physiques d&#39;Ising et de Potts. L&#39;étude théorique du modèle markovien met en avant la nécessité de choisir la bonne approche de minimisation, ainsi que de l&#39;algorithme d&#39;échantillonnage.

Nous avons été confrontés au problème de calibrage de différents paramètres. Les résultats montrent l&#39;efficacité de l&#39;algorithme ICM avec les potentiels issus du modèle markovien gaussien, qui, par comparaison aux autres modèles, présente une meilleure restitution de l&#39;image détériorée. La restauration est d&#39;autant meilleure que l&#39;on connaît des informations sur l&#39;image initiale.

&amp;nbsp;

---&gt;
</description>
    </item>
    
    <item>
      <title>Simplified Zuma video game</title>
      <link>https://michaelkarpe.github.io/programming-projects/zuma/</link>
      <pubDate>Fri, 02 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/programming-projects/zuma/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;presentation-of-the-project&#34;&gt;Presentation of the project&lt;/h1&gt;
&lt;p&gt;This project was carried out during a programming course at École des Ponts ParisTech. The goal of this project was to create in pairs and in about thirty hours a simple game based on the C++ language.&lt;/p&gt;
&lt;p&gt;We wanted to reproduce a Zuma game, as illustrated above, focusing on the algorithmic functioning of the game - graphics not being the priority of a C++ programming course, as you will see at the end of the article&amp;hellip;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;principle-of-the-game&#34;&gt;Principle of the game&lt;/h1&gt;
&lt;p&gt;&amp;ldquo;Zuma is a video game developed by PopCap Games, released in 2003. [&amp;hellip;] The goal is to form chains of balls of the same colour (which has the effect of making them disappear) before the balls arrive in the hole in the middle. The more levels progress, the more balls of different colours, the faster they go and the more courses there are. It is also possible to collect various bonuses (coins, various objects) that increase the score, and some balls have special powers (explosive, slowing down the game, etc.)&amp;rdquo;. &lt;em&gt;(Reference: French Wikipedia)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Thus for each level of the game, a global initialization of the level is made with the sending of a snake of balls, and the end of the level is defined according to certain criteria of difficulty: number of balls per snake, number of snakes, number of colors of balls, speed of movement of the snakes&amp;hellip;&lt;/p&gt;
&lt;p&gt;As long as the level is not finished, you have to manage :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the movement of the snakes&lt;/li&gt;
&lt;li&gt;the ball shot&lt;/li&gt;
&lt;li&gt;inserting the ball into the snake&lt;/li&gt;
&lt;li&gt;destruction of marbles if necessary ($3$ marbles or more of the same color aligned)&lt;/li&gt;
&lt;li&gt;the sending of another snake under conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;making-the-game-in-c&#34;&gt;Making the game in C++&lt;/h1&gt;
&lt;p&gt;Our main objective was to reproduce the scrolling of the log snakes around the frog, as well as the shooting of the frog&amp;rsquo;s logs to eliminate the log snakes. To do this, we created different classes briefly described below :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;zuma&lt;/em&gt;: launch and general operation of the game&lt;/li&gt;
&lt;li&gt;&lt;em&gt;frog&lt;/em&gt;: choice of shooting ball and shooting of balls&lt;/li&gt;
&lt;li&gt;&lt;em&gt;snake&lt;/em&gt;: creating and moving snakes, adding and destroying beads.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ball&lt;/em&gt;: creation and movement of balls&lt;/li&gt;
&lt;li&gt;&lt;em&gt;level&lt;/em&gt; : difficulty criteria of the current level (number of balls, snakes, colors, speed&amp;hellip;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;trajectory&lt;/em&gt; : path on which the snakes of marbles move&lt;/li&gt;
&lt;li&gt;&lt;em&gt;tools&lt;/em&gt;: global functions and parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;expectation-vs-reality&#34;&gt;Expectation vs. reality&lt;/h1&gt;
&lt;p&gt;Although the game was operational at the end of the time limit, we didn&amp;rsquo;t have time to work on the graphics&amp;hellip;&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/zuma/zumareal.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

&amp;nbsp;

# Présentation du projet

Ce projet a été réalisé pendant un cours de programmation à l&#39;École des Ponts ParisTech. Le but de ce projet était de réaliser par binôme et en une trentaine d&#39;heures un jeu simple basé sur le langage C++.

Nous avons voulu reproduire un jeu de Zuma, comme illustré ci-dessus, en nous concentrant sur le fonctionnement algorithmique du jeu - les graphismes n&#39;étant pas la priorité d&#39;un cours de programmation en C++, comme vous pourrez l&#39;observer à la fin de l&#39;article...

&amp;nbsp;

# Principe du jeu

&#34;Zuma est un jeu vidéo développé par PopCap Games, sorti en 2003. [...] Le but est de former des chaînes de boules de même couleur (ce qui a pour effet de les faire disparaître) avant l&#39;arrivée des boules dans le trou au milieu. Plus les niveaux avancent, plus il y a de boules de différentes couleurs, plus elles vont vite et plus les parcours sont nombreux. Il est aussi possible de récupérer différents bonus (pièces, objets divers) qui permettent d&#39;augmenter le score, et certaines boules ont des pouvoirs particuliers (explosives, ralentissement du jeu, etc.).&#34; *(Référence : Wikipédia)*

Ainsi pour chaque niveau du jeu, une initialisation globale du niveau est réalisée avec l&#39;envoi d&#39;un serpent de billes, et la fin du niveau est définie selon certains critères de difficulté : nombre de billes par serpent, nombre de serpents, nombre de couleurs de billes, vitesse de déplacement des serpents...

Tant que le niveau n&#39;est pas fini, il convient de gérer :
- le déplacement des serpents
- le tir de bille
- l&#39;insertion de la bille dans le serpent
- la destruction de billes si nécessaire ($3$ billes ou plus de même couleur alignées)
- l&#39;envoi d&#39;un autre serpent sous conditions

&amp;nbsp;

# Réalisation du jeu en C++

Notre objectif principal était de reproduire le défilement des serpents de billes autour de la grenouille, ainsi que les tirs de billes de la grenouille pour éliminer les serpents de billes. Pour cela, nous avons créé différentes classes brièvement décrites ci-dessous :

- *zuma* : lancement et fonctionnement général du jeu
- *grenouille* : choix de la bille de tir et tir de billes
- *serpent* : création et déplacement des serpents, ajout et destruction de billes
- *bille* : création et mouvement des billes
- *niveau* : critères de difficulté du niveau en cours (nombre de billes, serpents, couleurs, vitesse...)
- *trajectoire* : chemin sur lequel se déplacent les serpents de billes
- *outils* : fonctions et paramètres globaux

&amp;nbsp;

# Espérance contre réalité

Si le jeu était opérationnel à la fin du temps imparti, nous n&#39;avons en revanche pas eu le temps de travailler les graphismes...

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/zuma/zumareal.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

---&gt;
</description>
    </item>
    
    <item>
      <title>Optimal Course Allocation</title>
      <link>https://michaelkarpe.github.io/optimization-projects/allocation/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/optimization-projects/allocation/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;presentation-of-the-project&#34;&gt;Presentation of the project&lt;/h1&gt;
&lt;p&gt;As part of the management of the assignment of first year students to second semester courses and projects at Ecole des Ponts, we were asked to design a computer program that manages the assignment of students to courses. In order to meet the requirements of the first year department, this program must comply with a number of constraints.&lt;/p&gt;
&lt;p&gt;Our objective is to optimize the process of assigning students to courses and projects, by proposing a distribution of students that respects as much as possible the ranking of their wishes. Indeed, we have three main sets of courses and projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the department projects: there are $20$ projects in $2017$ and they take place on Monday afternoons.&lt;/li&gt;
&lt;li&gt;the opening courses: they are spread over $4$ days, each day having a well-specified set of courses that the students have to classify.&lt;/li&gt;
&lt;li&gt;introductory research projects: these are associated with the opening classes. Each opening course has a project on the same theme. As a result, every student must necessarily be assigned to a project that is associated with one of the courses he or she is taking.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, each course and each project has a maximum number of students and a minimum number of students required to open the course. These numbers may vary from one course to another. Finally, some courses or projects may be discontinued if demand is not high enough.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;mathematical-modelling&#34;&gt;Mathematical modelling&lt;/h1&gt;
&lt;h2 id=&#34;optimization-program-for-a-single-project--course-choice&#34;&gt;Optimization program for a single project / course choice&lt;/h2&gt;
&lt;p&gt;In order to account for the fact that a student has or has not received his first wish, we have introduced a regret function, the value of which is all the higher as the first wishes are not fulfilled. Our modeling aims at minimizing the total regret function. For a given project, the regret function of a student corresponds to the weight function evaluated at the wish number of the project. The weight function is a function that associates a number to a wish number that is higher the higher the wish number is, and is worth $0$ if it is the student&amp;rsquo;s $1$ wish. An example of a quadratic weight function ($f(x) = (x - 1)^2$) is given below :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Rank&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In order to know to which project a student is assigned, we use a matrix $A = (a_{e,p})&lt;em&gt;{e,p}$ whose coefficient $a&lt;/em&gt;{e,p}$ is $1$ if student $e$ is assigned to project $p$ and $0$ otherwise. Given this, the regret function of a student for a given project is $\sum_{p\in P} a_{e,p} w(c_{e,p})$. This minimization problem comes with several constraints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The enrolment of the project numbered $p$, noted $n_{p}$ and defined as the sum over the students of the assignments $a_{e,p}$, must be between the minimum $m_{p}$ and the maximum $M_{p}$ enrolment associated with this project. These bounds are multiplied by a binary variable $\delta_{p}$ which equals $0$ if the project is closed and $1$ if it is open. Thus, if there are not enough students who have applied for a given project, that is, if it is not in the first wishes of a sufficient number of students, then the project in question will have a zero enrolment.&lt;/li&gt;
&lt;li&gt;Each student must be assigned to one project only. For this purpose, the sum of the student&amp;rsquo;s $a_{e,p}$ assignments on projects is equal to $1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mathematically, the problem thus modeled is written as follows:&lt;/p&gt;
&lt;p&gt;$$\text{min} \sum_{e \in E} \sum_{p \in P} a_{e, p}.w(c_{e, p}) + Kr_{max}$$
$$\text{s.t.} \ \forall p \in P, \ n_p = \sum_{e\in E} a_{e, p}$$
$$\forall p \in P, \ \delta_p.m_p \leq n_p$$
$$\forall p \in P, \ n_p \leq \delta_p.M_p$$
$$\forall e \in E, \ \sum_{p \in P} a_{e, p} = 1$$
$$\forall p \in P, \ \delta_p \in {0, 1}$$
$$\forall p \in P, \ \forall e \in E, \ a_{e, p} \in {0, 1}$$&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;settings&#34;&gt;Settings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$J$: All days, two courses or projects on the same day cannot be cumulated.&lt;/li&gt;
&lt;li&gt;$E$: All students.&lt;/li&gt;
&lt;li&gt;$P$: Set of all projects / courses to be chosen partitioned according to days $P = \bigcup_{j \in J} P_j$.&lt;/li&gt;
&lt;li&gt;$C$: Matrix of students&amp;rsquo; choices tq. $c_{e, p}$ is the rank of the project $p$ for the student $e$.&lt;/li&gt;
&lt;li&gt;$m_p$ and $M_p$: Minimum and maximum numbers for the $p$ project.&lt;/li&gt;
&lt;li&gt;$w$: Weight function.&lt;/li&gt;
&lt;li&gt;$pr$: Function that, with an opening course, associates a project of initiation to the search.&lt;/li&gt;
&lt;li&gt;$K$: Constant increasing total regret.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;variables&#34;&gt;Variables&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$\delta_p = 1$ if project $p$ is open, $0$ otherwise.&lt;/li&gt;
&lt;li&gt;$A$ : Student assignment matrix tq $a_{e, p} = $1 if student $e$ is assigned to project $p$, $0$ otherwise.&lt;/li&gt;
&lt;li&gt;$n_p$ : Enrollment in project $p$.&lt;/li&gt;
&lt;li&gt;$r_{max}$ : Maximum pupil regret. May be a parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;optimization-program-for-multiple-project-or-course-choices&#34;&gt;Optimization program for multiple project or course choices&lt;/h2&gt;
&lt;p&gt;When there are several projects or courses to choose from, the allocation results must take into account each other. For example, if a student is assigned to a Wish $3$, their other assignments should correspond, as much as possible, to Wish $1$ courses. This is accomplished by adding a variable $r_{max}$ which corresponds to the maximum regret per student. Each student&amp;rsquo;s regret ($\sum_{p \in P} a_{e, p}w(c_{e, p})$) is therefore increased by $r_{max}$.&lt;/p&gt;
&lt;p&gt;In order to minimize this maximum regret, this variable is added to the total regret in the function to be minimized with a multiplicative coefficient $K$. This coefficient is a major of the total regret, therefore a very large number, so that the maximum regret per student only increases if there is no other possibility.&lt;/p&gt;
&lt;p&gt;Finally, it must be ensured that if a student is assigned to a research project, then he or she is necessarily assigned to the associated opening course. Consequently, the assignment variable $a_{e,p}$ to a given opening course $p$ increases the assignment variable $a_{e, pr(p)}$ of the associated research project. Hence the reformulation of the previous problem as :&lt;/p&gt;
&lt;p&gt;$$\text{min} \sum_{e \in E} \sum_{p \in P} a_{e, p}w(c_{e, p}) + Kr_{max}$$
$$\textrm{s.c.} \ \forall p \in P, \ n_p = \sum_{e_\in E} a_{e, p}$$
$$\forall p \in P, \ \delta_p.m_p \leq n_p$$
$$\forall p \in P, \ n_p \leq \delta_p.M_p$$
$$\forall e \in E, \ \sum_{p \in P} a_{e, p}.w(c_{e, p}) \leq r_{max}$$
$$\forall j \in J, \ \sum_{p \in P_j} a_{e, p} = 1$$
$$\forall e \in E, \ \forall p \in P_{\text{c.o.}}, \ a_{e, pr(p)} \leq a_{e, p}$$
$$\forall p \in P, \ \delta_p \in {0, 1}$$
$$\forall p \in P, \ \forall e \in E, \ a_{e, p} \in {0, 1}$$&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;h2 id=&#34;year-2017&#34;&gt;Year 2017&lt;/h2&gt;
&lt;p&gt;We compared our program to the $2017$ allocation results. We could see in the modeling that the fundamental parameter of the optimization is the $w$ weight function. By playing with the weights, we can change the result of the optimization entirely. In the case of our problem, we want to give massively first or second choices, while exceptionally allowing third choices. To do this, we worked with the weight function below, as well as with another weight function where the weight of the $3$ rank goes from $15$ to $50$.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Rank&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s compare the results obtained for these two weight functions with the results of the current optimization. The results are summarized in the table below, since with our optimization program we obtain the same results as those obtained manually by the administration: they were already at an optimum.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Choice&lt;/th&gt;
&lt;th&gt;Monday&lt;/th&gt;
&lt;th&gt;Tuesday&lt;/th&gt;
&lt;th&gt;Wednesday&lt;/th&gt;
&lt;th&gt;Thursday&lt;/th&gt;
&lt;th&gt;Research&lt;/th&gt;
&lt;th&gt;Department&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;137&lt;/td&gt;
&lt;td&gt;126&lt;/td&gt;
&lt;td&gt;114&lt;/td&gt;
&lt;td&gt;111&lt;/td&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;td&gt;107&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;td&gt;35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Above all, our software saves time since it runs in about $30$ seconds. The method previously used required to manually take over the results of the algorithm and lasted about $15$ hours. The time saved is therefore considerable.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;year-2018&#34;&gt;Year 2018&lt;/h2&gt;
&lt;p&gt;Due to the very good results of our software in $2017$, it has been used every year since then by the first year department. Here are the results obtained for the year $2018$, where $152$ students had to grant wishes, compared to $142$ the year before:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Choice&lt;/th&gt;
&lt;th&gt;Monday&lt;/th&gt;
&lt;th&gt;Tuesday&lt;/th&gt;
&lt;th&gt;Wednesday&lt;/th&gt;
&lt;th&gt;Thursday&lt;/th&gt;
&lt;th&gt;Research&lt;/th&gt;
&lt;th&gt;Department&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;121&lt;/td&gt;
&lt;td&gt;143&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;td&gt;102&lt;/td&gt;
&lt;td&gt;NC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;31&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;td&gt;NC&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;NC&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The results are very satisfying, although there is an increase in the number of wishes $3$. This is mainly due to the fact that despite the increase in the number of students, the number of students in the proposed courses has not been revised upwards.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

&amp;nbsp;

# Présentation du projet

Dans le cadre de la gestion de l&#39;affectation des élèves de première année aux cours et projets du second semestre à l&#39;Ecole des Ponts, il nous a été demandé de concevoir un programme informatique qui gère l&#39;affectation des élèves aux cours. Pour qu&#39;il réponde aux exigences du département de première année, ce programme doit respecter un certain nombre de contraintes.

Notre objectif est d&#39;optimiser le processus d&#39;affectation des élèves aux cours et aux projets, en proposant une répartition des élèves qui respecte au maximum le classement de leurs voeux. En effet, nous disposons de trois grands ensembles de cours et de projets :
- les projets de département : ils sont au nombre de $20$ en $2017$ et se déroulent le lundi après-midi.

- les cours d&#39;ouverture : ils se répartissent sur $4$ jours, chaque jour disposant d&#39;un ensemble bien spécifié de cours que les élèves doivent classer.
- les projets d&#39;initiation à la recherche : ils sont associés aux cours d&#39;ouverture. Chaque cours d&#39;ouverture dispose d&#39;un projet qui porte sur le même thème. De ce fait, tout élève doit nécessairement être affecté à un projet qui est associé à l&#39;un des cours qu&#39;il suit.

D&#39;autre part, chaque cours et chaque projet possède un effectif maximal d&#39;élèves et un effectif minimal nécessaire à l&#39;ouverture du cours. Ces effectifs peuvent varier d&#39;un cours à un autre. Enfin, certains cours ou projets peuvent être supprimés si la demande n&#39;est pas assez forte.

&amp;nbsp;

# Modélisation mathématique

## Programme d&#39;optimisation pour un seul choix de projet/cours

Afin de rendre compte du fait qu&#39;un élève a ou n&#39;a pas reçu son premier voeu, nous avons introduit une fonction de regret dont la valeur est d&#39;autant plus élevée que les premiers voeux ne sont pas remplis. Notre modélisation vise à minimiser la fonction de regret total. Pour un projet donné, la fonction de regret d&#39;un élève correspond à la fonction de poids évaluée au numéro de voeu du projet. La fonction de poids est une fonction qui, à un numéro de voeu, associe un nombre d&#39;autant plus élevé que le numéro du voeu est élevé, et qui vaut $0$ s&#39;il s&#39;agit du voeu $1$ de l&#39;élève. Un exemple de fonction de poids quadratique ($f(x) = (x - 1)^2$) est donné ci-dessous :

| Rang     | Poids    |
|----------|----------|
| 1        | 0        |
| 2        | 1        |
| 3        | 4        |
| 4        | 9        |
| 5        | 16       |
| ...      | ...      |

Afin de savoir à quel projet un élève est affecté, on utilise une matrice $A = (a_{e,p})_{e,p}$ dont le coefficient $a_{e,p}$ vaut $1$ si l&#39;élève e est affecté au projet p et $0$ sinon. Compte tenu de cela, la fonction de regret d&#39;un élève pour un projet donné est $\sum_{p\in P} a_{e,p} w(c_{e,p})$. Ce problème de minimisation s&#39;accompagne de plusieurs contraintes:

- L&#39;effectif du projet numéroté $p$, noté $n_{p}$ et défini comme la somme sur les élèves des affectations $a_{e,p}$, doit être compris entre l&#39;effectif minimum $m_{p}$ et l&#39;effectif maximum $M_{p}$ associés à ce projet. Ces bornes sont multipliées par une variable binaire $\delta_{p}$ qui est égale à $0$ si le projet est fermé et $1$ s&#39;il est ouvert. Ainsi, s&#39;il n&#39;y a pas assez d&#39;élèves qui ont demandé un projet donné, c&#39;est-à-dire qu&#39;il ne figure pas dans les premiers voeux d&#39;un nombre suffisant d&#39;élèves, alors le projet en question aura un effectif nul.
- Chaque élève doit être affecté à un seul et unique projet. À cet effet, la somme sur les projets des affectations $a_{e,p}$ de l&#39;élève est égale à $1$.

Mathématiquement, le problème ainsi modélisé s&#39;écrit :

$$\text{min} \sum_{e \in E} \sum_{p \in P} a_{e, p}.w(c_{e, p}) + Kr_{max}$$
$$\text{s.c.} \ \forall p \in P, \ n_p = \sum_{e\in E} a_{e, p}$$
$$\forall p \in P, \ \delta_p.m_p \leq n_p$$
$$\forall p \in P, \ n_p \leq \delta_p.M_p$$
$$\forall e \in E, \ \sum_{p \in P} a_{e, p} = 1$$
$$\forall p \in P, \ \delta_p \in \{0, 1\}$$
$$\forall p \in P, \ \forall e \in E, \ a_{e, p} \in \{0, 1\}$$

&amp;nbsp;

### Paramètres

- $J$ : Ensemble des jours, deux cours ou projets le même jour ne peuvent pas être cumulés.
- $E$ : Ensemble des élèves.
- $P$ : Ensemble de tous les projets / cours à choisir partitionnés selon les jours $P = \bigcup_{j \in J} P_j$.
- $C$ : Matrice des choix des élèves tq. $c_{e, p}$ est le rang du projet $p$ pour l&#39;élève $e$.
- $m_p$ et $M_p$ : Effectifs minimal et maximal pour le projet $p$.
- $w$ : Fonction de poids.
- $pr$ : Fonction qui, à un cours d&#39;ouverture, associe un projet d&#39;initiation à la recherche.
- $K$ : Constante majorant le regret total.

&amp;nbsp;

### Variables

- $\delta_p = 1$ si le projet p est ouvert, $0$ sinon.
- $A$ : Matrice d&#39;affectation des élèves tq $a_{e, p} = 1$ si l&#39;élève $e$ est affecté au projet $p$, $0$ sinon.
- $n_p$ : Effectif dans le projet $p$.
- $r_{max}$ : Regret par élève maximal. Éventuellement un paramètre.

&amp;nbsp;

## Programme d&#39;optimisation pour plusieurs choix de projets ou cours

Lorsqu&#39;il y a plusieurs projets ou cours à choisir, il faut que les résultats d&#39;affectation tiennent compte les uns des autres. Ainsi, si par exemple un étudiant est affecté à un projet placé en voeu $3$, ses autres affectations doivent correspondre, dans la mesure du possible, à des cours placés en voeu 1. Pour cela, on ajoute une variable $r_{max}$ qui correspond au regret maximal par élève. Le regret de chaque élève ($\sum_{p \in P} a_{e, p}w(c_{e, p})$) est donc majoré par $r_{max}$.

De façon à minimiser ce regret maximum, on ajoute cette variable au regret total dans la fonction à minimiser avec un coefficient multiplicatif $K$. Ce coefficient est un majorant du regret total, donc un nombre très grand, de façon à ce que le regret maximal par élève n&#39;augmente que s&#39;il n&#39;y a pas d&#39;autre possibilité.

Enfin, il faut faire en sorte que si un élève est attribué à un projet de recherche, alors il est nécessairement attribué au cours d&#39;ouverture associé. En conséquence, la variable d&#39;affectation $a_{e,p}$ à un cours d&#39;ouverture donné $p$ majore la variable d&#39;affectation $a_{e, pr(p)}$ du projet de recherche associé. D&#39;où la reformulation du problème précédent sous la forme :

$$\text{min} \sum_{e \in E} \sum_{p \in P} a_{e, p}w(c_{e, p}) + Kr_{max}$$
$$\textrm{s.c.} \ \forall p \in P, \ n_p = \sum_{e_\in E} a_{e, p}$$
$$\forall p \in P, \ \delta_p.m_p \leq n_p$$
$$\forall p \in P, \ n_p \leq \delta_p.M_p$$
$$\forall e \in E, \ \sum_{p \in P} a_{e, p}.w(c_{e, p}) \leq r_{max}$$
$$\forall j \in J, \ \sum_{p \in P_j} a_{e, p} = 1$$
$$\forall e \in E, \ \forall p \in P_{\text{c.o.}}, \ a_{e, pr(p)} \leq a_{e, p}$$
$$\forall p \in P, \ \delta_p \in \{0, 1\}$$
$$\forall p \in P, \ \forall e \in E, \ a_{e, p} \in \{0, 1\}$$

&amp;nbsp;

# Résultats

## Année 2017

Nous avons comparé notre programme aux résultats d&#39;affectation obtenus en $2017$. Nous avons pu voir dans la modélisation que le paramètre fondamental de l&#39;optimisation est la fonction de poids $w$. En jouant sur les poids, on peut changer entièrement le résultat de l&#39;optimisation. Dans le cas de notre problème, on souhaite donner massivement des premiers ou des deuxièmes choix, tout en autorisant exceptionnellement les troisièmes choix. Pour cela, nous avons travaillé avec la fonction de poids ci-dessous, ainsi qu&#39;avec une autre fonction de poids où le poids du rang $3$ passe de $15$ à $50$.

| Rang     | Poids    |
|----------|----------|
| 1        | 0        |
| 2        | 5        |
| 3        | 15       |
| 4        | 1000     |
| 5        | 10000    |
| ...      | ...      |

Comparons les résultats obtenus pour ces deux fonctions de poids avec les résultats de l&#39;optimisation actuelle. Les résultats sont synthétisés dans le tableau ci-dessous, puisque nous obtenons avec notre programme d&#39;optimisation les mêmes résultats que ceux obtenus manuellement par l&#39;administration : ils correspondaient donc déjà à un optimum.

| Choix | Lundi | Mardi | Mercredi | Jeudi | Recherche | Département |
|-------|-------|-------|----------|-------|-----------|-------------|
| 1     | 137   | 126   | 114      | 111   | 92        | 107         |
| 2     | 5     | 16    | 28       | 31    | 45        | 35          |
| 3     | 0     | 0     | 0        | 0     | 5         | 0           |

Notre logiciel offre surtout un gain de temps puisqu&#39;il tourne en une trentaine de secondes. La méthode précédemment employée obligeait à reprendre manuellement les résultats de l&#39;algorithme et durait environ $15$ heures. Le gain de temps est donc considérable.

&amp;nbsp;

## Année 2018

En raison des très bons résultats de notre logiciel en $2017$, celui-ci est depuis utilisé tous les ans par le département de première année. Voici les résultats obtenus pour l&#39;année $2018$, où $152$ élèves ont eu à réaliser des voeux, contre $142$ l&#39;année précédente :

| Choix | Lundi | Mardi | Mercredi | Jeudi | Recherche | Département |
|-------|-------|-------|----------|-------|-----------|-------------|
| 1     | 121   | 143   | 120      | 110   | 102       | NC          |
| 2     | 30    | 7     | 31       | 40    | 43        | NC          |
| 3     | 1     | 2     | 1        | 2     | 7         | NC          |

Les résultats sont très satisfaisants, bien que l&#39;on constate une augmentation du nombre de voeux $3$. Cela est notamment dû au fait que malgré l&#39;augmentation du nombre d&#39;élèves, les effectifs des cours proposés n&#39;ont pas été revus à la hausse.

&amp;nbsp;

---&gt;
</description>
    </item>
    
    <item>
      <title>★ Perfect Trajectory Forecasts in Billiards</title>
      <link>https://michaelkarpe.github.io/programming-projects/billiards/</link>
      <pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/programming-projects/billiards/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;presentation-of-the-project&#34;&gt;Presentation of the project&lt;/h1&gt;
&lt;p&gt;Many pool players, myself included, would dream of pocketing all the balls one after the other. However, because of the many parameters that govern the movement of the balls, this is difficult, but not impossible. So we&amp;rsquo;re going to look for the perfect trajectory in billiards to find out &amp;ldquo;how to pocket a billiard ball for sure&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;*Since we are trying to make trajectories as predictable as possible, we will consider in our study balls moving without effect, therefore in a straight line.&lt;/p&gt;
&lt;p&gt;We will first focus on an experimental study of friction, then on the theoretical study of the $3$ main physical laws governing motion, and finally I will develop the design of computer programs that will allow us to determine the perfect trajectories to pocket a ball.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;experimental-study-of-friction&#34;&gt;Experimental study of friction&lt;/h1&gt;
&lt;p&gt;First of all, we will look at the friction of the ball with the mat, which is obviously the cause of the loss of speed of the ball. To study the evolution of this speed, I proceeded to a video scoring with the &lt;em&gt;LatisPro&lt;/em&gt; software, after having placed a mark and a $1.82 $m standard corresponding to the length of the billiard table.&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/latispro.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;The software&amp;rsquo;s zoom allows for very accurate readings, however, the video has a frame rate of $30$ frames per second, so the ball only moves a few pixels between frames. The plot of the speed evolution on Excel is then rather chaotic. I then measured the position of the ball every $5$ images, to see a linear evolution of the speed, with a deceleration of $0.17$ $m.s^{-2}$, and an uncertainty on this deceleration of $0.044$ $m.s^{-2}$ obtained by the method of least squares :&lt;/p&gt;
&lt;p&gt;$$a = -0,170 \pm 0,044 \ m.s^{-2}$$&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/regression.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;theoretical-study-of-the-3-main-laws-of-motion&#34;&gt;Theoretical study of the 3 main laws of motion&lt;/h1&gt;
&lt;p&gt;We are now moving on to the theoretical study to justify this constant deceleration, as well as the movement of the balls during the rebounds on the belts and during the shocks between balls.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;study-of-rebounds&#34;&gt;Study of rebounds&lt;/h2&gt;
&lt;p&gt;When considering rigid bands, bounces follow Descartes&amp;rsquo; law of reflection, analogous to light rays in optics.&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/rebond.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;study-of-shocks&#34;&gt;Study of shocks&lt;/h2&gt;
&lt;p&gt;Under the hypothesis of balls evolving without effect, the shocks between balls are governed by the laws of elastic shocks, i.e. conservation of the quantity of motion and kinetic energy :&lt;/p&gt;
&lt;p&gt;$$\overrightarrow{p_1} = \overrightarrow{p_1&amp;rsquo;} + \overrightarrow{p_2&amp;rsquo;}$$
$$\frac{1}{2}mv_1^2 = \frac{1}{2}mv_1&amp;rsquo;^2 + \frac{1}{2}mv_2&amp;rsquo;^2$$&lt;/p&gt;
&lt;p&gt;This involves deflecting the balls along paths forming an angle $\alpha = 90°$.&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/choc.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;study-of-friction&#34;&gt;Study of friction&lt;/h2&gt;
&lt;p&gt;Finally, as far as friction is concerned, a distinction is theoretically made between two rolling phases. However, we will only be interested in the second phase of rolling without slip, the first phase of rolling with slip taking place only for a negligible period of time. The position of the ball during the rolling phase with slippage is given by the formula:&lt;/p&gt;
&lt;p&gt;$$\overrightarrow{OG}(t) = \overrightarrow{OG}(0) + \overrightarrow{v_{/R}}(G, 0)t - \frac{\overrightarrow{v_{/R}}(I, 0)}{||\overrightarrow{v_{/R}}(I, 0)||} \frac{fgt^2}{2}$$&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/roulement.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;It is shown with the fundamental principle of dynamics and the theorem of the kinetic moment that, since the ball is rolling without slipping, one cannot neglect the force of resistance to the advance due to the sinking of the ball into the belt:&lt;/p&gt;
&lt;p&gt;$$\overrightarrow{F} = -f_cmg\frac{\overrightarrow{v_{/R}}(G)}{||\overrightarrow{v_{/R}}(G)||}$$&lt;/p&gt;
&lt;p&gt;Régis Petit, engineer-researcher, tells us in his &lt;em&gt;Théorie du jeu sur le billard&lt;/em&gt; that this force is opposed to speed, and proportional to a coefficient $f_c$ :&lt;/p&gt;
&lt;p&gt;$$f_c = \frac{\sin(\gamma)}{0.4 + \cos(\gamma)}$$&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/glissement.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;A good approximation of this coefficient is obtained by measuring the time $t$ taken by the ball to travel the $L$ length of the billiard table, arriving at this $L$ length at zero speed:&lt;/p&gt;
&lt;p&gt;$$f_c \approx \frac{2L}{gt^2}$$&lt;/p&gt;
&lt;p&gt;For our experimental billiard table, it takes $5.5$ $s$ to the ball to go through the $1.82$ $m$ arriving at zero speed, which corresponds to an angle $\gamma = 1°$ and a coefficient $f_c = 1.25%$. These results are consistent since billiards are designed so that friction is as negligible as possible.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;trajectory-research-program-design&#34;&gt;Trajectory research program design&lt;/h1&gt;
&lt;p&gt;Now that we know the laws governing the movement of marbles, we will be able to make computer programs that will show us the perfect trajectories to pocket a marble.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In order to propose predictable and easily achievable trajectories on a real billiard game, we consider that only the cue ball moves, only bouncing, and $2$ maximum. When it hits the colored ball that we want to pocket, the colored ball goes directly into the hole without rebounding.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;searching-for-the-set-of-trajectories-to-reach-a-ball&#34;&gt;Searching for the set of trajectories to reach a ball&lt;/h2&gt;
&lt;p&gt;As a first step, we need to carry out programs that trace the billiard table, the balls and the trajectories, so that we can check the proper functioning of our future programs.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;performing-a-graphical-simulation&#34;&gt;Performing a graphical simulation&lt;/h3&gt;
&lt;p&gt;The main plotting program I have made is a program that takes as argument a number $n$, randomly plots $n$ balls on the billiard table, and returns the list of coordinates of the balls. A function traces the trajectory passing through a given position, and another function traces the trajectory after a bounce on a strip according to Descartes&amp;rsquo; law. These two functions return the coordinates of the impact of the balls on the boards.&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/tracealeatoire.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;searching-for-positions-to-reach-to-pocket-a-ball&#34;&gt;Searching for positions to reach to pocket a ball&lt;/h3&gt;
&lt;p&gt;Once these functions were completed, I first looked at where to hit a ball so that it would go into a hole. The elastic shock hypothesis implies that if the colored ball is stationary, there is a single shock position that allows the ball to be pocketed in a given hole. This position is defined by the relations presented here, knowing that at the moment of impact, the two balls and the hole must be aligned:&lt;/p&gt;
&lt;p&gt;$$a^2 + b^2 = 4R^2 \text{ and } m = \frac{y_C - y_B}{x_C - x_B} = \frac{b}{a}$$&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/positionchoc.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Since we know the coordinates of the colored ball and the holes, we can determine $m$, then deduce $a$ and $b$ which define the coordinates of the shock position :&lt;/p&gt;
&lt;p&gt;$$b = m \cdot a \text{ and } a = \pm \frac{2R}{\sqrt{1 + m^2}}$$&lt;/p&gt;
&lt;p&gt;We can then realize a function which, taking as argument the coordinates of a ball, returns the coordinates of the $6$ hit positions corresponding to the $6$ holes of the billiard table, and draws these positions on the graphical interface.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;determination-of-the-set-of-possible-paths&#34;&gt;Determination of the set of possible paths&lt;/h3&gt;
&lt;p&gt;We then have to see if these positions are accessible or not, i.e. if they can be reached in $2$ maximum rebounds.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h4 id=&#34;direct-or-parallel-strip-shooting&#34;&gt;Direct or parallel strip shooting&lt;/h4&gt;
&lt;p&gt;In direct shooting, the trajectory is simply a segment delimited by the cue ball and the cue position. With one or two rebounds on parallel stripes, i.e. like this or like that, any position on the billiard table is accessible.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h4 id=&#34;shooting-in-perpendicular-strips&#34;&gt;Shooting in perpendicular strips&lt;/h4&gt;
&lt;p&gt;In two bounces on perpendicular strips, on the other hand, like this for example, the whole billiard table is not accessible. Indeed, in this configuration, it is impossible with the cue ball to reach the black ball by a bounce on this cushion and then on the black ball. It is thus necessary to define a function which makes it possible to determine the zone of the billiard table accessible in $2$ rebounds on $2$ given perpendicular strips, while avoiding the rebounds in the holes.&lt;/p&gt;
&lt;p&gt;This zone is delimited by a limit trajectory, which corresponds to the trajectory where the ball bounces as close as possible to the hole. The function then returns the coordinates of the segment that delimits this zone.&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/changementbase.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;We will then use this segment to determine whether the cue ball positions are accessible. By transforming this segment into a vector and normalizing it, we will create a new orthonormal base:&lt;/p&gt;
&lt;p&gt;$$\left(O&amp;rsquo;, \ \overrightarrow{x&amp;rsquo;}, \ \overrightarrow{y&amp;rsquo;}\right) = \left(B, \ \frac{\overrightarrow{BC}}{||\overrightarrow{BC}||}, \ \frac{\overrightarrow{BD}}{||\overrightarrow{BD}||}\right)$$&lt;/p&gt;
&lt;p&gt;The following base change matrix is then defined:&lt;/p&gt;
&lt;p&gt;$$ P = \frac{1}{||\overrightarrow{BC}||} \begin{pmatrix}
x_{\overrightarrow{BC}} &amp;amp; -y_{\overrightarrow{BC}} \\\
y_{\overrightarrow{BC}} &amp;amp; x_{\overrightarrow{BC}}
\end{pmatrix}$$&lt;/p&gt;
&lt;p&gt;Then a function is created that returns the coordinates of the shock positions in the new base. If a position is accessible, the $y&amp;rsquo;$ coordinate of this position will then be positive in this example.&lt;/p&gt;
&lt;p&gt;We can then determine the trajectory to be taken in two rebounds thanks to Thales&amp;rsquo; theorem by unfolding the triangles formed by the trajectory to be taken.&lt;/p&gt;
&lt;p&gt;$$\frac{a}{c} = \frac{b_1}{b_2 + d} \iff b_1 = \frac{a(b + d)}{a + c}$$
$$\alpha = arctan\left(\frac{b + d}{a + c}\right)$$&lt;/p&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/trianglesthales.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/trianglesdeplies.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Since we know the coordinates of the cue ball and the cue position, thus the parameters $a$, $b$, $c$ and $d$, we can determine the angle to be given to the cue ball. The formula is presented here for $2$ rebounds, but it is analogous for $3$ rebounds or more.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;elimination-of-unattainable-trajectories&#34;&gt;Elimination of unattainable trajectories&lt;/h2&gt;
&lt;p&gt;Now that we have determined all the trajectories to pocket a ball in $2$ maximum rebound, we must eliminate the trajectories that are unattainable, i.e. those where balls or holes are present on the trajectory.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;removing-paths-where-other-balls-are-present&#34;&gt;Removing paths where other balls are present&lt;/h3&gt;
&lt;p&gt;In order to detect the balls present on the trajectory, basic changes are made again with the $x$-axis of the trajectory taken. A log is present on the path if the absolute value of its ordinate in the new base is less than twice the radius, i.e. $5$ $cm$ :&lt;/p&gt;
&lt;p&gt;$$x&amp;rsquo; \geq 0 \text{ and } |y&amp;rsquo;| &amp;lt; 2R$$&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;removal-of-paths-bouncing-in-holes&#34;&gt;Removal of paths bouncing in holes&lt;/h3&gt;
&lt;p&gt;To eliminate rebound paths in the hole areas, simply define conditions on the abscissa and ordinate sides of the rebound positions. The final program then plots the achievable paths on the GUI.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;taking-into-account-the-deceleration-of-the-balls&#34;&gt;Taking into account the deceleration of the balls&lt;/h2&gt;
&lt;p&gt;However, we have not yet taken into account the deceleration of the balls during movement, which is an important parameter. Indeed, a cue ball sent too softly will not reach its goal, and a ball going too fast will not check the reflection law during rebounds, and therefore will not reach its goal either. It is therefore necessary to calculate the minimum initial speed necessary to pocket a ball.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;calculation-of-the-speed-restored-on-impact&#34;&gt;Calculation of the speed restored on impact&lt;/h3&gt;
&lt;p&gt;By considering a linear decrease of the cue ball velocity as determined experimentally with $a = 0.17$ $m.s^{-2}$, we know its velocity as a function of time and its position by integration :
$$v(t) = v_0 - at$$
$$x(t) = x_0 + v_0t - a\frac{t^2}{2}$$&lt;/p&gt;
&lt;p&gt;At the moment $t_1$, the ball will have travelled a distance of $x_1$ to bounce against a strip :
$$x_1 = v_0t_1 - a\frac{t_1^2}{2} \qquad \Longrightarrow \qquad t_1 = \frac{v_0 - \sqrt{v_0^2 - 2ax_1}}{a}$$&lt;/p&gt;
&lt;p&gt;We then know the speed just before the bounce, then just after, considering that the ball starts again with a certain percentage $\eta = 0.8$ of its speed. In the same way, because of the angle between the balls during the impact, only a percentage $\beta$ of the speed of the cue ball is transmitted to the colored ball.&lt;/p&gt;
&lt;p&gt;$$v_{1, ap} = \eta v_{1, av} = \eta(v_0 - at_1) = \eta \sqrt{v_0^2 - 2ax_1}$$&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;determining-the-speed-of-the-ball-after-n-collisions&#34;&gt;Determining the speed of the ball after $n$ collisions&lt;/h3&gt;
&lt;p&gt;We then obtain by recurrence the final speed of the ball after $n$ rebounds or shocks according to the initial speed :&lt;/p&gt;
&lt;p&gt;$$v_n = \left(v_0^2 \prod_{i = 1}^n \eta_i^2 - 2a \sum_{i = 1}^{n + 1} x_i \prod_{j = i}^n \eta_j^2\right)^{\frac{1}{2}}$$&lt;/p&gt;
&lt;p&gt;We then define the minimum initial speed to be given to the cue ball, which is obtained when the colored ball is pocketed at zero speed :&lt;/p&gt;
&lt;p&gt;$$ v_n = 0 \iff v_0 = \left(\frac{2a \sum_{i = 1}^{n + 1} x_i \prod_{j = i}^n \eta_j^2}{\prod_{i = 1}^n \eta_i^2}\right)^{\frac{1}{2}} $$&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;correction-of-programs-by-eliminating-paths&#34;&gt;Correction of programs by eliminating paths&lt;/h3&gt;
&lt;p&gt;Finally, in order for the cue ball to respect Descartes&amp;rsquo; law of reflection, we define a maximum initial velocity of $2$ $m.s^{-1}$. We then add the formula of the initial velocity in the final program so that it calculates the minimum initial velocity required. If this speed is greater than the maximum initial speed, the move corresponding to the trajectory is considered to be unfeasible. Otherwise, the trajectory is plotted in orange.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Thus, to answer the initial problem, namely &amp;ldquo;how to pocket a billiard ball for sure&amp;rdquo;, we can first of all notice that the cue ball must be sent at medium speed, because then Descartes&amp;rsquo; law of reflection makes it possible to easily determine the possible trajectories to pocket a ball.&lt;/p&gt;
&lt;p&gt;It should be remembered that the whole study is carried out under the hypothesis of balls moving without effect. Therefore the initial shot plays an essential role, because if one gives effect to the ball, the trajectories are parabolic and therefore the laws of reflection and elastic shocks are no longer valid at all.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;!---

&amp;nbsp;

# Présentation du projet

De nombreux joueurs de billard, moi y compris, rêveraient d’empocher toutes les billes les unes à la suite des autres. Toutefois, en raison des nombreux paramètres qui régissent le mouvement des billes, la tâche est difficile à réaliser, mais pas impossible. Nous allons donc rechercher la trajectoire parfaite au billard pour savoir « comment empocher à coup sûr une bille de billard ».

*Puisque nous cherchons à rendre les trajectoires le plus prévisible possible, nous considérerons dans notre étude des billes évoluant sans effet, donc de façon rectiligne.*

Nous nous focaliserons dans un premier temps sur une étude expérimentale des frottements, puis sur l’étude théorique des $3$ principales lois physiques régissant le mouvement, et enfin je développerai la conception des programmes informatiques qui nous permettront de déterminer la, voire les trajectoires parfaites pour empocher une bille.

&amp;nbsp;

# Etude expérimentale des frottements

Tout d’abord, nous allons nous intéresser aux frottements de la bille avec le tapis, qui sont bien évidemment la cause de la perte de vitesse de la bille. Pour étudier l’évolution de cette vitesse, j’ai procédé à un pointage vidéo avec le logiciel *LatisPro*, après avoir placé un repère et un étalon de $1,82$ $m$ correspondant à la longueur du billard.

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/latispro.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

Le zoom du logiciel permet de faire des relevés très précis, néanmoins la vidéo ayant une cadence de $30$ images par seconde, la bille ne se déplace que de quelques pixels entre deux images. Le tracé de l’évolution de la vitesse sur Excel est alors assez chaotique. J’ai donc ensuite relevé la position de la bille toutes les $5$ images, pour constater une évolution linéaire de la vitesse, avec une décélération de $0,17$ $m.s^{-2}$, et une incertitude sur cette décélération de $0,044$ $m.s^{-2}$ obtenue par la méthode des moindres carrés :

$$a = -0,170 \pm 0,044 \ m.s^{-2}$$

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/regression.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

# Etude théorique des 3 principales lois du mouvement

Nous passons désormais à l’étude théorique pour justifier cette décélération constante, ainsi que le mouvement des billes lors des rebonds sur les bandes et lors des chocs entre billes.

&amp;nbsp;

## Etude des rebonds

En considérant des bandes rigides, les rebonds suivent la loi de réflexion de Descartes, de façon analogue aux rayons lumineux en optique.

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/rebond.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

## Etude des chocs

Sous l’hypothèse de billes évoluant sans effet, les chocs entre billes sont régis par les lois des chocs élastiques, à savoir conservation de la quantité de mouvement et de l’énergie cinétique :

$$\overrightarrow{p_1} = \overrightarrow{p_1&#39;} + \overrightarrow{p_2&#39;}$$
$$\frac{1}{2}mv_1^2 = \frac{1}{2}mv_1&#39;^2 + \frac{1}{2}mv_2&#39;^2$$

Cela implique une déviation des billes selon des trajectoires formant un angle $\alpha = 90°$.

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/choc.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

## Etude des frottements

Enfin, pour les frottements, on distingue théoriquement deux phases de roulement. Cependant, nous nous intéresserons qu’à la seconde phase de roulement sans glissement, la première phase de roulement avec glissement ne se déroulant que pendant une durée négligeable. La position de la bille pendant la phase de roulement avec glissement est donnée par la formule :

$$\overrightarrow{OG}(t) = \overrightarrow{OG}(0) + \overrightarrow{v_{/R}}(G, 0)t - \frac{\overrightarrow{v_{/R}}(I, 0)}{||\overrightarrow{v_{/R}}(I, 0)||} \frac{fgt^2}{2}$$

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/roulement.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

On montre avec le principe fondamental de la dynamique et le théorème du moment cinétique que, la bille étant en roulement sans glissement, on ne peut pas négliger la force de résistance à l’avancement due à l’enfoncement de la bille dans le tapis :

$$\overrightarrow{F} = -f_cmg\frac{\overrightarrow{v_{/R}}(G)}{||\overrightarrow{v_{/R}}(G)||}$$

Régis Petit, ingénieur-chercheur, nous indique dans sa *Théorie du jeu sur le billard* que cette force est opposée à la vitesse, et proportionnelle à un coefficient $f_c$ :

$$f_c = \frac{\sin(\gamma)}{0.4 + \cos(\gamma)}$$

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/glissement.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

Une bonne approximation de ce coefficient s’obtient en mesurant le temps $t$ mis par la bille pour parcourir la longueur $L$ du billard, en arrivant à cette longueur $L$ à vitesse nulle :

$$f_c \approx \frac{2L}{gt^2}$$

Pour notre billard expérimental, il faut $5,5$ $s$ à la bille pour parcourir les $1,82$ $m$ en arrivant à vitesse nulle, ce qui correspond à un angle $\gamma = 1°$ et un coefficient $f_c = 1,25\%$. Ces résultats sont cohérents puisque les billards sont conçus de façon à ce que les frottements soient le plus négligeable possible.

&amp;nbsp;

# Conception des programmes de recherche des trajectoires

Maintenant que nous connaissons les lois régissant le mouvement des billes, nous allons pouvoir réaliser les programmes informatiques qui vont nous indiquer les trajectoires parfaites pour empocher une bille.

*Dans le but de proposer des trajectoires prévisibles et réalisables facilement sur un vrai jeu de billard, on considère que seule la bille blanche se déplace, en effectuant uniquement des rebonds, et $2$ au maximum. Lorsqu’elle frappe la bille de couleur que nous souhaitons empocher, la bille de couleur se dirige directement dans le trou sans effectuer de rebonds.*

&amp;nbsp;

## Recherche de l&#39;ensemble des trajectoires permettant d&#39;atteindre une bille

Dans un premier temps, il nous faut réaliser des programmes qui tracent le billard, les billes et les trajectoires, pour pouvoir vérifier le bon fonctionnement de nos futurs programmes.

&amp;nbsp;

### Réalisation d&#39;une simulation graphique

Le principal programme de tracé que j’ai réalisé est un programme qui prend comme argument un nombre $n$, trace de façon aléatoire $n$ billes sur le billard, et renvoie la liste des coordonnées des billes. Une fonction trace la trajectoire passant par une position donnée, et une autre fonction trace la trajectoire après un rebond sur une bande selon la loi de Descartes. Ces deux fonctions renvoient les coordonnées d’impact des billes sur les bandes.

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/tracealeatoire.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

### Recherche des positions à atteindre pour empocher une bille

Une fois ces fonctions réalisées, j’ai d’abord cherché à savoir où frapper une bille pour qu’elle se dirige dans un trou. L’hypothèse des chocs élastiques implique que si la bille de couleur est immobile, il existe une unique position de choc qui permette d’empocher la bille dans un trou donné. Cette position est définie par les relations présentées ici, sachant qu’à l’instant du choc, les deux billes et le trou doivent être alignés :

$$a^2 + b^2 = 4R^2 \text{ et } m = \frac{y_C - y_B}{x_C - x_B} = \frac{b}{a}$$

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/positionchoc.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

Puisqu’on connaît les coordonnées de la bille de couleur et des trous, on peut déterminer $m$, puis en déduire $a$ et $b$ qui définissent les coordonnées de la position de choc :

$$b = m \cdot a \text{ et } a = \pm \frac{2R}{\sqrt{1 + m^2}}$$

On peut alors réaliser une fonction qui, prenant en argument les coordonnées d’une bille, renvoie les coordonnées des $6$ positions de choc correspondant aux $6$ trous du billard, et trace ces positions sur l’interface graphique.

&amp;nbsp;

### Détermination de l&#39;ensemble des trajectoires possibles

Il faut ensuite voir si ces positions sont accessibles ou non, c’est-à-dire si on peut les atteindre en $2$ rebonds maximum.

&amp;nbsp;

#### Tir direct ou en bandes parallèles

En tir direct, la trajectoire est simplement un segment délimité par la bille blanche et la position de choc. En un rebond ou en deux rebonds sur bandes parallèles, c’est-à-dire comme ça ou comme ça, n’importe quelle position sur le billard est accessible.

&amp;nbsp;

#### Tir en bandes perpendiculaires

En deux rebonds sur bandes perpendiculaires par contre, comme ceci par exemple, tout le billard n’est pas accessible. En effet, dans cette configuration, il est impossible avec la bille blanche d’atteindre la bille noire par un rebond sur cette bande puis sur celle-ci. Il faut donc définir une fonction qui permet de déterminer la zone du billard accessible en $2$ rebonds sur $2$ bandes perpendiculaires données, tout en évitant les rebonds dans les trous.

Cette zone est délimitée par une trajectoire limite, qui correspond à la trajectoire où la bille rebondit le plus près possible du trou. La fonction renvoie alors les coordonnées du segment qui délimite cette zone.

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/changementbase.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

Nous allons ensuite nous servir de ce segment pour déterminer si les positions de choc d’une bille sont accessibles. En transformant ce segment en vecteur et en le normant, nous allons créer une nouvelle base orthonormée :

$$\left(O&#39;, \ \overrightarrow{x&#39;}, \ \overrightarrow{y&#39;}\right) = \left(B, \ \frac{\overrightarrow{BC}}{||\overrightarrow{BC}||}, \ \frac{\overrightarrow{BD}}{||\overrightarrow{BD}||}\right)$$

On définit alors la matrice de changement de base présente suivante :

$$ P = \frac{1}{||\overrightarrow{BC}||} \begin{pmatrix}
x_{\overrightarrow{BC}} &amp; -y_{\overrightarrow{BC}} \\\\\\
y_{\overrightarrow{BC}} &amp; x_{\overrightarrow{BC}}
\end{pmatrix}$$

Puis on crée une fonction qui renvoie les coordonnées des positions de choc dans la nouvelle base. Si une position est accessible, la coordonnée $y’$ de cette position sera alors positive dans cet exemple.

On peut alors déterminer la trajectoire à emprunter en deux rebonds grâce au théorème de Thalès par un dépliage des triangles formés par la trajectoire à emprunter.

$$\frac{a}{c} = \frac{b_1}{b_2 + d} \iff b_1 = \frac{a(b + d)}{a + c}$$
$$\alpha = arctan\left(\frac{b + d}{a + c}\right)$$

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/trianglesthales.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://michaelkarpe.github.io/media/billiards/trianglesdeplies.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&amp;nbsp;

Etant donné qu’on connaît les coordonnées de la bille blanche et de la position de choc, donc les paramètres $a$, $b$, $c$ et $d$, on peut déterminer l’angle à donner à la bille blanche. La formule est présentée ici dans le cas de $2$ rebonds, mais elle est analogue pour $3$ rebonds ou plus.

&amp;nbsp;

## Elimination des trajectoires irréalisables

Maintenant que nous avons déterminé toutes les trajectoires pour empocher une bille en $2$ rebonds maximum, il nous faut éliminer les trajectoires qui ne sont pas réalisables, à savoir celles où des billes ou bien des trous sont présents sur la trajectoire.

&amp;nbsp;

### Suppression des trajectoires où d&#39;autres billes sont présentes

Pour détecter les billes présentes sur la trajectoire, on procède de  nouveau à des changements de base avec pour axe des abscisses la trajectoire empruntée. Une bille est présente sur la trajectoire si la valeur absolue de son ordonnée dans la nouvelle base est inférieure à deux fois le rayon, soit $5$ $cm$:

$$x&#39; \geq 0 \text{ et } |y&#39;| &lt; 2R$$

&amp;nbsp;

### Suppression des trajectoires rebondissant dans les trous

Pour éliminer les trajectoires rebondissant dans les zones des trous, il suffit de définir des conditions sur les abscisses et ordonnées des positions de rebonds. Le programme final trace alors les trajectoires réalisables sur l’interface graphique.

&amp;nbsp;

## Prise en compte de la décélération des billes

Toutefois, nous n’avons pas encore pris en compte la décélération des billes lors du mouvement, qui est un paramètre important. En effet, une bille blanche envoyée trop doucement n’atteindra pas son but, et une bille qui va trop vite ne vérifie plus en pratique la loi de réflexion lors des rebonds, et donc n’atteindra pas son but également. Il faut donc calculer la vitesse initiale minimale nécessaire pour empocher une bille.

&amp;nbsp;

### Calcul de la vitesse restituée lors d&#39;un choc

En considérant une décroissance linéaire de la vitesse de la bille blanche telle que déterminée expérimentalement avec $a = 0,17$ $m.s^{-2}$, on connaît sa vitesse en fonction du temps et sa position par intégration :
$$v(t) = v_0 - at$$
$$x(t) = x_0 + v_0t - a\frac{t^2}{2}$$

À l’instant $t_1$, la bille aura parcouru une distance $x_1$ pour rebondir contre une bande :
$$x_1 = v_0t_1 - a\frac{t_1^2}{2} \qquad \Longrightarrow \qquad t_1 = \frac{v_0 - \sqrt{v_0^2 - 2ax_1}}{a}$$

On connaît alors la vitesse juste avant le rebond, puis juste après en considérant que la bille repart avec un certain pourcentage $\eta = 0.8$ de sa vitesse. De même, en raison de l’angle entre les billes lors du choc, seul un pourcentage $\beta$ de la vitesse de la bille blanche est transmis à la bille de couleur.

$$v_{1, ap} = \eta v_{1, av} = \eta(v_0 - at_1) = \eta \sqrt{v_0^2 - 2ax_1}$$

&amp;nbsp;

### Détermination de la vitesse de la bille après $n$ collisions

On obtient alors par récurrence la vitesse finale de la bille après $n$ rebonds ou chocs en fonction de la vitesse initiale :

$$v_n = \left(v_0^2 \prod_{i = 1}^n \eta_i^2 - 2a \sum_{i = 1}^{n + 1} x_i \prod_{j = i}^n \eta_j^2\right)^{\frac{1}{2}}$$

On définit ensuite la vitesse initiale minimale à donner à la bille blanche, qui est obtenue lorsque la bille de couleur est empochée à une vitesse nulle :

$$ v_n = 0 \iff v_0 = \left(\frac{2a \sum_{i = 1}^{n + 1} x_i \prod_{j = i}^n \eta_j^2}{\prod_{i = 1}^n \eta_i^2}\right)^{\frac{1}{2}} $$

&amp;nbsp;

### Correction des programmes par élimination des trajectoires

Enfin, pour que la bille blanche respecte la loi de réflexion de Descartes, on définit une vitesse initiale maximale de $2$ $m.s^{-1}$. On ajoute alors la formule de la vitesse initiale dans le programme final pour qu’il calcule la vitesse initiale minimale nécessaire. Si cette vitesse est supérieure à la vitesse initiale maximale, le coup correspondant à la trajectoire est considéré comme irréalisable. Sinon, la trajectoire est tracée en orange.

&amp;nbsp;

# Conclusion

Ainsi, pour répondre à la problématique initiale, à savoir « comment empocher à coup sûr une bille de billard », nous pouvons tout d’abord remarquer qu’il faut envoyer la bille blanche à vitesse moyenne, car alors la loi de réflexion de Descartes permet de déterminer facilement les trajectoires possibles pour empocher une bille.

Il faut rappeler que toute l’étude est réalisée dans l’hypothèse de billes se déplaçant sans effet. Par conséquent le tir initial joue un rôle essentiel, car si l’on donne de l’effet à la bille, les trajectoires sont paraboliques et donc les lois de réflexion et des chocs élastiques ne sont plus du tout valables.

&amp;nbsp;

---&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://michaelkarpe.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://michaelkarpe.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
